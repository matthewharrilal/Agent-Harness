<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Competitive Landscape Analysis</title>
  <style>
    :root {
      --bg: #0d1117;
      --bg-card: #161b22;
      --bg-elevated: #1c2128;
      --text: #e6edf3;
      --text-dim: #8b949e;
      --text-muted: #6e7681;
      --blue: #58a6ff;
      --green: #3fb950;
      --yellow: #d29922;
      --red: #f85149;
      --purple: #a371f7;
      --border: #30363d;
    }

    * { box-sizing: border-box; margin: 0; padding: 0; }

    body {
      font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
      font-size: 15px;
      line-height: 1.6;
      color: var(--text);
      background: var(--bg);
      padding: 3rem 2rem;
    }

    .container { max-width: 860px; margin: 0 auto; }

    h1 {
      font-size: 1.75rem;
      font-weight: 600;
      margin-bottom: 0.5rem;
    }
    .subtitle {
      color: var(--text-dim);
      margin-bottom: 2.5rem;
    }

    /* Category Headers */
    .category {
      margin: 3rem 0 1.5rem;
      padding-bottom: 0.75rem;
      border-bottom: 1px solid var(--border);
    }
    .category-name {
      font-size: 0.8rem;
      font-weight: 600;
      text-transform: uppercase;
      letter-spacing: 0.1em;
      color: var(--purple);
      margin-bottom: 0.25rem;
    }
    .category-desc {
      color: var(--text-dim);
      font-size: 0.9rem;
    }

    /* Tool Sections */
    .tool {
      background: var(--bg-card);
      border: 1px solid var(--border);
      border-radius: 8px;
      padding: 1.5rem;
      margin-bottom: 1.25rem;
    }
    .tool-title {
      display: flex;
      align-items: center;
      gap: 0.75rem;
      margin-bottom: 0.75rem;
    }
    .tool-name {
      font-size: 1.1rem;
      font-weight: 600;
    }
    .tool-badge {
      font-size: 0.75rem;
      padding: 0.2rem 0.6rem;
      border-radius: 4px;
      font-weight: 500;
    }
    .tool-badge.high { background: rgba(63,185,80,0.15); color: var(--green); }
    .tool-badge.medium { background: rgba(210,153,34,0.15); color: var(--yellow); }
    .tool-badge.low { background: rgba(248,81,73,0.15); color: var(--red); }

    .tool-desc {
      color: var(--text-dim);
      margin-bottom: 1.25rem;
      line-height: 1.7;
    }

    /* Section labels */
    .section-label {
      font-size: 0.7rem;
      font-weight: 600;
      text-transform: uppercase;
      letter-spacing: 0.08em;
      margin-bottom: 0.5rem;
    }
    .section-label.works { color: var(--green); }
    .section-label.why { color: var(--purple); }
    .section-label.breaks { color: var(--red); }
    .section-label.gap { color: var(--yellow); }

    .section-content {
      color: var(--text-dim);
      margin-bottom: 1.25rem;
      padding-left: 0.75rem;
      border-left: 2px solid var(--border);
    }
    .section-content.works { border-color: rgba(63,185,80,0.4); }
    .section-content.why { border-color: rgba(163,113,247,0.4); }
    .section-content.breaks { border-color: rgba(248,81,73,0.4); }
    .section-content.gap { border-color: rgba(210,153,34,0.4); }

    .section-content ul {
      list-style: none;
      padding: 0;
    }
    .section-content li {
      margin: 0.4rem 0;
      padding-left: 1rem;
      position: relative;
    }
    .section-content li::before {
      content: "•";
      position: absolute;
      left: 0;
      color: var(--text-muted);
    }

    /* Divider */
    .divider {
      border: none;
      border-top: 1px solid var(--border);
      margin: 3rem 0;
    }

    /* Summary Table */
    .summary-section {
      margin: 3rem 0;
    }
    .summary-title {
      font-size: 1.25rem;
      font-weight: 600;
      margin-bottom: 1.5rem;
    }

    .capability-row {
      display: grid;
      grid-template-columns: 1fr auto 1fr;
      gap: 1rem;
      padding: 0.75rem 0;
      border-bottom: 1px solid var(--border);
      align-items: start;
    }
    .capability-row:last-child { border-bottom: none; }

    .capability-name {
      color: var(--text);
      font-weight: 500;
    }
    .capability-exists {
      text-align: center;
      font-size: 0.85rem;
      font-weight: 600;
      min-width: 50px;
    }
    .capability-exists.yes { color: var(--green); }
    .capability-exists.no { color: var(--red); }
    .capability-who {
      color: var(--text-muted);
      font-size: 0.9rem;
    }

    /* Assessment Box */
    .assessment {
      background: var(--bg-card);
      border: 1px solid var(--border);
      border-radius: 8px;
      padding: 1.5rem;
      margin: 2rem 0;
    }
    .assessment-title {
      font-size: 1.1rem;
      font-weight: 600;
      margin-bottom: 1rem;
      color: var(--yellow);
    }
    .assessment-item {
      margin: 0.75rem 0;
      padding-left: 1.5rem;
      position: relative;
      color: var(--text-dim);
    }
    .assessment-item::before {
      content: attr(data-num);
      position: absolute;
      left: 0;
      color: var(--text-muted);
      font-size: 0.85rem;
    }
    .assessment-item strong {
      color: var(--text);
    }

    /* Context box */
    .context {
      background: var(--bg-elevated);
      border: 1px solid var(--border);
      border-radius: 8px;
      padding: 1.25rem;
      margin-bottom: 2rem;
    }
    .context-row {
      display: flex;
      margin: 0.4rem 0;
    }
    .context-key {
      color: var(--text-muted);
      min-width: 140px;
      font-size: 0.9rem;
    }
    .context-value {
      color: var(--text);
      font-size: 0.9rem;
    }

    /* Code/scenario blocks */
    code {
      background: var(--bg-elevated);
      padding: 0.15rem 0.4rem;
      border-radius: 4px;
      font-family: 'SF Mono', Monaco, monospace;
      font-size: 0.9em;
      color: var(--blue);
    }

    .scenario {
      background: var(--bg);
      border: 1px solid var(--border);
      border-radius: 6px;
      padding: 1rem;
      margin: 0.75rem 0;
      font-family: 'SF Mono', Monaco, monospace;
      font-size: 0.8rem;
      line-height: 1.6;
      color: var(--text-muted);
      white-space: pre-wrap;
      overflow-x: auto;
    }
  </style>
</head>
<body>

<div class="container">

  <h1>Competitive Landscape Analysis</h1>
  <p class="subtitle">8 tools analyzed for hybrid Claude + Gemini orchestration</p>

  <div class="context">
    <div class="context-row">
      <span class="context-key">Problem</span>
      <span class="context-value">85% of Claude Max rate limits hit in 1-2 days</span>
    </div>
    <div class="context-row">
      <span class="context-key">Proposed Solution</span>
      <span class="context-value">Invisible middleware routing between Claude + Gemini</span>
    </div>
    <div class="context-row">
      <span class="context-key">Key Question</span>
      <span class="context-value">Is the ~40% gap between existing tools and the vision worth building?</span>
    </div>
  </div>

  <!-- TERMINOLOGY SECTION -->
  <div class="tool" style="margin-top: 2rem; border-color: var(--purple);">
    <div class="tool-title">
      <span class="tool-name">What Is An Agent Harness?</span>
    </div>

    <p class="tool-desc">
      The 2026 industry consensus (Philipp Schmid/HuggingFace, Sequoia Capital, Aakash Gupta) is that <strong>"2026 Is Agent Harnesses"</strong> — the harness, not the model, is where differentiation happens. Manus rewrote their harness 5 times with the same underlying models; each rewrite improved reliability. The infrastructure matters more than the weights.
    </p>

    <div class="section-label" style="color: var(--purple);">Industry Definition</div>
    <div class="section-content" style="border-color: rgba(163,113,247,0.4);">
      <p style="margin-bottom: 0.75rem;">An <strong>agent harness</strong> is the complete architectural system surrounding an LLM that manages what the model can't do itself:</p>
      <ul>
        <li><strong>Execute tool calls</strong> — Model outputs intent, harness performs the action</li>
        <li><strong>Manage memory</strong> — Persist context beyond conversation window</li>
        <li><strong>Structure workflows</strong> — Coordinate multi-step operations</li>
        <li><strong>Handle long-running tasks</strong> — Checkpointing, recovery, continuation</li>
        <li><strong>Enforce rules</strong> — Guardrails, permissions, validation</li>
      </ul>
      <p style="margin-top: 0.75rem; color: var(--text-muted);">The harness is scaffolding — not the model, not the business logic, but the infrastructure that makes agents reliable.</p>
    </div>

    <div class="section-label" style="color: var(--yellow);">Your Concept: A Hybrid</div>
    <div class="section-content" style="border-color: rgba(210,153,34,0.4);">
      <p style="margin-bottom: 0.75rem;">What you're building is <strong>more than a harness</strong> — it combines three architectural components:</p>
      <ul>
        <li><strong>Harness component</strong> — Memory management, tool execution, context persistence across sessions</li>
        <li><strong>Orchestrator component</strong> — Routing logic deciding Claude vs Gemini per task based on semantics</li>
        <li><strong>Proxy/Router component</strong> — Intercepts requests, directs traffic, handles failover</li>
      </ul>
      <p style="margin-top: 0.75rem; color: var(--text-muted);">If you only call it a "harness," people expect infrastructure for ONE model. Your vision includes orchestration BETWEEN models — deciding which chef handles which dish, sharing prep work between kitchens.</p>
    </div>

    <div class="section-label" style="color: var(--blue);">Better Terminology Options</div>
    <div class="section-content" style="border-color: rgba(88,166,255,0.4);">
      <ul>
        <li><strong>Agent Orchestration Layer</strong> — Clearest, industry-standard. Emphasizes coordination between providers.</li>
        <li><strong>Multi-Model Harness</strong> — Extends the harness definition to explicitly cover cross-provider work.</li>
        <li><strong>Agentic Router</strong> — If emphasizing the routing decision logic over infrastructure.</li>
      </ul>
    </div>

    <div class="section-label" style="color: var(--text-dim);">Why Existing Tools Aren't This</div>
    <div class="section-content" style="border-color: var(--border);">
      <p style="margin-bottom: 0.75rem;">The 8 tools analyzed each handle ONE piece:</p>
      <ul>
        <li><strong>CCProxy, Portkey, OpenRouter</strong> — Proxies/gateways that route, but don't manage state</li>
        <li><strong>claude-code-mux</strong> — Failover proxy, but loses reasoning state during handoff</li>
        <li><strong>PAL MCP</strong> — Delegation tool, but no persistent memory or automatic routing</li>
        <li><strong>hcom</strong> — Inter-agent messaging, but passive (tells what happened, not what should happen)</li>
        <li><strong>Conductor Build</strong> — Parallel execution, but Claude-only, no cross-provider orchestration</li>
        <li><strong>CLIProxyAPI</strong> — Quota visibility, but display-only, doesn't act on thresholds</li>
      </ul>
      <p style="margin-top: 0.75rem; color: var(--text-muted);"><strong>None combine:</strong> semantic routing + persistent memory + proactive rate prediction + mid-task context handoff. That's the ~40% gap.</p>
    </div>
  </div>

  <!-- ROUTING/PROXY LAYER -->
  <div class="category">
    <div class="category-name">Routing / Proxy Layer</div>
    <div class="category-desc">Tools that sit between your client and AI providers, intercepting and routing requests</div>
  </div>

  <div class="tool">
    <div class="tool-title">
      <span class="tool-name">CCProxy</span>
      <span class="tool-badge high">~65%</span>
    </div>
    <p class="tool-desc">
      Python-based proxy using LiteLLM that intercepts Claude Code API requests. Routes based on 4 rule types: <code>TokenCountRule</code> (if tokens > threshold), <code>MatchModelRule</code> (model name matching), <code>ThinkingRule</code> (thinking parameter presence), and <code>MatchToolRule</code> (tool invocation detection). Design philosophy is zero overhead, transparent proxy — rules evaluate in &lt;1ms.
    </p>

    <div class="section-label works">What It Does Well</div>
    <div class="section-content works">
      <ul>
        <li>Token-based routing works perfectly for large context scenarios — request with 180K tokens automatically routes to Gemini's 1M context window</li>
        <li>Tool-based routing enables WebSearch → Perplexity scenarios without manual intervention</li>
        <li>Zero latency overhead since rules are simple conditionals, not model calls</li>
        <li>Works with Claude Max subscriptions (OAuth compatible)</li>
        <li>Transparent to Claude Code — it doesn't know the proxy exists</li>
      </ul>
    </div>

    <div class="section-label why">Why It Works This Way</div>
    <div class="section-content why">
      <ul>
        <li><strong>Architectural constraint:</strong> A proxy sees the request envelope (tokens, model name, tools), not the semantic intent inside. It operates at the HTTP boundary.</li>
        <li><strong>Speed over intelligence:</strong> Rules evaluate in &lt;1ms. Adding semantic classification would add 50-200ms latency per request — that's no longer a "transparent proxy."</li>
        <li><strong>Simplicity over accuracy:</strong> 4 clear rule types vs. a classifier that needs training data, maintenance, and can misroute 10-15% of requests unpredictably.</li>
        <li><strong>Fails predictably:</strong> If token-count routing is wrong, it's wrong consistently. Classifier errors are unpredictable and harder to debug.</li>
        <li><strong>This is a valid trade-off:</strong> CCProxy chose transparency and speed. Not every tool needs to be intelligent — some should just be fast and reliable.</li>
      </ul>
    </div>

    <div class="section-label breaks">What Breaks</div>
    <div class="section-content breaks">
      <ul>
        <li>Cannot understand task semantics — "Research JWT vulnerabilities" and "Debug auth.ts" both look like ~10 tokens with no tools, route identically</li>
        <li>No rate limit awareness — doesn't know you're at 85% Claude quota, routes next request to Claude anyway</li>
        <li>No failover — if Anthropic fails, request fails. No retry logic.</li>
        <li>No shared state between requests — each request is independent, no learning from patterns</li>
        <li>Mixed-intent requests route entirely to one provider — "Research X then code Y" goes 100% to Claude, wasting quota on research</li>
      </ul>
    </div>

    <div class="section-label gap">The Gap</div>
    <div class="section-content gap">
      <ul>
        <li><strong>Missing capability:</strong> Task-intent classification — distinguishing research from coding from debugging based on the prompt content</li>
        <li><strong>Missing capability:</strong> Rate limit awareness — knowing quota state and routing accordingly</li>
        <li><strong>Open question:</strong> Is adding 50-200ms latency for semantic routing worth the quota savings? Depends on how often misrouting actually happens in your workflow.</li>
        <li><strong>Open question:</strong> Could heuristics (keyword matching) get 80% of semantic routing value without the latency cost?</li>
      </ul>
    </div>
  </div>

  <div class="tool">
    <div class="tool-title">
      <span class="tool-name">claude-code-mux</span>
      <span class="tool-badge high">~60%</span>
    </div>
    <p class="tool-desc">
      Rust-based HTTP proxy providing automatic failover between AI providers. When Anthropic returns 429, it immediately tries your backup providers (OpenRouter, Vertex AI, etc.). Supports 18+ providers, OAuth authentication, &lt;1ms routing overhead, ~5MB memory footprint. Designed as a stateless proxy — each request is independent.
    </p>

    <div class="section-label works">What It Does Well</div>
    <div class="section-content works">
      <ul>
        <li>Seamless failover on 429 errors — you send a request, Anthropic fails, mux catches it, tries OpenRouter, returns response. You never know the switch happened.</li>
        <li>Priority-based provider configuration — set Anthropic as priority 1, OpenRouter as priority 2, Vertex as priority 3</li>
        <li>OAuth authentication support — works with Claude Max subscriptions</li>
        <li>Minimal resource usage — Rust binary, &lt;1ms routing, ~5MB memory</li>
        <li>Fault isolation — one bad request doesn't affect subsequent requests</li>
      </ul>
    </div>

    <div class="section-label why">Why It Works This Way</div>
    <div class="section-content why">
      <ul>
        <li><strong>Stateless by design:</strong> Each request is independent. No database, no storage, no persistence. This means zero infrastructure requirements and no state corruption risks.</li>
        <li><strong>Proxy, not orchestrator:</strong> Adding memory would change the tool's nature entirely. It would need a database, state extraction logic, injection logic — a different product.</li>
        <li><strong>The hard problem:</strong> Extracting "reasoning state" from Claude's session is non-trivial. You can't just dump the conversation — Claude's internal hypothesis chain isn't in the text.</li>
        <li><strong>Failure mode simplicity:</strong> Stateless means easy debugging (each request reproducible), horizontal scaling (spin up more proxies), and crash recovery (restart, zero data loss).</li>
        <li><strong>This is a valid trade-off:</strong> For a failover proxy, reliability beats intelligence. Memory adds complexity and new failure modes.</li>
      </ul>
    </div>

    <div class="section-label breaks">What Breaks</div>
    <div class="section-content breaks">
      <ul>
        <li><strong>Context continuity breaks during failover.</strong> Turn 1-5: debugging a race condition, Claude eliminated 2 hypotheses, testing the third. Turn 6: 429, switches to OpenRouter. Fresh instance re-reads messages, might re-investigate eliminated hypotheses. You lose 5-10 minutes.</li>
        <li>No semantic routing — doesn't understand task type, just priority-based failover</li>
        <li>No rate limit prediction — reacts to 429 after it happens, doesn't predict or prevent it</li>
        <li>Message history preserved, reasoning state lost — the text is there but the "mental model" isn't</li>
      </ul>
    </div>

    <div class="section-label gap">The Gap</div>
    <div class="section-content gap">
      <ul>
        <li><strong>Missing capability:</strong> Reasoning state preservation during provider handoff — some way to capture "where Claude was" in its debugging process</li>
        <li><strong>Missing capability:</strong> Proactive rate prediction — routing before hitting limits, not after</li>
        <li><strong>Open question:</strong> How do you capture reasoning state? Is it in the message history (could be summarized)? Or is it truly internal to the inference session (fundamentally lost)?</li>
        <li><strong>Open question:</strong> Would a simple "here's what we've tried so far" summary injected into the new session be enough? Or do you need something more sophisticated?</li>
      </ul>
    </div>
  </div>

  <div class="tool">
    <div class="tool-title">
      <span class="tool-name">Portkey</span>
      <span class="tool-badge low">~30%</span>
    </div>
    <p class="tool-desc">
      Cloud AI gateway for enterprise use. Every request goes through Portkey's servers where it can be routed based on metadata, logged, cached, and monitored. Supports complex conditional routing rules, excellent observability (costs, latency, tokens), request caching, failover/retries, and per-team spending limits. SOC 2 compliant.
    </p>

    <div class="section-label works">What It Does Well</div>
    <div class="section-content works">
      <ul>
        <li>Metadata-based conditional routing — <code>IF metadata.task_type == "research" THEN route to Gemini</code></li>
        <li>Excellent observability — every request logged with costs, latency, token counts</li>
        <li>Caching — identical requests return cached responses, saves money</li>
        <li>Failover and retries — if Provider A fails, try Provider B</li>
        <li>Enterprise features — audit logs, cost controls per team/project, SOC 2 compliance</li>
      </ul>
    </div>

    <div class="section-label why">Why It Works This Way</div>
    <div class="section-content why">
      <ul>
        <li><strong>Gateway principle:</strong> The routing layer shouldn't own business logic. Your application understands your tasks better than a generic gateway ever could.</li>
        <li><strong>Separation of concerns:</strong> Portkey's job is to forward, log, and cache. Classification is YOUR responsibility — you know what "research" means in your context.</li>
        <li><strong>Flexibility:</strong> You can use ANY classifier upstream (keyword matching, embeddings, LLM calls, heuristics). Portkey doesn't constrain your approach.</li>
        <li><strong>Stateless simplicity:</strong> Gateway stays simple, fast, and reliable. Complexity lives upstream where it belongs.</li>
        <li><strong>API keys by design:</strong> Enterprise gateways work with API keys because that's how providers bill enterprises. Subscription OAuth is a consumer pattern.</li>
      </ul>
    </div>

    <div class="section-label breaks">What Breaks</div>
    <div class="section-content breaks">
      <ul>
        <li><strong>Cannot use Claude Max subscriptions.</strong> Anthropic banned third-party OAuth access in Jan 2026. Portkey requires API keys. You'd pay for Max AND API credits — double cost.</li>
        <li>No semantic understanding — routes based on metadata YOU provide. "Upstream logic" problem: YOU classify, then tell Portkey via metadata.</li>
        <li>Stateless — no memory between requests, no context persistence</li>
        <li>Cloud dependency — every request routes through Portkey's servers, adding latency and requiring trust</li>
      </ul>
    </div>

    <div class="section-label gap">The Gap</div>
    <div class="section-content gap">
      <ul>
        <li><strong>Missing capability:</strong> Automatic semantic classification — something needs to decide "this is research" without requiring your code to do it</li>
        <li><strong>Missing capability:</strong> Subscription authentication — using Max/$200mo through routing infrastructure</li>
        <li><strong>Open question:</strong> Should the classifier live in the gateway (simpler for user, less flexible) or upstream (more flexible, more work for user)?</li>
        <li><strong>Open question:</strong> Is Portkey's enterprise focus incompatible with individual Max subscriptions, or is there a bridge?</li>
      </ul>
    </div>
  </div>

  <div class="tool">
    <div class="tool-title">
      <span class="tool-name">OpenRouter</span>
      <span class="tool-badge low">~25%</span>
    </div>
    <p class="tool-desc">
      API gateway providing unified access to 300+ AI models from 60+ providers through a single endpoint. One integration, all models — call <code>openrouter.ai/api/v1/messages</code> with any model name (Claude, GPT-4, Gemini, Llama, Mistral). Has "Auto Router" feature powered by NotDiamond that learns which models perform best for task types. BYOK option available with 5% fee.
    </p>

    <div class="section-label works">What It Does Well</div>
    <div class="section-content works">
      <ul>
        <li>Single endpoint for 300+ models — one integration, all providers</li>
        <li>Easy model switching — just change the model name in the API call</li>
        <li>Auto Router — learned routing powered by NotDiamond, picks optimal model per task</li>
        <li>Cost comparison — see pricing across all models in one place</li>
        <li>Fallback support — if model A fails, try model B</li>
        <li>BYOK option — use your own API keys (with 5% fee)</li>
      </ul>
    </div>

    <div class="section-label why">Why It Works This Way</div>
    <div class="section-content why">
      <ul>
        <li><strong>Unified interface principle:</strong> One endpoint simplifies integration. You don't need 60 SDKs or manage 60 API keys.</li>
        <li><strong>Explicit over implicit:</strong> Auto Router doesn't auto-activate. YOU decide when to use it. This gives control to the user.</li>
        <li><strong>Learning from YOUR patterns:</strong> Auto Router needs 15+ examples to learn what works FOR YOU. Generic routing wouldn't match your specific task distribution.</li>
        <li><strong>Avoiding over-fitting:</strong> Not every team uses models the same way. The training requirement ensures personalization rather than generic one-size-fits-all routing.</li>
        <li><strong>BYOK flexibility:</strong> Some users want to pay per-token through OpenRouter, others want to use their existing API keys. Both options exist.</li>
      </ul>
    </div>

    <div class="section-label breaks">What Breaks</div>
    <div class="section-content breaks">
      <ul>
        <li><strong>Cannot use Claude Max subscriptions.</strong> Same OAuth problem as Portkey — requires API keys, not subscription auth.</li>
        <li>Auto Router requires 15+ labeled training examples. Cold start problem — you need to manually label tasks before it helps.</li>
        <li>Claude Code hardcodes model selection — Auto Router feature not exposed to CLI tools like Claude Code</li>
        <li>Stateless — no shared memory between requests, no context persistence</li>
        <li>5% BYOK fee adds cost on top of provider pricing</li>
      </ul>
    </div>

    <div class="section-label gap">The Gap</div>
    <div class="section-content gap">
      <ul>
        <li><strong>Missing capability:</strong> Zero-config routing — something that works out of the box without 15+ training examples</li>
        <li><strong>Missing capability:</strong> Integration with Claude Code — exposing routing decisions to CLI tools</li>
        <li><strong>Open question:</strong> Is the 15-example training requirement actually a feature (personalization) or a friction point? Would generic defaults be good enough?</li>
        <li><strong>Open question:</strong> Could heuristics bootstrap Auto Router faster than manual labeling?</li>
      </ul>
    </div>
  </div>

  <!-- DELEGATION LAYER -->
  <div class="category">
    <div class="category-name">Delegation / MCP Layer</div>
    <div class="category-desc">Tools that let Claude delegate tasks to other AI models</div>
  </div>

  <div class="tool">
    <div class="tool-title">
      <span class="tool-name">PAL MCP</span>
      <span class="tool-badge medium">~45%</span>
    </div>
    <p class="tool-desc">
      MCP server (10K+ GitHub stars) that runs inside Claude Code, providing the <code>clink</code> tool ("CLI Link") that spawns external CLI processes. Claude can delegate to Gemini, Codex, or other Claude instances. Supports role-based prompting via pre-configured system prompts, file passing, and structured JSON output. Core idea: Claude Code has limits (200K context, no web search), PAL lets it call models with different strengths.
    </p>

    <div class="section-label works">What It Does Well</div>
    <div class="section-content works">
      <ul>
        <li>Clean delegation interface — <code>clink(cli_name="gemini", role="security-analyzer", prompt="Analyze auth.ts")</code></li>
        <li>Role-based prompting — pre-configured system prompts for different task types load automatically</li>
        <li>Result integration — Gemini's output flows naturally back into Claude's conversation as a tool result</li>
        <li>Multiple CLI support — configure Gemini, Codex, other Claude instances, anything with a CLI</li>
        <li>Delegates large context work to Gemini (1M context) while Claude orchestrates</li>
        <li>Battle-tested, production-ready with 10K+ GitHub stars</li>
      </ul>
    </div>

    <div class="section-label why">Why It Works This Way</div>
    <div class="section-content why">
      <ul>
        <li><strong>Process isolation:</strong> Fresh subprocess = fresh state = no cleanup, no corruption risk. One call's failure doesn't contaminate the next.</li>
        <li><strong>Clarity over magic:</strong> Users know exactly what Gemini sees (the prompt you sent). No hidden context injection surprises.</li>
        <li><strong>Manual control:</strong> YOU decide when to delegate via explicit clink calls. This avoids classification errors where the system misroutes your task.</li>
        <li><strong>Debuggability:</strong> "Show me what PAL sent to Gemini" is straightforward. With automatic routing, debugging "why did it choose Gemini?" is harder.</li>
        <li><strong>MCP scope:</strong> PAL is a delegation tool, not an orchestration layer. Adding persistent memory would expand scope significantly.</li>
      </ul>
    </div>

    <div class="section-label breaks">What Breaks</div>
    <div class="section-content breaks">
      <ul>
        <li><strong>No persistent memory across sessions.</strong> Day 1: Gemini finds JWT bug. Day 2: PAL spawns NEW Gemini with ZERO memory. Re-analyzes from scratch, might find different things.</li>
        <li>Delegation is MANUAL — no automatic routing. You explicitly call clink or write CLAUDE.md rules.</li>
        <li>25K token output limit — MCP tool responses capped, large analyses get truncated</li>
        <li>2-minute timeout — long-running Gemini tasks may timeout</li>
        <li>No rate limit awareness — doesn't know if Claude/Gemini approaching limits</li>
        <li>Context passed, not shared — Gemini sees files you specify, not Claude's reasoning state</li>
      </ul>
    </div>

    <div class="section-label gap">The Gap</div>
    <div class="section-content gap">
      <ul>
        <li><strong>Missing capability:</strong> Cross-session memory — Day 2 should know what Day 1 discovered without manual copy-paste</li>
        <li><strong>Missing capability:</strong> Automatic delegation — routing based on task type without explicit clink calls</li>
        <li><strong>Open question:</strong> Is manual delegation actually a feature (user control) or a friction point? Would automatic routing misroute often enough to be worse?</li>
        <li><strong>Open question:</strong> What's the right memory model? File-based (simple, grep-able) vs. vector DB (semantic search) vs. something else?</li>
      </ul>
    </div>
  </div>

  <!-- INTER-AGENT COMMUNICATION -->
  <div class="category">
    <div class="category-name">Inter-Agent Communication</div>
    <div class="category-desc">Tools that let multiple AI agents communicate with each other</div>
  </div>

  <div class="tool">
    <div class="tool-title">
      <span class="tool-name">hcom</span>
      <span class="tool-badge low">~35%</span>
    </div>
    <p class="tool-desc">
      Hook-based messaging layer using SQLite as a message bus. When Claude writes a file, a hook posts to SQLite. Gemini's hook polls SQLite and sees the event. Enables real-time inter-agent awareness through Claude Code's native hook system. Messages are structured event notifications (file_write, review_comment), not text chat.
    </p>

    <div class="section-label works">What It Does Well</div>
    <div class="section-content works">
      <ul>
        <li>Real-time inter-agent awareness — Claude writes rateLimit.ts → hook posts to SQLite → Gemini polls → reviews file → posts findings → Claude sees feedback. ~100 second loop.</li>
        <li>Collision detection — both agents notified if they touch same file</li>
        <li>Persistent event log — SQLite preserves history across polling gaps</li>
        <li>Native integration — works with Claude Code's existing hook system, no new infrastructure</li>
        <li>Lightweight — just SQLite + hook scripts</li>
        <li>Coordination without explicit orchestration — agents can react to each other without a central controller</li>
      </ul>
    </div>

    <div class="section-label why">Why It Works This Way</div>
    <div class="section-content why">
      <ul>
        <li><strong>Lightweight over heavyweight:</strong> SQLite + hooks is minimal infrastructure. No central orchestrator to run, maintain, or become a single point of failure.</li>
        <li><strong>No single point of failure:</strong> If the message bus goes down, agents still run independently. They just lose awareness, not functionality.</li>
        <li><strong>Agent autonomy:</strong> Agents decide how to react to messages. The bus doesn't prescribe behavior — it just broadcasts events.</li>
        <li><strong>Messaging ≠ Routing:</strong> hcom answers "what happened?" not "who should do this?" That's a deliberate scope boundary.</li>
        <li><strong>Native to Claude Code:</strong> Uses existing hook system. No new concepts to learn, no foreign infrastructure.</li>
      </ul>
    </div>

    <div class="section-label breaks">What Breaks</div>
    <div class="section-content breaks">
      <ul>
        <li><strong>Messaging only, NOT routing.</strong> hcom tells agents what happened, not what SHOULD happen. "Claude wrote auth.ts" is information, not a decision.</li>
        <li>Collision detected, not prevented — both agents edit same file, THEN see the collision. No locking before edit.</li>
        <li>No automatic context injection — messages are raw events, not synthesized actionable summaries</li>
        <li>Requires both agents running — Gemini must be active to receive messages</li>
        <li>Polling delays — 500ms-2sec gaps between event and reaction</li>
        <li>No failure handling — if Gemini's review times out, messages pile up unprocessed</li>
      </ul>
    </div>

    <div class="section-label gap">The Gap</div>
    <div class="section-content gap">
      <ul>
        <li><strong>Missing capability:</strong> Task routing — something that decides WHO should handle a task, not just announces what happened</li>
        <li><strong>Missing capability:</strong> Collision prevention — locking files before write, not detecting conflicts after</li>
        <li><strong>Open question:</strong> Is centralized routing better than autonomous agents? Central routing is clearer but creates a single point of failure.</li>
        <li><strong>Open question:</strong> Could hcom's event stream be enriched with routing hints without becoming a full orchestrator?</li>
      </ul>
    </div>
  </div>

  <!-- PARALLEL EXECUTION -->
  <div class="category">
    <div class="category-name">Parallel Execution</div>
    <div class="category-desc">Tools that run multiple AI agents simultaneously</div>
  </div>

  <div class="tool">
    <div class="tool-title">
      <span class="tool-name">Conductor Build</span>
      <span class="tool-badge low">~15%</span>
    </div>
    <p class="tool-desc">
      macOS Electron app (YC-backed, by Melty Labs) that runs multiple Claude Code instances in parallel, each isolated in its own git worktree. Visual dashboard monitors all agents simultaneously. Core idea: 5 independent features? Spin up 5 Claude instances, each on a different branch, merge when done. Conductor CREATES agents — it's the controller, not a viewer.
    </p>

    <div class="section-label works">What It Does Well</div>
    <div class="section-content works">
      <ul>
        <li>True parallel execution — 5 agents on 5 CPU cores simultaneously</li>
        <li>Git isolation via worktrees — Agent 1 modifies auth.ts without affecting Agent 2's copy. No merge conflicts during development.</li>
        <li>Visual dashboard — see all agents' progress, terminal output, file changes, diffs at a glance</li>
        <li>Merge coordination — helps combine branches when agents finish</li>
        <li>No manual git commands — worktree management is automatic</li>
        <li>YC-backed, actively developed with real support</li>
      </ul>
    </div>

    <div class="section-label why">Why It Works This Way</div>
    <div class="section-content why">
      <ul>
        <li><strong>Deep integration over broad integration:</strong> Conductor optimizes for Claude's strengths (precision coding, debugging). Supporting multiple models would dilute focus.</li>
        <li><strong>Simplicity of scope:</strong> 5 Claudes is simpler than Claude + Gemini + GPT-4. One model = consistent behavior, cost structure, performance expectations.</li>
        <li><strong>Merge strategy assumes homogeneity:</strong> Git worktree merging works because all agents produce similar code style. Mixed models would complicate merges.</li>
        <li><strong>Rate limit semantics are clear:</strong> One subscription, one pool. Easier to reason about than tracking multiple provider limits.</li>
        <li><strong>Visual-first design:</strong> A dashboard for 5 identical Claude instances is simpler than a dashboard showing heterogeneous model behaviors.</li>
      </ul>
    </div>

    <div class="section-label breaks">What Breaks</div>
    <div class="section-content breaks">
      <ul>
        <li><strong>5 agents = 5x rate limit burn.</strong> All use your single subscription. What lasts a week lasts 1-2 days. Your profile: 85% in 1-2 days. Conductor: 100% in ~8 hours.</li>
        <li>Claude-only — no Gemini, no GPT-4, no model selection for different task types</li>
        <li>No cross-agent communication — agents can't see what each other is doing</li>
        <li>No shared memory — each agent is fully isolated in its worktree</li>
        <li>No semantic task routing — you manually assign tasks to agents</li>
        <li>Silent failures — agents stall on 429 without clear notification</li>
        <li>Conductor solves "5 independent tasks, 48-hour sprint." Your problem is "continuous work across a full week."</li>
      </ul>
    </div>

    <div class="section-label gap">The Gap</div>
    <div class="section-content gap">
      <ul>
        <li><strong>Missing capability:</strong> Multi-provider support — using Claude AND Gemini to spread rate limits across independent pools</li>
        <li><strong>Missing capability:</strong> Rate limit awareness — knowing when to throttle or route to prevent 429s</li>
        <li><strong>Open question:</strong> Is parallel execution the right model for your workflow (burst work) or is sequential with intelligent routing (sustained work) better?</li>
        <li><strong>Open question:</strong> Would cross-agent communication be useful, or is isolation actually a feature (no interference)?</li>
      </ul>
    </div>
  </div>

  <!-- QUOTA MONITORING -->
  <div class="category">
    <div class="category-name">Quota Monitoring</div>
    <div class="category-desc">Tools that track and display rate limit usage</div>
  </div>

  <div class="tool">
    <div class="tool-title">
      <span class="tool-name">CLIProxyAPI / CodexBar</span>
      <span class="tool-badge high">~60%</span>
    </div>
    <p class="tool-desc">
      Local proxy server with macOS menu bar widget showing real-time quota usage across AI providers. Also does round-robin load balancing if you have multiple API keys/accounts. Menu bar displays: Claude 78% | Gemini 15% | $4.23 today. Works with Claude Max subscriptions.
    </p>

    <div class="section-label works">What It Does Well</div>
    <div class="section-content works">
      <ul>
        <li>Glanceable quota visibility — menu bar always shows current usage percentages</li>
        <li>Multi-account load distribution — if you have 3 API keys, spreads load across them (3x effective quota)</li>
        <li>Post-hoc failover — if one account fails, tries the next</li>
        <li>Cost tracking — see total spend per provider per day</li>
        <li>Lightweight — just a menu bar widget, minimal overhead</li>
        <li>Works with Claude Max — tracks subscription usage, not just API</li>
      </ul>
    </div>

    <div class="section-label why">Why It Works This Way</div>
    <div class="section-content why">
      <ul>
        <li><strong>Observability vs control separation:</strong> CLIProxyAPI's job is to show you what's happening. YOUR job is to decide what to do about it.</li>
        <li><strong>User keeps agency:</strong> You see Claude at 85% and decide: "Keep going on this important debug" or "Switch to Gemini for this research." Human judgment.</li>
        <li><strong>Simplicity:</strong> Monitoring is straightforward (intercept, count tokens, display). Decision-making is hard and context-dependent.</li>
        <li><strong>Avoiding over-automation:</strong> Not every context warrants switching. You might WANT to exhaust Claude on critical debugging rather than switching to Gemini.</li>
        <li><strong>Multi-account focus:</strong> Round-robin makes sense for multiple SAME-provider accounts. Cross-provider routing is a different, more complex problem.</li>
      </ul>
    </div>

    <div class="section-label breaks">What Breaks</div>
    <div class="section-content breaks">
      <ul>
        <li><strong>Visibility without agency.</strong> 10:00 AM: Claude at 72%. 10:45 AM: Claude at 85%, still debugging. 11:15 AM: 429. You SAW quota but it didn't ACT. Display only.</li>
        <li>Reactive only — fails over AFTER 429. Request already failed, context may be lost.</li>
        <li>No semantic routing — treats all requests equally regardless of task type</li>
        <li>No proactive thresholds — can't configure "route to Gemini when Claude hits 80%"</li>
        <li>No prediction — shows current state, not "at this rate, 2.3 hours until limit"</li>
        <li>Round-robin loses caching benefits — Anthropic caches per-account, rotating breaks this</li>
      </ul>
    </div>

    <div class="section-label gap">The Gap</div>
    <div class="section-content gap">
      <ul>
        <li><strong>Missing capability:</strong> Proactive threshold routing — acting on quota data before hitting 429, not just displaying it</li>
        <li><strong>Missing capability:</strong> Rate prediction — "at current consumption rate, X hours until limit"</li>
        <li><strong>Open question:</strong> Is automatic routing at thresholds better than human judgment? User might WANT to burn Claude quota on important work.</li>
        <li><strong>Open question:</strong> What's the right threshold? 80%? 90%? Context-dependent? Should user configure it or should the system learn?</li>
      </ul>
    </div>
  </div>

  <hr class="divider">

  <!-- SUMMARY TABLE -->
  <div class="summary-section">
    <div class="summary-title">Capability Matrix: What Exists Today?</div>

    <div class="capability-row">
      <span class="capability-name">Route by token count/metadata</span>
      <span class="capability-exists yes">YES</span>
      <span class="capability-who">CCProxy, Portkey, OpenRouter</span>
    </div>

    <div class="capability-row">
      <span class="capability-name">Auto-failover on 429</span>
      <span class="capability-exists yes">YES</span>
      <span class="capability-who">claude-code-mux, CLIProxyAPI</span>
    </div>

    <div class="capability-row">
      <span class="capability-name">Manual delegation to Gemini</span>
      <span class="capability-exists yes">YES</span>
      <span class="capability-who">PAL MCP (clink tool)</span>
    </div>

    <div class="capability-row">
      <span class="capability-name">Real-time agent messaging</span>
      <span class="capability-exists yes">YES</span>
      <span class="capability-who">hcom</span>
    </div>

    <div class="capability-row">
      <span class="capability-name">Parallel Claude instances</span>
      <span class="capability-exists yes">YES</span>
      <span class="capability-who">Conductor Build</span>
    </div>

    <div class="capability-row">
      <span class="capability-name">Quota visibility</span>
      <span class="capability-exists yes">YES</span>
      <span class="capability-who">CLIProxyAPI, CodexBar</span>
    </div>

    <div class="capability-row" style="margin-top: 1rem; padding-top: 1rem; border-top: 1px solid var(--border);">
      <span class="capability-name">Semantic task routing (research vs code)</span>
      <span class="capability-exists no">NO</span>
      <span class="capability-who">Nobody — requires upstream classifier</span>
    </div>

    <div class="capability-row">
      <span class="capability-name">Persistent cross-session memory</span>
      <span class="capability-exists no">NO</span>
      <span class="capability-who">Nobody — PAL has 3hr limit, hcom logs but doesn't inject</span>
    </div>

    <div class="capability-row">
      <span class="capability-name">Proactive rate prediction (before 429)</span>
      <span class="capability-exists no">NO</span>
      <span class="capability-who">Nobody — all are reactive</span>
    </div>

    <div class="capability-row">
      <span class="capability-name">Automatic mid-task context handoff</span>
      <span class="capability-exists no">NO</span>
      <span class="capability-who">Nobody — claude-code-mux loses reasoning state</span>
    </div>

    <div class="capability-row">
      <span class="capability-name">Cross-model critique orchestration</span>
      <span class="capability-exists no">NO</span>
      <span class="capability-who">Nobody — no loopback architecture</span>
    </div>
  </div>

  <hr class="divider">

  <!-- HONEST ASSESSMENT -->
  <div class="assessment">
    <div class="assessment-title">Honest Assessment: Is the Gap Worth Building For?</div>

    <p style="color: var(--text-dim); margin-bottom: 1rem;">
      The 40% gap is real, but concentrated in specific scenarios:
    </p>

    <div class="assessment-item" data-num="1.">
      <strong>If you rarely hit rate limits mid-task:</strong> CCProxy + PAL MCP + CodexBar is probably enough. Install baseline, use for 2-4 weeks, see if friction is real.
    </div>

    <div class="assessment-item" data-num="2.">
      <strong>If you hit limits mid-debug 2-3x/week:</strong> The context handoff gap is painful — you lose reasoning state, waste time re-explaining. Worth building minimal harness.
    </div>

    <div class="assessment-item" data-num="3.">
      <strong>If you want semantic routing:</strong> Nobody has this. You'd need to build the classifier regardless of other tools. But CLAUDE.md heuristics might be 80% as good.
    </div>

    <div class="assessment-item" data-num="4.">
      <strong>If you want cross-session memory:</strong> Nobody has this. You'd need to build the memory layer. But manual AGENTS.md files might work for light use.
    </div>

    <p style="color: var(--text-muted); margin-top: 1.5rem; font-size: 0.9rem;">
      Bottom line: The ONLY capability you truly can't replicate with existing tools + CLAUDE.md instructions is <strong>mid-task context handoff</strong>. Everything else has workarounds. Test the baseline before building.
    </p>
  </div>

</div>

</body>
</html>
