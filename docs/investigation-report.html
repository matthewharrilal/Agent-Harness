<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Competitive Landscape Analysis</title>
  <style>
    :root {
      --bg: #0d1117;
      --bg-card: #161b22;
      --bg-elevated: #1c2128;
      --text: #e6edf3;
      --text-dim: #8b949e;
      --text-muted: #6e7681;
      --blue: #58a6ff;
      --green: #3fb950;
      --yellow: #d29922;
      --red: #f85149;
      --purple: #a371f7;
      --border: #30363d;
    }

    * { box-sizing: border-box; margin: 0; padding: 0; }

    body {
      font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
      font-size: 15px;
      line-height: 1.6;
      color: var(--text);
      background: var(--bg);
      padding: 3rem 2rem;
    }

    .container { max-width: 860px; margin: 0 auto; }

    h1 {
      font-size: 1.75rem;
      font-weight: 600;
      margin-bottom: 0.5rem;
    }
    .subtitle {
      color: var(--text-dim);
      margin-bottom: 2.5rem;
    }

    /* Category Headers */
    .category {
      margin: 3rem 0 1.5rem;
      padding-bottom: 0.75rem;
      border-bottom: 1px solid var(--border);
    }
    .category-name {
      font-size: 0.8rem;
      font-weight: 600;
      text-transform: uppercase;
      letter-spacing: 0.1em;
      color: var(--purple);
      margin-bottom: 0.25rem;
    }
    .category-desc {
      color: var(--text-dim);
      font-size: 0.9rem;
    }

    /* Tool Sections */
    .tool {
      background: var(--bg-card);
      border: 1px solid var(--border);
      border-radius: 8px;
      padding: 1.5rem;
      margin-bottom: 1.25rem;
    }
    .tool-title {
      display: flex;
      align-items: center;
      gap: 0.75rem;
      margin-bottom: 0.75rem;
    }
    .tool-name {
      font-size: 1.1rem;
      font-weight: 600;
    }
    .tool-badge {
      font-size: 0.75rem;
      padding: 0.2rem 0.6rem;
      border-radius: 4px;
      font-weight: 500;
    }
    .tool-badge.high { background: rgba(63,185,80,0.15); color: var(--green); }
    .tool-badge.medium { background: rgba(210,153,34,0.15); color: var(--yellow); }
    .tool-badge.low { background: rgba(248,81,73,0.15); color: var(--red); }

    .tool-desc {
      color: var(--text-dim);
      margin-bottom: 1.25rem;
      line-height: 1.7;
    }

    /* Section labels */
    .section-label {
      font-size: 0.7rem;
      font-weight: 600;
      text-transform: uppercase;
      letter-spacing: 0.08em;
      margin-bottom: 0.5rem;
    }
    .section-label.works { color: var(--green); }
    .section-label.breaks { color: var(--red); }
    .section-label.harness { color: var(--blue); }

    .section-content {
      color: var(--text-dim);
      margin-bottom: 1.25rem;
      padding-left: 0.75rem;
      border-left: 2px solid var(--border);
    }
    .section-content.works { border-color: rgba(63,185,80,0.4); }
    .section-content.breaks { border-color: rgba(248,81,73,0.4); }
    .section-content.harness { border-color: rgba(88,166,255,0.4); }

    .section-content ul {
      list-style: none;
      padding: 0;
    }
    .section-content li {
      margin: 0.4rem 0;
      padding-left: 1rem;
      position: relative;
    }
    .section-content li::before {
      content: "•";
      position: absolute;
      left: 0;
      color: var(--text-muted);
    }

    /* Divider */
    .divider {
      border: none;
      border-top: 1px solid var(--border);
      margin: 3rem 0;
    }

    /* Summary Table */
    .summary-section {
      margin: 3rem 0;
    }
    .summary-title {
      font-size: 1.25rem;
      font-weight: 600;
      margin-bottom: 1.5rem;
    }

    .capability-row {
      display: grid;
      grid-template-columns: 1fr auto 1fr;
      gap: 1rem;
      padding: 0.75rem 0;
      border-bottom: 1px solid var(--border);
      align-items: start;
    }
    .capability-row:last-child { border-bottom: none; }

    .capability-name {
      color: var(--text);
      font-weight: 500;
    }
    .capability-exists {
      text-align: center;
      font-size: 0.85rem;
      font-weight: 600;
      min-width: 50px;
    }
    .capability-exists.yes { color: var(--green); }
    .capability-exists.no { color: var(--red); }
    .capability-who {
      color: var(--text-muted);
      font-size: 0.9rem;
    }

    /* Assessment Box */
    .assessment {
      background: var(--bg-card);
      border: 1px solid var(--border);
      border-radius: 8px;
      padding: 1.5rem;
      margin: 2rem 0;
    }
    .assessment-title {
      font-size: 1.1rem;
      font-weight: 600;
      margin-bottom: 1rem;
      color: var(--yellow);
    }
    .assessment-item {
      margin: 0.75rem 0;
      padding-left: 1.5rem;
      position: relative;
      color: var(--text-dim);
    }
    .assessment-item::before {
      content: attr(data-num);
      position: absolute;
      left: 0;
      color: var(--text-muted);
      font-size: 0.85rem;
    }
    .assessment-item strong {
      color: var(--text);
    }

    /* Context box */
    .context {
      background: var(--bg-elevated);
      border: 1px solid var(--border);
      border-radius: 8px;
      padding: 1.25rem;
      margin-bottom: 2rem;
    }
    .context-row {
      display: flex;
      margin: 0.4rem 0;
    }
    .context-key {
      color: var(--text-muted);
      min-width: 140px;
      font-size: 0.9rem;
    }
    .context-value {
      color: var(--text);
      font-size: 0.9rem;
    }

    /* Code/scenario blocks */
    code {
      background: var(--bg-elevated);
      padding: 0.15rem 0.4rem;
      border-radius: 4px;
      font-family: 'SF Mono', Monaco, monospace;
      font-size: 0.9em;
      color: var(--blue);
    }

    .scenario {
      background: var(--bg);
      border: 1px solid var(--border);
      border-radius: 6px;
      padding: 1rem;
      margin: 0.75rem 0;
      font-family: 'SF Mono', Monaco, monospace;
      font-size: 0.8rem;
      line-height: 1.6;
      color: var(--text-muted);
      white-space: pre-wrap;
      overflow-x: auto;
    }
  </style>
</head>
<body>

<div class="container">

  <h1>Competitive Landscape Analysis</h1>
  <p class="subtitle">8 tools analyzed for hybrid Claude + Gemini orchestration</p>

  <div class="context">
    <div class="context-row">
      <span class="context-key">Problem</span>
      <span class="context-value">85% of Claude Max rate limits hit in 1-2 days</span>
    </div>
    <div class="context-row">
      <span class="context-key">Proposed Solution</span>
      <span class="context-value">Invisible middleware routing between Claude + Gemini</span>
    </div>
    <div class="context-row">
      <span class="context-key">Key Question</span>
      <span class="context-value">Is the ~40% gap between existing tools and the vision worth building?</span>
    </div>
  </div>

  <!-- TERMINOLOGY SECTION -->
  <div class="tool" style="margin-top: 2rem; border-color: var(--purple);">
    <div class="tool-title">
      <span class="tool-name">What Is An Agent Harness?</span>
    </div>

    <p class="tool-desc">
      The 2026 industry consensus (Philipp Schmid/HuggingFace, Sequoia Capital, Aakash Gupta) is that <strong>"2026 Is Agent Harnesses"</strong> — the harness, not the model, is where differentiation happens. Manus rewrote their harness 5 times with the same underlying models; each rewrite improved reliability. The infrastructure matters more than the weights.
    </p>

    <div class="section-label" style="color: var(--purple);">Industry Definition</div>
    <div class="section-content" style="border-color: rgba(163,113,247,0.4);">
      <p style="margin-bottom: 0.75rem;">An <strong>agent harness</strong> is the complete architectural system surrounding an LLM that manages what the model can't do itself:</p>
      <ul>
        <li><strong>Execute tool calls</strong> — Model outputs intent, harness performs the action</li>
        <li><strong>Manage memory</strong> — Persist context beyond conversation window</li>
        <li><strong>Structure workflows</strong> — Coordinate multi-step operations</li>
        <li><strong>Handle long-running tasks</strong> — Checkpointing, recovery, continuation</li>
        <li><strong>Enforce rules</strong> — Guardrails, permissions, validation</li>
      </ul>
      <p style="margin-top: 0.75rem; color: var(--text-muted);">The harness is scaffolding — not the model, not the business logic, but the infrastructure that makes agents reliable.</p>
    </div>

    <div class="section-label" style="color: var(--yellow);">Your Concept: A Hybrid</div>
    <div class="section-content" style="border-color: rgba(210,153,34,0.4);">
      <p style="margin-bottom: 0.75rem;">What you're building is <strong>more than a harness</strong> — it combines three architectural components:</p>
      <ul>
        <li><strong>Harness component</strong> — Memory management, tool execution, context persistence across sessions</li>
        <li><strong>Orchestrator component</strong> — Routing logic deciding Claude vs Gemini per task based on semantics</li>
        <li><strong>Proxy/Router component</strong> — Intercepts requests, directs traffic, handles failover</li>
      </ul>
      <p style="margin-top: 0.75rem; color: var(--text-muted);">If you only call it a "harness," people expect infrastructure for ONE model. Your vision includes orchestration BETWEEN models — deciding which chef handles which dish, sharing prep work between kitchens.</p>
    </div>

    <div class="section-label" style="color: var(--blue);">Better Terminology Options</div>
    <div class="section-content" style="border-color: rgba(88,166,255,0.4);">
      <ul>
        <li><strong>Agent Orchestration Layer</strong> — Clearest, industry-standard. Emphasizes coordination between providers.</li>
        <li><strong>Multi-Model Harness</strong> — Extends the harness definition to explicitly cover cross-provider work.</li>
        <li><strong>Agentic Router</strong> — If emphasizing the routing decision logic over infrastructure.</li>
      </ul>
    </div>

    <div class="section-label" style="color: var(--text-dim);">Why Existing Tools Aren't This</div>
    <div class="section-content" style="border-color: var(--border);">
      <p style="margin-bottom: 0.75rem;">The 8 tools analyzed each handle ONE piece:</p>
      <ul>
        <li><strong>CCProxy, Portkey, OpenRouter</strong> — Proxies/gateways that route, but don't manage state</li>
        <li><strong>claude-code-mux</strong> — Failover proxy, but loses reasoning state during handoff</li>
        <li><strong>PAL MCP</strong> — Delegation tool, but no persistent memory or automatic routing</li>
        <li><strong>hcom</strong> — Inter-agent messaging, but passive (tells what happened, not what should happen)</li>
        <li><strong>Conductor Build</strong> — Parallel execution, but Claude-only, no cross-provider orchestration</li>
        <li><strong>CLIProxyAPI</strong> — Quota visibility, but display-only, doesn't act on thresholds</li>
      </ul>
      <p style="margin-top: 0.75rem; color: var(--text-muted);"><strong>None combine:</strong> semantic routing + persistent memory + proactive rate prediction + mid-task context handoff. That's the ~40% gap.</p>
    </div>
  </div>

  <!-- ROUTING/PROXY LAYER -->
  <div class="category">
    <div class="category-name">Routing / Proxy Layer</div>
    <div class="category-desc">Tools that sit between your client and AI providers, intercepting and routing requests</div>
  </div>

  <div class="tool">
    <div class="tool-title">
      <span class="tool-name">CCProxy</span>
      <span class="tool-badge high">~65%</span>
    </div>
    <p class="tool-desc">
      Python-based proxy using LiteLLM that intercepts Claude Code API requests. Routes based on 4 rule types: <code>TokenCountRule</code> (if tokens > threshold), <code>MatchModelRule</code> (model name matching), <code>ThinkingRule</code> (thinking parameter presence), and <code>MatchToolRule</code> (tool invocation detection). Design philosophy is zero overhead, transparent proxy — rules evaluate in &lt;1ms.
    </p>

    <div class="section-label works">What It Does Well</div>
    <div class="section-content works">
      <ul>
        <li>Token-based routing works perfectly for large context scenarios — request with 180K tokens automatically routes to Gemini's 1M context window</li>
        <li>Tool-based routing enables WebSearch → Perplexity scenarios without manual intervention</li>
        <li>Zero latency overhead since rules are simple conditionals, not model calls</li>
        <li>Works with Claude Max subscriptions (OAuth compatible)</li>
        <li>Transparent to Claude Code — it doesn't know the proxy exists</li>
      </ul>
    </div>

    <div class="section-label breaks">What Breaks</div>
    <div class="section-content breaks">
      <ul>
        <li>Cannot understand task semantics — "Research JWT vulnerabilities" and "Debug auth.ts" both look like ~10 tokens with no tools, route identically</li>
        <li>No rate limit awareness — doesn't know you're at 85% Claude quota</li>
        <li>No failover — if Anthropic fails, request fails</li>
        <li>No shared state between requests — each request is independent</li>
        <li>Why no semantic routing? Would require LLM call (50-200ms, costs tokens) or trained classifier (needs labeled data). CCProxy prioritizes simplicity.</li>
      </ul>
    </div>

    <div class="section-label harness">With Harness</div>
    <div class="section-content harness">
      <ul>
        <li>Semantic router classifies "research" vs "coding" vs "debugging" automatically, routes research to Gemini without explicit rules</li>
        <li>Proactive rate prediction at 80% threshold, auto-routes non-critical tasks before hitting 429</li>
        <li>Automatic retry with backup provider on failure</li>
      </ul>
    </div>
  </div>

  <div class="tool">
    <div class="tool-title">
      <span class="tool-name">claude-code-mux</span>
      <span class="tool-badge high">~60%</span>
    </div>
    <p class="tool-desc">
      Rust-based HTTP proxy providing automatic failover between AI providers. When Anthropic returns 429, it immediately tries your backup providers (OpenRouter, Vertex AI, etc.). Supports 18+ providers, OAuth authentication, &lt;1ms routing overhead, ~5MB memory footprint. Designed as a stateless proxy — each request is independent.
    </p>

    <div class="section-label works">What It Does Well</div>
    <div class="section-content works">
      <ul>
        <li>Seamless failover on 429 errors — you send a request, Anthropic fails, mux catches it, tries OpenRouter, returns response. You never know the switch happened.</li>
        <li>Priority-based provider configuration — set Anthropic as priority 1, OpenRouter as priority 2, Vertex as priority 3</li>
        <li>OAuth authentication support — works with Claude Max subscriptions</li>
        <li>Minimal resource usage — Rust binary, efficient</li>
      </ul>
    </div>

    <div class="section-label breaks">What Breaks</div>
    <div class="section-content breaks">
      <ul>
        <li><strong>Context continuity breaks during failover.</strong> Turn 1-5: debugging a race condition, Claude eliminated 2 hypotheses, testing the third. Turn 6: 429 error, mux switches to OpenRouter. OpenRouter Claude gets the message history but it's a FRESH instance — no memory of the reasoning process. It might re-investigate the eliminated hypotheses. You lose 5-10 minutes.</li>
        <li>No semantic routing — doesn't understand task type, just does priority-based failover</li>
        <li>No rate limit prediction — reacts to 429, doesn't predict it</li>
        <li>Why no Mem0? By design it's stateless. Adding memory would change it from "proxy" to "orchestration layer."</li>
      </ul>
    </div>

    <div class="section-label harness">With Harness</div>
    <div class="section-content harness">
      <ul>
        <li>Shared memory captures reasoning state: "Investigating race condition. Eliminated hypothesis A (timing) and B (lock scope). Testing hypothesis C (async init order)."</li>
        <li>New provider instance receives synthesized context, continues from hypothesis C</li>
        <li>Proactive routing before hitting 429</li>
      </ul>
    </div>
  </div>

  <div class="tool">
    <div class="tool-title">
      <span class="tool-name">Portkey</span>
      <span class="tool-badge low">~30%</span>
    </div>
    <p class="tool-desc">
      Cloud AI gateway for enterprise use. Every request goes through Portkey's servers where it can be routed based on metadata, logged, cached, and monitored. Supports complex conditional routing rules, excellent observability (costs, latency, tokens), request caching, failover/retries, and per-team spending limits. SOC 2 compliant.
    </p>

    <div class="section-label works">What It Does Well</div>
    <div class="section-content works">
      <ul>
        <li>Metadata-based conditional routing — <code>IF metadata.task_type == "research" THEN route to Gemini</code></li>
        <li>Excellent observability — every request logged with costs, latency, token counts</li>
        <li>Caching — identical requests return cached responses, saves money</li>
        <li>Failover and retries — if Provider A fails, try Provider B</li>
        <li>Enterprise features — audit logs, cost controls per team/project</li>
      </ul>
    </div>

    <div class="section-label breaks">What Breaks</div>
    <div class="section-content breaks">
      <ul>
        <li><strong>Cannot use Claude Max subscriptions.</strong> Anthropic banned third-party OAuth access in Jan 2026. Portkey requires API keys (pay-per-token). You'd pay for Max AND API credits — double cost.</li>
        <li>No semantic routing — routes based on metadata YOU provide. "Upstream logic" problem: YOU must classify "research" vs "coding" BEFORE calling Portkey, then attach <code>metadata.task_type</code>. Portkey is the router, you are the classifier.</li>
        <li>Stateless — no memory between requests</li>
        <li>Cloud dependency — requests route through Portkey's servers</li>
      </ul>
    </div>

    <div class="section-label harness">With Harness</div>
    <div class="section-content harness">
      <ul>
        <li>Harness does the semantic classification upstream, Portkey handles the routing infrastructure</li>
        <li>Harness uses Max subscription credentials directly, no API key needed</li>
        <li>Persistent memory layer added on top</li>
      </ul>
    </div>
  </div>

  <div class="tool">
    <div class="tool-title">
      <span class="tool-name">OpenRouter</span>
      <span class="tool-badge low">~25%</span>
    </div>
    <p class="tool-desc">
      API gateway providing unified access to 300+ AI models from 60+ providers through a single endpoint. One integration, all models — call <code>openrouter.ai/api/v1/messages</code> with any model name (Claude, GPT-4, Gemini, Llama, Mistral). Has "Auto Router" feature powered by NotDiamond that learns which models perform best for task types. BYOK option available with 5% fee.
    </p>

    <div class="section-label works">What It Does Well</div>
    <div class="section-content works">
      <ul>
        <li>Single endpoint for 300+ models — one integration, all providers</li>
        <li>Easy model switching — just change the model name in the API call</li>
        <li>Auto Router — learned routing that analyzes prompts and picks optimal model</li>
        <li>Cost comparison — see pricing across all models in one place</li>
        <li>Fallback support — if model A fails, try model B</li>
        <li>BYOK option — use your own API keys (with 5% fee)</li>
      </ul>
    </div>

    <div class="section-label breaks">What Breaks</div>
    <div class="section-content breaks">
      <ul>
        <li><strong>Cannot use Claude Max subscriptions.</strong> Same OAuth problem as Portkey — requires API keys, not subscription auth.</li>
        <li>Auto Router requires 15+ labeled training examples before it works well. Cold start problem.</li>
        <li>Claude Code hardcodes model selection — Auto Router feature not exposed to CLI tools</li>
        <li>Stateless — no shared memory between requests</li>
        <li>5% BYOK fee adds cost on top of provider pricing</li>
      </ul>
    </div>

    <div class="section-label harness">With Harness</div>
    <div class="section-content harness">
      <ul>
        <li>Harness preserves Max subscription benefits while adding routing</li>
        <li>Zero-config semantic routing without 15+ training examples</li>
        <li>MCP server exposes routing to Claude Code</li>
      </ul>
    </div>
  </div>

  <!-- DELEGATION LAYER -->
  <div class="category">
    <div class="category-name">Delegation / MCP Layer</div>
    <div class="category-desc">Tools that let Claude delegate tasks to other AI models</div>
  </div>

  <div class="tool">
    <div class="tool-title">
      <span class="tool-name">PAL MCP</span>
      <span class="tool-badge medium">~45%</span>
    </div>
    <p class="tool-desc">
      MCP server (10K+ GitHub stars) that runs inside Claude Code, providing the <code>clink</code> tool ("CLI Link") that spawns external CLI processes. Claude can delegate to Gemini, Codex, or other Claude instances. Supports role-based prompting via pre-configured system prompts, file passing, and structured JSON output. Core idea: Claude Code has limits (200K context, no web search), PAL lets it call models with different strengths.
    </p>

    <div class="section-label works">What It Does Well</div>
    <div class="section-content works">
      <ul>
        <li>Clean delegation interface — <code>clink(cli_name="gemini", role="security-analyzer", prompt="Analyze auth.ts")</code></li>
        <li>Role-based prompting — pre-configured system prompts for different task types load automatically</li>
        <li>Result integration — Gemini's output flows naturally back into Claude's conversation as a tool result</li>
        <li>Multiple CLI support — configure Gemini, Codex, other Claude instances, anything with a CLI</li>
        <li>Delegates large context work to Gemini (1M context) while Claude orchestrates</li>
        <li>Battle-tested, production-ready with active community</li>
      </ul>
    </div>

    <div class="section-label breaks">What Breaks</div>
    <div class="section-content breaks">
      <ul>
        <li><strong>No persistent memory across sessions.</strong> Day 1: Gemini finds JWT bug, rate limiting gap. Day 2: you call clink again, PAL spawns NEW Gemini process with ZERO memory of Day 1. Re-analyzes from scratch, might produce different findings.</li>
        <li>Delegation is MANUAL — no automatic routing. Either you explicitly say "use Gemini" or you write CLAUDE.md rules and Claude decides. No semantic classification.</li>
        <li>25K token output limit — MCP tool responses capped, large analyses get truncated</li>
        <li>2-minute timeout — long-running Gemini tasks may timeout</li>
        <li>No rate limit awareness — doesn't know if Claude/Gemini approaching limits</li>
        <li>Context passed, not shared — Gemini sees files you specify, not Claude's reasoning state</li>
      </ul>
    </div>

    <div class="section-label harness">With Harness</div>
    <div class="section-content harness">
      <ul>
        <li>Persistent memory store auto-injects prior findings: "Yesterday's analysis found JWT bug in auth.ts:42"</li>
        <li>Semantic router classifies tasks automatically, routes without explicit clink calls</li>
        <li>Proactive prediction switches providers at 80% quota</li>
        <li>Shared memory includes Claude's reasoning, not just files</li>
      </ul>
    </div>
  </div>

  <!-- INTER-AGENT COMMUNICATION -->
  <div class="category">
    <div class="category-name">Inter-Agent Communication</div>
    <div class="category-desc">Tools that let multiple AI agents communicate with each other</div>
  </div>

  <div class="tool">
    <div class="tool-title">
      <span class="tool-name">hcom</span>
      <span class="tool-badge low">~35%</span>
    </div>
    <p class="tool-desc">
      Hook-based messaging layer using SQLite as a message bus. When Claude writes a file, a hook posts to SQLite. Gemini's hook polls SQLite and sees the event. Enables real-time inter-agent awareness through Claude Code's native hook system. Messages are structured event notifications (file_write, review_comment), not text chat.
    </p>

    <div class="section-label works">What It Does Well</div>
    <div class="section-content works">
      <ul>
        <li>Real-time inter-agent awareness — Claude writes rateLimit.ts → hook posts to SQLite → Gemini's hook polls → Gemini reviews file → posts findings back → Claude sees feedback. ~100 second feedback loop.</li>
        <li>Collision detection — both agents notified if they touch same file</li>
        <li>Persistent event log — SQLite preserves history across polling gaps</li>
        <li>Native integration — works with Claude Code's existing hook system</li>
        <li>Lightweight — just SQLite + hook scripts, no heavy infrastructure</li>
        <li>Enables coordination without explicit orchestration</li>
      </ul>
    </div>

    <div class="section-label breaks">What Breaks</div>
    <div class="section-content breaks">
      <ul>
        <li><strong>Messaging only, NOT routing.</strong> hcom tells agents what happened, not what SHOULD happen. "Claude wrote auth.ts" is information, not a decision about who should write tests.</li>
        <li>Collision detected, not prevented — both agents edit same file, THEN see the collision. No locking.</li>
        <li>No automatic context injection — messages are raw events, not synthesized actionable context</li>
        <li>Requires both agents running simultaneously — Gemini must be active to receive messages</li>
        <li>Polling delays — 500ms-2sec gaps between event and reaction</li>
        <li>No failure handling — if Gemini's review times out, messages pile up</li>
      </ul>
    </div>

    <div class="section-label harness">With Harness</div>
    <div class="section-content harness">
      <ul>
        <li>Semantic router assigns tasks to appropriate agent proactively, not reactively</li>
        <li>File locking before write prevents collisions</li>
        <li>Events synthesized into actionable context summaries</li>
        <li>Rate limit capacity info included in messages</li>
      </ul>
    </div>
  </div>

  <!-- PARALLEL EXECUTION -->
  <div class="category">
    <div class="category-name">Parallel Execution</div>
    <div class="category-desc">Tools that run multiple AI agents simultaneously</div>
  </div>

  <div class="tool">
    <div class="tool-title">
      <span class="tool-name">Conductor Build</span>
      <span class="tool-badge low">~15%</span>
    </div>
    <p class="tool-desc">
      macOS Electron app (YC-backed, by Melty Labs) that runs multiple Claude Code instances in parallel, each isolated in its own git worktree. Visual dashboard monitors all agents simultaneously. Core idea: 5 independent features? Spin up 5 Claude instances, each on a different branch, merge when done. Conductor CREATES agents — it's the controller, not a viewer.
    </p>

    <div class="section-label works">What It Does Well</div>
    <div class="section-content works">
      <ul>
        <li>True parallel execution — 5 agents on 5 CPU cores simultaneously</li>
        <li>Git isolation via worktrees — Agent 1 modifies auth.ts without affecting Agent 2's copy. No merge conflicts during development.</li>
        <li>Visual dashboard — see all agents' progress, terminal output, file changes, diffs at a glance</li>
        <li>Merge coordination — helps combine branches when agents finish</li>
        <li>No manual git commands — worktree management is automatic</li>
        <li>YC-backed, actively developed with real support</li>
      </ul>
    </div>

    <div class="section-label breaks">What Breaks</div>
    <div class="section-content breaks">
      <ul>
        <li><strong>5 agents = 5x rate limit burn.</strong> All use your single Claude Max subscription. What lasts a week now lasts 1-2 days. Your profile: already hit 85% in 1-2 days. Conductor: would hit 100% in ~8 hours.</li>
        <li>Claude-only — no Gemini, no GPT-4, no model selection</li>
        <li>No cross-agent communication — agents can't see what each other is doing</li>
        <li>No shared memory — each agent is fully isolated</li>
        <li>No semantic task routing — you manually assign tasks</li>
        <li>Silent failures — agents stall on 429 without clear notification</li>
        <li>Conductor solves "5 independent tasks, 48-hour sprint." Your problem is "continuous work across a full week."</li>
      </ul>
    </div>

    <div class="section-label harness">With Harness</div>
    <div class="section-content harness">
      <ul>
        <li>Claude + Gemini + others — two independent rate limit pools</li>
        <li>Quota-aware throttling, load balancing across providers</li>
        <li>Shared message bus enables cross-agent awareness</li>
        <li>Semantic routing to appropriate model per task</li>
      </ul>
    </div>
  </div>

  <!-- QUOTA MONITORING -->
  <div class="category">
    <div class="category-name">Quota Monitoring</div>
    <div class="category-desc">Tools that track and display rate limit usage</div>
  </div>

  <div class="tool">
    <div class="tool-title">
      <span class="tool-name">CLIProxyAPI / CodexBar</span>
      <span class="tool-badge high">~60%</span>
    </div>
    <p class="tool-desc">
      Local proxy server with macOS menu bar widget showing real-time quota usage across AI providers. Also does round-robin load balancing if you have multiple API keys/accounts. Menu bar displays: Claude 78% | Gemini 15% | $4.23 today. Works with Claude Max subscriptions.
    </p>

    <div class="section-label works">What It Does Well</div>
    <div class="section-content works">
      <ul>
        <li>Glanceable quota visibility — menu bar always shows current usage percentages</li>
        <li>Multi-account load distribution — if you have 3 API keys, spreads load across them (3x effective quota)</li>
        <li>Post-hoc failover — if one account fails, tries the next</li>
        <li>Cost tracking — see total spend per provider per day</li>
        <li>Lightweight — just a menu bar widget, minimal overhead</li>
        <li>Works with Claude Max — tracks subscription usage, not just API</li>
      </ul>
    </div>

    <div class="section-label breaks">What Breaks</div>
    <div class="section-content breaks">
      <ul>
        <li><strong>Visibility without agency.</strong> 10:00 AM: see Claude at 72%. 10:45 AM: see Claude at 85%, still debugging. 11:15 AM: 429 error. You SAW the quota but CLIProxyAPI didn't ACT on it. Display only.</li>
        <li>Reactive only — fails over AFTER 429, not before. You already hit the limit, request failed.</li>
        <li>No semantic routing — treats all requests equally regardless of task type</li>
        <li>No proactive thresholds — can't configure "switch at 80%"</li>
        <li>No prediction — shows current state, not "2.3 hours until limit at current rate"</li>
        <li>Round-robin loses caching benefits — Anthropic caches per-account, rotating breaks this</li>
      </ul>
    </div>

    <div class="section-label harness">With Harness</div>
    <div class="section-content harness">
      <ul>
        <li>Routes at 80% threshold automatically — non-critical research goes to Gemini, preserves Claude quota for debugging</li>
        <li>Semantic awareness — knows "this is research, safe to route to Gemini"</li>
        <li>Prediction: "At current rate, 2.3 hours until limit"</li>
        <li>Data drives routing decisions, not just display</li>
      </ul>
    </div>
  </div>

  <hr class="divider">

  <!-- SUMMARY TABLE -->
  <div class="summary-section">
    <div class="summary-title">Capability Matrix: What Exists Today?</div>

    <div class="capability-row">
      <span class="capability-name">Route by token count/metadata</span>
      <span class="capability-exists yes">YES</span>
      <span class="capability-who">CCProxy, Portkey, OpenRouter</span>
    </div>

    <div class="capability-row">
      <span class="capability-name">Auto-failover on 429</span>
      <span class="capability-exists yes">YES</span>
      <span class="capability-who">claude-code-mux, CLIProxyAPI</span>
    </div>

    <div class="capability-row">
      <span class="capability-name">Manual delegation to Gemini</span>
      <span class="capability-exists yes">YES</span>
      <span class="capability-who">PAL MCP (clink tool)</span>
    </div>

    <div class="capability-row">
      <span class="capability-name">Real-time agent messaging</span>
      <span class="capability-exists yes">YES</span>
      <span class="capability-who">hcom</span>
    </div>

    <div class="capability-row">
      <span class="capability-name">Parallel Claude instances</span>
      <span class="capability-exists yes">YES</span>
      <span class="capability-who">Conductor Build</span>
    </div>

    <div class="capability-row">
      <span class="capability-name">Quota visibility</span>
      <span class="capability-exists yes">YES</span>
      <span class="capability-who">CLIProxyAPI, CodexBar</span>
    </div>

    <div class="capability-row" style="margin-top: 1rem; padding-top: 1rem; border-top: 1px solid var(--border);">
      <span class="capability-name">Semantic task routing (research vs code)</span>
      <span class="capability-exists no">NO</span>
      <span class="capability-who">Nobody — requires upstream classifier</span>
    </div>

    <div class="capability-row">
      <span class="capability-name">Persistent cross-session memory</span>
      <span class="capability-exists no">NO</span>
      <span class="capability-who">Nobody — PAL has 3hr limit, hcom logs but doesn't inject</span>
    </div>

    <div class="capability-row">
      <span class="capability-name">Proactive rate prediction (before 429)</span>
      <span class="capability-exists no">NO</span>
      <span class="capability-who">Nobody — all are reactive</span>
    </div>

    <div class="capability-row">
      <span class="capability-name">Automatic mid-task context handoff</span>
      <span class="capability-exists no">NO</span>
      <span class="capability-who">Nobody — claude-code-mux loses reasoning state</span>
    </div>

    <div class="capability-row">
      <span class="capability-name">Cross-model critique orchestration</span>
      <span class="capability-exists no">NO</span>
      <span class="capability-who">Nobody — no loopback architecture</span>
    </div>
  </div>

  <hr class="divider">

  <!-- HONEST ASSESSMENT -->
  <div class="assessment">
    <div class="assessment-title">Honest Assessment: Is the Gap Worth Building For?</div>

    <p style="color: var(--text-dim); margin-bottom: 1rem;">
      The 40% gap is real, but concentrated in specific scenarios:
    </p>

    <div class="assessment-item" data-num="1.">
      <strong>If you rarely hit rate limits mid-task:</strong> CCProxy + PAL MCP + CodexBar is probably enough. Install baseline, use for 2-4 weeks, see if friction is real.
    </div>

    <div class="assessment-item" data-num="2.">
      <strong>If you hit limits mid-debug 2-3x/week:</strong> The context handoff gap is painful — you lose reasoning state, waste time re-explaining. Worth building minimal harness.
    </div>

    <div class="assessment-item" data-num="3.">
      <strong>If you want semantic routing:</strong> Nobody has this. You'd need to build the classifier regardless of other tools. But CLAUDE.md heuristics might be 80% as good.
    </div>

    <div class="assessment-item" data-num="4.">
      <strong>If you want cross-session memory:</strong> Nobody has this. You'd need to build the memory layer. But manual AGENTS.md files might work for light use.
    </div>

    <p style="color: var(--text-muted); margin-top: 1.5rem; font-size: 0.9rem;">
      Bottom line: The ONLY capability you truly can't replicate with existing tools + CLAUDE.md instructions is <strong>mid-task context handoff</strong>. Everything else has workarounds. Test the baseline before building.
    </p>
  </div>

</div>

</body>
</html>
