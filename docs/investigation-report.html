<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Competitive Landscape Analysis</title>
  <style>
    :root {
      --bg: #0d1117;
      --bg-card: #161b22;
      --bg-elevated: #1c2128;
      --text: #e6edf3;
      --text-dim: #8b949e;
      --text-muted: #6e7681;
      --blue: #58a6ff;
      --green: #3fb950;
      --yellow: #d29922;
      --red: #f85149;
      --purple: #a371f7;
      --border: #30363d;
    }

    * { box-sizing: border-box; margin: 0; padding: 0; }

    body {
      font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
      font-size: 15px;
      line-height: 1.6;
      color: var(--text);
      background: var(--bg);
      padding: 3rem 2rem;
    }

    .container { max-width: 860px; margin: 0 auto; }

    h1 {
      font-size: 1.75rem;
      font-weight: 600;
      margin-bottom: 0.5rem;
    }
    .subtitle {
      color: var(--text-dim);
      margin-bottom: 2.5rem;
    }

    /* Category Headers */
    .category {
      margin: 3rem 0 1.5rem;
      padding-bottom: 0.75rem;
      border-bottom: 1px solid var(--border);
    }
    .category-name {
      font-size: 0.8rem;
      font-weight: 600;
      text-transform: uppercase;
      letter-spacing: 0.1em;
      color: var(--purple);
      margin-bottom: 0.25rem;
    }
    .category-desc {
      color: var(--text-dim);
      font-size: 0.9rem;
    }

    /* Tool Sections */
    .tool {
      background: var(--bg-card);
      border: 1px solid var(--border);
      border-radius: 8px;
      padding: 1.5rem;
      margin-bottom: 1.25rem;
    }
    .tool-title {
      display: flex;
      align-items: center;
      gap: 0.75rem;
      margin-bottom: 0.75rem;
    }
    .tool-name {
      font-size: 1.1rem;
      font-weight: 600;
    }
    .tool-badge {
      font-size: 0.75rem;
      padding: 0.2rem 0.6rem;
      border-radius: 4px;
      font-weight: 500;
    }
    .tool-badge.high { background: rgba(63,185,80,0.15); color: var(--green); }
    .tool-badge.medium { background: rgba(210,153,34,0.15); color: var(--yellow); }
    .tool-badge.low { background: rgba(248,81,73,0.15); color: var(--red); }

    .tool-desc {
      color: var(--text-dim);
      margin-bottom: 1.25rem;
      line-height: 1.7;
    }

    /* Section labels */
    .section-label {
      font-size: 0.7rem;
      font-weight: 600;
      text-transform: uppercase;
      letter-spacing: 0.08em;
      margin-bottom: 0.5rem;
    }
    .section-label.works { color: var(--green); }
    .section-label.why { color: var(--purple); }
    .section-label.breaks { color: var(--red); }
    .section-label.gap { color: var(--yellow); }
    .section-label.journey { color: var(--blue); }

    .section-content.journey { border-color: rgba(88,166,255,0.4); }

    .journey-scenario {
      background: var(--bg);
      border: 1px solid var(--border);
      border-radius: 6px;
      padding: 1rem;
      margin: 0.75rem 0;
      font-size: 0.9rem;
      line-height: 1.7;
    }
    .journey-scenario .time {
      color: var(--blue);
      font-weight: 600;
      font-family: 'SF Mono', Monaco, monospace;
      font-size: 0.85rem;
    }
    .journey-scenario .action {
      color: var(--text-dim);
    }
    .journey-contrast {
      display: grid;
      grid-template-columns: 1fr 1fr;
      gap: 1rem;
      margin: 0.75rem 0;
    }
    .journey-contrast-item {
      background: var(--bg);
      border: 1px solid var(--border);
      border-radius: 6px;
      padding: 0.75rem;
      font-size: 0.85rem;
    }
    .journey-contrast-item .label {
      font-weight: 600;
      font-size: 0.75rem;
      text-transform: uppercase;
      letter-spacing: 0.05em;
      margin-bottom: 0.5rem;
    }
    .journey-contrast-item.current .label { color: var(--text-muted); }
    .journey-contrast-item.hypothetical .label { color: var(--purple); }

    .section-content {
      color: var(--text-dim);
      margin-bottom: 1.25rem;
      padding-left: 0.75rem;
      border-left: 2px solid var(--border);
    }
    .section-content.works { border-color: rgba(63,185,80,0.4); }
    .section-content.why { border-color: rgba(163,113,247,0.4); }
    .section-content.breaks { border-color: rgba(248,81,73,0.4); }
    .section-content.gap { border-color: rgba(210,153,34,0.4); }

    .section-content ul {
      list-style: none;
      padding: 0;
    }
    .section-content li {
      margin: 0.4rem 0;
      padding-left: 1rem;
      position: relative;
    }
    .section-content li::before {
      content: "•";
      position: absolute;
      left: 0;
      color: var(--text-muted);
    }

    /* Divider */
    .divider {
      border: none;
      border-top: 1px solid var(--border);
      margin: 3rem 0;
    }

    /* Summary Table */
    .summary-section {
      margin: 3rem 0;
    }
    .summary-title {
      font-size: 1.25rem;
      font-weight: 600;
      margin-bottom: 1.5rem;
    }

    .capability-row {
      display: grid;
      grid-template-columns: 1fr auto 1fr;
      gap: 1rem;
      padding: 0.75rem 0;
      border-bottom: 1px solid var(--border);
      align-items: start;
    }
    .capability-row:last-child { border-bottom: none; }

    .capability-name {
      color: var(--text);
      font-weight: 500;
    }
    .capability-exists {
      text-align: center;
      font-size: 0.85rem;
      font-weight: 600;
      min-width: 50px;
    }
    .capability-exists.yes { color: var(--green); }
    .capability-exists.no { color: var(--red); }
    .capability-who {
      color: var(--text-muted);
      font-size: 0.9rem;
    }

    /* Assessment Box */
    .assessment {
      background: var(--bg-card);
      border: 1px solid var(--border);
      border-radius: 8px;
      padding: 1.5rem;
      margin: 2rem 0;
    }
    .assessment-title {
      font-size: 1.1rem;
      font-weight: 600;
      margin-bottom: 1rem;
      color: var(--yellow);
    }
    .assessment-item {
      margin: 0.75rem 0;
      padding-left: 1.5rem;
      position: relative;
      color: var(--text-dim);
    }
    .assessment-item::before {
      content: attr(data-num);
      position: absolute;
      left: 0;
      color: var(--text-muted);
      font-size: 0.85rem;
    }
    .assessment-item strong {
      color: var(--text);
    }

    /* Context box */
    .context {
      background: var(--bg-elevated);
      border: 1px solid var(--border);
      border-radius: 8px;
      padding: 1.25rem;
      margin-bottom: 2rem;
    }
    .context-row {
      display: flex;
      margin: 0.4rem 0;
    }
    .context-key {
      color: var(--text-muted);
      min-width: 140px;
      font-size: 0.9rem;
    }
    .context-value {
      color: var(--text);
      font-size: 0.9rem;
    }

    /* Code/scenario blocks */
    code {
      background: var(--bg-elevated);
      padding: 0.15rem 0.4rem;
      border-radius: 4px;
      font-family: 'SF Mono', Monaco, monospace;
      font-size: 0.9em;
      color: var(--blue);
    }

    .scenario {
      background: var(--bg);
      border: 1px solid var(--border);
      border-radius: 6px;
      padding: 1rem;
      margin: 0.75rem 0;
      font-family: 'SF Mono', Monaco, monospace;
      font-size: 0.8rem;
      line-height: 1.6;
      color: var(--text-muted);
      white-space: pre-wrap;
      overflow-x: auto;
    }
  </style>
</head>
<body>

<div class="container">

  <h1>Competitive Landscape Analysis</h1>
  <p class="subtitle">8 tools analyzed for hybrid Claude + Gemini orchestration</p>

  <div class="context">
    <div class="context-row">
      <span class="context-key">Problem</span>
      <span class="context-value">85% of Claude Max rate limits hit in 1-2 days</span>
    </div>
    <div class="context-row">
      <span class="context-key">Proposed Solution</span>
      <span class="context-value">Invisible middleware routing between Claude + Gemini</span>
    </div>
    <div class="context-row">
      <span class="context-key">Key Question</span>
      <span class="context-value">Is the ~40% gap between existing tools and the vision worth building?</span>
    </div>
  </div>

  <!-- TERMINOLOGY SECTION -->
  <div class="tool" style="margin-top: 2rem; border-color: var(--purple);">
    <div class="tool-title">
      <span class="tool-name">What Is An Agent Harness?</span>
    </div>

    <p class="tool-desc">
      The 2026 industry consensus (Philipp Schmid/HuggingFace, Sequoia Capital, Aakash Gupta) is that <strong>"2026 Is Agent Harnesses"</strong> — the harness, not the model, is where differentiation happens. Manus rewrote their harness 5 times with the same underlying models; each rewrite improved reliability. The infrastructure matters more than the weights.
    </p>

    <div class="section-label" style="color: var(--purple);">Industry Definition</div>
    <div class="section-content" style="border-color: rgba(163,113,247,0.4);">
      <p style="margin-bottom: 0.75rem;">An <strong>agent harness</strong> is the complete architectural system surrounding an LLM that manages what the model can't do itself:</p>
      <ul>
        <li><strong>Execute tool calls</strong> — Model outputs intent, harness performs the action</li>
        <li><strong>Manage memory</strong> — Persist context beyond conversation window</li>
        <li><strong>Structure workflows</strong> — Coordinate multi-step operations</li>
        <li><strong>Handle long-running tasks</strong> — Checkpointing, recovery, continuation</li>
        <li><strong>Enforce rules</strong> — Guardrails, permissions, validation</li>
      </ul>
      <p style="margin-top: 0.75rem; color: var(--text-muted);">The harness is scaffolding — not the model, not the business logic, but the infrastructure that makes agents reliable.</p>
    </div>

    <div class="section-label" style="color: var(--yellow);">Your Concept: A Hybrid</div>
    <div class="section-content" style="border-color: rgba(210,153,34,0.4);">
      <p style="margin-bottom: 0.75rem;">What you're building is <strong>more than a harness</strong> — it combines three architectural components:</p>
      <ul>
        <li><strong>Harness component</strong> — Memory management, tool execution, context persistence across sessions</li>
        <li><strong>Orchestrator component</strong> — Routing logic deciding Claude vs Gemini per task based on semantics</li>
        <li><strong>Proxy/Router component</strong> — Intercepts requests, directs traffic, handles failover</li>
      </ul>
      <p style="margin-top: 0.75rem; color: var(--text-muted);">If you only call it a "harness," people expect infrastructure for ONE model. Your vision includes orchestration BETWEEN models — deciding which chef handles which dish, sharing prep work between kitchens.</p>
    </div>

    <div class="section-label" style="color: var(--blue);">Better Terminology Options</div>
    <div class="section-content" style="border-color: rgba(88,166,255,0.4);">
      <ul>
        <li><strong>Agent Orchestration Layer</strong> — Clearest, industry-standard. Emphasizes coordination between providers.</li>
        <li><strong>Multi-Model Harness</strong> — Extends the harness definition to explicitly cover cross-provider work.</li>
        <li><strong>Agentic Router</strong> — If emphasizing the routing decision logic over infrastructure.</li>
      </ul>
    </div>

    <div class="section-label" style="color: var(--text-dim);">Why Existing Tools Aren't This</div>
    <div class="section-content" style="border-color: var(--border);">
      <p style="margin-bottom: 0.75rem;">The 8 tools analyzed each handle ONE piece:</p>
      <ul>
        <li><strong>CCProxy, Portkey, OpenRouter</strong> — Proxies/gateways that route, but don't manage state</li>
        <li><strong>claude-code-mux</strong> — Failover proxy, but loses reasoning state during handoff</li>
        <li><strong>PAL MCP</strong> — Delegation tool, but no persistent memory or automatic routing</li>
        <li><strong>hcom</strong> — Inter-agent messaging, but passive (tells what happened, not what should happen)</li>
        <li><strong>Conductor Build</strong> — Parallel execution, but Claude-only, no cross-provider orchestration</li>
        <li><strong>CLIProxyAPI</strong> — Quota visibility, but display-only, doesn't act on thresholds</li>
      </ul>
      <p style="margin-top: 0.75rem; color: var(--text-muted);"><strong>None combine:</strong> semantic routing + persistent memory + proactive rate prediction + mid-task context handoff. That's the ~40% gap.</p>
    </div>

    <div class="section-label" style="color: var(--blue);">User Journey: What Might Change (Speculative)</div>
    <div class="section-content" style="border-color: rgba(88,166,255,0.4);">
      <p style="margin-bottom: 0.75rem;"><strong>Today's mental model:</strong> You manage the orchestration. You track rate limits, decide when to delegate, copy context between sessions, and choose which provider handles which task.</p>

      <div class="journey-contrast">
        <div class="journey-contrast-item current">
          <div class="label">Current Reality</div>
          <ul style="margin: 0; padding-left: 1rem; color: var(--text-dim); font-size: 0.85rem;">
            <li>You watch CodexBar: "Claude 82%"</li>
            <li>You decide: "This research can go to Gemini"</li>
            <li>You invoke clink manually</li>
            <li>Tomorrow, you copy findings to new session</li>
          </ul>
        </div>
        <div class="journey-contrast-item hypothetical">
          <div class="label">Hypothetical Harness</div>
          <ul style="margin: 0; padding-left: 1rem; color: var(--text-dim); font-size: 0.85rem;">
            <li>System sees: "Claude 82%, task is research"</li>
            <li>System routes to Gemini automatically</li>
            <li>You see routing decision in ambient log</li>
            <li>Tomorrow, memory auto-injects context</li>
          </ul>
        </div>
      </div>

      <p style="margin-top: 0.75rem;"><strong>What shifts TO the system:</strong> Rate limit tracking, routing decisions, context persistence, provider selection.</p>
      <p><strong>What stays WITH you:</strong> High-level task decomposition, veto authority over routing, approval of plans, understanding why decisions were made.</p>
      <p><strong>New responsibilities:</strong> Trusting routing you didn't choose. Debugging when delegation fails. Understanding why the system chose Gemini over Claude. Handling conflicts if memory is stale.</p>
      <p style="color: var(--text-muted); margin-top: 0.75rem;"><strong>Honest uncertainty:</strong> We don't know if this is worth building until we test the baseline. Does automatic routing actually save enough quota to justify the complexity? Does the mental model shift feel like relief or loss of control?</p>
    </div>
  </div>

  <!-- ROUTING/PROXY LAYER -->
  <div class="category">
    <div class="category-name">Routing / Proxy Layer</div>
    <div class="category-desc">Tools that sit between your client and AI providers, intercepting and routing requests</div>
  </div>

  <div class="tool">
    <div class="tool-title">
      <span class="tool-name">CCProxy</span>
      <span class="tool-badge high">~65%</span>
    </div>
    <p class="tool-desc">
      Python-based proxy using LiteLLM that intercepts Claude Code API requests. Routes based on 4 rule types: <code>TokenCountRule</code> (if tokens > threshold), <code>MatchModelRule</code> (model name matching), <code>ThinkingRule</code> (thinking parameter presence), and <code>MatchToolRule</code> (tool invocation detection). Design philosophy is zero overhead, transparent proxy — rules evaluate in &lt;1ms.
    </p>

    <div class="section-label works">What It Does Well</div>
    <div class="section-content works">
      <ul>
        <li>Token-based routing works perfectly for large context scenarios — request with 180K tokens automatically routes to Gemini's 1M context window</li>
        <li>Tool-based routing enables WebSearch → Perplexity scenarios without manual intervention</li>
        <li>Zero latency overhead since rules are simple conditionals, not model calls</li>
        <li>Works with Claude Max subscriptions (OAuth compatible)</li>
        <li>Transparent to Claude Code — it doesn't know the proxy exists</li>
      </ul>
    </div>

    <div class="section-label why">Why It Works This Way</div>
    <div class="section-content why">
      <ul>
        <li><strong>Architectural constraint:</strong> A proxy sees the request envelope (tokens, model name, tools), not the semantic intent inside. It operates at the HTTP boundary.</li>
        <li><strong>Speed over intelligence:</strong> Rules evaluate in &lt;1ms. Adding semantic classification would add 50-200ms latency per request — that's no longer a "transparent proxy."</li>
        <li><strong>Simplicity over accuracy:</strong> 4 clear rule types vs. a classifier that needs training data, maintenance, and can misroute 10-15% of requests unpredictably.</li>
        <li><strong>Fails predictably:</strong> If token-count routing is wrong, it's wrong consistently. Classifier errors are unpredictable and harder to debug.</li>
        <li><strong>This is a valid trade-off:</strong> CCProxy chose transparency and speed. Not every tool needs to be intelligent — some should just be fast and reliable.</li>
      </ul>
    </div>

    <div class="section-label breaks">What Breaks</div>
    <div class="section-content breaks">
      <ul>
        <li>Cannot understand task semantics — "Research JWT vulnerabilities" and "Debug auth.ts" both look like ~10 tokens with no tools, route identically</li>
        <li>No rate limit awareness — doesn't know you're at 85% Claude quota, routes next request to Claude anyway</li>
        <li>No failover — if Anthropic fails, request fails. No retry logic.</li>
        <li>No shared state between requests — each request is independent, no learning from patterns</li>
        <li>Mixed-intent requests route entirely to one provider — "Research X then code Y" goes 100% to Claude, wasting quota on research</li>
      </ul>
    </div>

    <div class="section-label gap">The Gap</div>
    <div class="section-content gap">
      <ul>
        <li><strong>Missing capability:</strong> Task-intent classification — distinguishing research from coding from debugging based on the prompt content</li>
        <li><strong>Missing capability:</strong> Rate limit awareness — knowing quota state and routing accordingly</li>
        <li><strong>Open question:</strong> Is adding 50-200ms latency for semantic routing worth the quota savings? Depends on how often misrouting actually happens in your workflow.</li>
        <li><strong>Open question:</strong> Could heuristics (keyword matching) get 80% of semantic routing value without the latency cost?</li>
      </ul>
    </div>

    <div class="section-label journey">User Journey</div>
    <div class="section-content journey">
      <p style="margin-bottom: 0.75rem;"><strong>Mental model:</strong> CCProxy is a fast, transparent gatekeeper. It's not intelligent — it's predictable. You understand what it can see (tokens, tools, model name) and what it can't (intent, semantics, task type).</p>

      <div class="journey-scenario">
        <p><span class="time">9:47 AM</span> <span class="action">— You configured CCProxy: "if tokens > 100K, route to Gemini." You type <code>claude</code> normally. CCProxy sits at localhost:4000, invisible.</span></p>
        <p><span class="time">9:50 AM</span> <span class="action">— You ask: "Research latest XSS attack vectors and fix auth.ts." CCProxy sees: 12 tokens, no tools. Routes to Claude (default). You realize: this is research, but CCProxy can't tell. You accept the trade-off — research runs on Claude.</span></p>
        <p><span class="time">10:15 AM</span> <span class="action">— Research done (15K tokens). You pivot to coding. CCProxy still under 100K threshold. No routing change. The research→coding semantic boundary was invisible to it.</span></p>
      </div>

      <p style="margin-top: 0.75rem;"><strong>Division of responsibility:</strong></p>
      <ul>
        <li><strong>CCProxy handles:</strong> Exact token counting, &lt;1ms rule evaluation, forwarding to correct endpoint</li>
        <li><strong>You handle:</strong> Understanding your task is research vs coding, knowing when the threshold should have triggered, recognizing token count ≠ semantic complexity</li>
      </ul>

      <div class="journey-contrast">
        <div class="journey-contrast-item current">
          <div class="label">Without CCProxy</div>
          <span style="color: var(--text-dim); font-size: 0.85rem;">Manually stop Claude, start Gemini for large analysis, copy-paste context between CLI instances.</span>
        </div>
        <div class="journey-contrast-item hypothetical">
          <div class="label">With Orchestration Layer</div>
          <span style="color: var(--text-dim); font-size: 0.85rem;">System detects "research task" → routes to Gemini. Detects "coding phase" → switches back to Claude mid-conversation.</span>
        </div>
      </div>

      <p style="margin-top: 0.75rem; color: var(--text-muted);"><strong>Workflow fit:</strong> Token-aware load distribution when you accept manual decomposition of semantic intent. Works beautifully for "large audits → Gemini, code gen → Claude." Less useful for the research-then-code pattern where the semantic boundary matters more than token count.</p>
    </div>
  </div>

  <div class="tool">
    <div class="tool-title">
      <span class="tool-name">claude-code-mux</span>
      <span class="tool-badge high">~60%</span>
    </div>
    <p class="tool-desc">
      Rust-based HTTP proxy providing automatic failover between AI providers. When Anthropic returns 429, it immediately tries your backup providers (OpenRouter, Vertex AI, etc.). Supports 18+ providers, OAuth authentication, &lt;1ms routing overhead, ~5MB memory footprint. Designed as a stateless proxy — each request is independent.
    </p>

    <div class="section-label works">What It Does Well</div>
    <div class="section-content works">
      <ul>
        <li>Seamless failover on 429 errors — you send a request, Anthropic fails, mux catches it, tries OpenRouter, returns response. You never know the switch happened.</li>
        <li>Priority-based provider configuration — set Anthropic as priority 1, OpenRouter as priority 2, Vertex as priority 3</li>
        <li>OAuth authentication support — works with Claude Max subscriptions</li>
        <li>Minimal resource usage — Rust binary, &lt;1ms routing, ~5MB memory</li>
        <li>Fault isolation — one bad request doesn't affect subsequent requests</li>
      </ul>
    </div>

    <div class="section-label why">Why It Works This Way</div>
    <div class="section-content why">
      <ul>
        <li><strong>Stateless by design:</strong> Each request is independent. No database, no storage, no persistence. This means zero infrastructure requirements and no state corruption risks.</li>
        <li><strong>Proxy, not orchestrator:</strong> Adding memory would change the tool's nature entirely. It would need a database, state extraction logic, injection logic — a different product.</li>
        <li><strong>The hard problem:</strong> Extracting "reasoning state" from Claude's session is non-trivial. You can't just dump the conversation — Claude's internal hypothesis chain isn't in the text.</li>
        <li><strong>Failure mode simplicity:</strong> Stateless means easy debugging (each request reproducible), horizontal scaling (spin up more proxies), and crash recovery (restart, zero data loss).</li>
        <li><strong>This is a valid trade-off:</strong> For a failover proxy, reliability beats intelligence. Memory adds complexity and new failure modes.</li>
      </ul>
    </div>

    <div class="section-label breaks">What Breaks</div>
    <div class="section-content breaks">
      <ul>
        <li><strong>Context continuity breaks during failover.</strong> Turn 1-5: debugging a race condition, Claude eliminated 2 hypotheses, testing the third. Turn 6: 429, switches to OpenRouter. Fresh instance re-reads messages, might re-investigate eliminated hypotheses. You lose 5-10 minutes.</li>
        <li>No semantic routing — doesn't understand task type, just priority-based failover</li>
        <li>No rate limit prediction — reacts to 429 after it happens, doesn't predict or prevent it</li>
        <li>Message history preserved, reasoning state lost — the text is there but the "mental model" isn't</li>
      </ul>
    </div>

    <div class="section-label gap">The Gap</div>
    <div class="section-content gap">
      <ul>
        <li><strong>Missing capability:</strong> Reasoning state preservation during provider handoff — some way to capture "where Claude was" in its debugging process</li>
        <li><strong>Missing capability:</strong> Proactive rate prediction — routing before hitting limits, not after</li>
        <li><strong>Open question:</strong> How do you capture reasoning state? Is it in the message history (could be summarized)? Or is it truly internal to the inference session (fundamentally lost)?</li>
        <li><strong>Open question:</strong> Would a simple "here's what we've tried so far" summary injected into the new session be enough? Or do you need something more sophisticated?</li>
      </ul>
    </div>

    <div class="section-label journey">User Journey</div>
    <div class="section-content journey">
      <p style="margin-bottom: 0.75rem;"><strong>Mental model:</strong> claude-code-mux is a stateless message router. It preserves your conversation text but not the model's reasoning state. When failover happens, the new instance re-reads messages but builds its own understanding from scratch.</p>

      <div class="journey-scenario">
        <p><span class="time">10:00 AM</span> <span class="action">— You're 25 minutes into debugging a race condition. Claude has eliminated 2 hypotheses, is testing a third involving async initialization order.</span></p>
        <p><span class="time">10:25 AM</span> <span class="action">— Turn 6: You ask "Can you trace the initialization sequence?" Anthropic returns 429. Mux catches it, switches to OpenRouter (3-5 second delay).</span></p>
        <p><span class="time">10:26 AM</span> <span class="action">— OpenRouter Claude receives all 6 turns of message history. But it's a fresh instance — no memory of the hypothesis chain. It might re-investigate the eliminated hypotheses. You notice the response feels like it's "starting over."</span></p>
        <p><span class="time">10:35 AM</span> <span class="action">— You manually re-orient: "We already ruled out timing and lock scope. Focus on async init." The debugging continues, but you lost ~10 minutes.</span></p>
      </div>

      <p style="margin-top: 0.75rem;"><strong>Division of responsibility:</strong></p>
      <ul>
        <li><strong>Mux handles:</strong> 429 detection, automatic failover, provider priority, message forwarding</li>
        <li><strong>You handle:</strong> Recognizing when failover happened, validating response quality, re-orienting the new instance if needed, deciding when to switch back</li>
      </ul>

      <div class="journey-contrast">
        <div class="journey-contrast-item current">
          <div class="label">Without mux</div>
          <span style="color: var(--text-dim); font-size: 0.85rem;">429 error. You manually open Gemini, copy conversation, paste, explain "we were debugging X." 3-5 minute interruption.</span>
        </div>
        <div class="journey-contrast-item hypothetical">
          <div class="label">With Persistent Memory</div>
          <span style="color: var(--text-dim); font-size: 0.85rem;">Mux fails over + injects: "Prior reasoning: eliminated hypotheses A, B. Testing C." New instance continues from the exact point.</span>
        </div>
      </div>

      <p style="margin-top: 0.75rem; color: var(--text-muted);"><strong>Workflow fit:</strong> Short, stateless requests where reasoning continuity doesn't matter. Less suited for long debugging chains where the model's "mental model" is critical to progress.</p>
    </div>
  </div>

  <div class="tool">
    <div class="tool-title">
      <span class="tool-name">Portkey</span>
      <span class="tool-badge low">~30%</span>
    </div>
    <p class="tool-desc">
      Cloud AI gateway for enterprise use. Every request goes through Portkey's servers where it can be routed based on metadata, logged, cached, and monitored. Supports complex conditional routing rules, excellent observability (costs, latency, tokens), request caching, failover/retries, and per-team spending limits. SOC 2 compliant.
    </p>

    <div class="section-label works">What It Does Well</div>
    <div class="section-content works">
      <ul>
        <li>Metadata-based conditional routing — <code>IF metadata.task_type == "research" THEN route to Gemini</code></li>
        <li>Excellent observability — every request logged with costs, latency, token counts</li>
        <li>Caching — identical requests return cached responses, saves money</li>
        <li>Failover and retries — if Provider A fails, try Provider B</li>
        <li>Enterprise features — audit logs, cost controls per team/project, SOC 2 compliance</li>
      </ul>
    </div>

    <div class="section-label why">Why It Works This Way</div>
    <div class="section-content why">
      <ul>
        <li><strong>Gateway principle:</strong> The routing layer shouldn't own business logic. Your application understands your tasks better than a generic gateway ever could.</li>
        <li><strong>Separation of concerns:</strong> Portkey's job is to forward, log, and cache. Classification is YOUR responsibility — you know what "research" means in your context.</li>
        <li><strong>Flexibility:</strong> You can use ANY classifier upstream (keyword matching, embeddings, LLM calls, heuristics). Portkey doesn't constrain your approach.</li>
        <li><strong>Stateless simplicity:</strong> Gateway stays simple, fast, and reliable. Complexity lives upstream where it belongs.</li>
        <li><strong>API keys by design:</strong> Enterprise gateways work with API keys because that's how providers bill enterprises. Subscription OAuth is a consumer pattern.</li>
      </ul>
    </div>

    <div class="section-label breaks">What Breaks</div>
    <div class="section-content breaks">
      <ul>
        <li><strong>Cannot use Claude Max subscriptions.</strong> Anthropic banned third-party OAuth access in Jan 2026. Portkey requires API keys. You'd pay for Max AND API credits — double cost.</li>
        <li>No semantic understanding — routes based on metadata YOU provide. "Upstream logic" problem: YOU classify, then tell Portkey via metadata.</li>
        <li>Stateless — no memory between requests, no context persistence</li>
        <li>Cloud dependency — every request routes through Portkey's servers, adding latency and requiring trust</li>
      </ul>
    </div>

    <div class="section-label gap">The Gap</div>
    <div class="section-content gap">
      <ul>
        <li><strong>Missing capability:</strong> Automatic semantic classification — something needs to decide "this is research" without requiring your code to do it</li>
        <li><strong>Missing capability:</strong> Subscription authentication — using Max/$200mo through routing infrastructure</li>
        <li><strong>Open question:</strong> Should the classifier live in the gateway (simpler for user, less flexible) or upstream (more flexible, more work for user)?</li>
        <li><strong>Open question:</strong> Is Portkey's enterprise focus incompatible with individual Max subscriptions, or is there a bridge?</li>
      </ul>
    </div>

    <div class="section-label journey">User Journey</div>
    <div class="section-content journey">
      <p style="margin-bottom: 0.75rem;"><strong>Mental model:</strong> "I am the classifier; Portkey is the enforcer." You determine task type upstream, attach it as metadata, and Portkey routes based on your classification. The gateway doesn't understand your task — it trusts your labels.</p>

      <div class="journey-scenario">
        <p><span class="time">Setup</span> <span class="action">— You build a wrapper that classifies tasks. When you type "research JWT security," your code tags it <code>metadata.task_type = "research"</code> before calling Portkey.</span></p>
        <p><span class="time">10:00 AM</span> <span class="action">— You debug OAuth implementation. Your classifier sees "debug" keyword, tags it <code>task_type = "coding"</code>. Portkey routes to Claude.</span></p>
        <p><span class="time">10:30 AM</span> <span class="action">— You ask "What's the latest OAuth 2.1 spec?" Your classifier tags <code>"research"</code>. Portkey routes to Gemini. But wait — you're using Claude Max, which Portkey can't use. You'd need a separate Gemini API key.</span></p>
      </div>

      <p style="margin-top: 0.75rem;"><strong>Division of responsibility:</strong></p>
      <ul>
        <li><strong>Portkey handles:</strong> Conditional routing based on metadata, logging, caching, failover, cost tracking</li>
        <li><strong>You handle:</strong> Building the classifier, maintaining it, handling edge cases where classification is wrong, managing separate API keys for each provider</li>
      </ul>

      <p style="margin-top: 0.75rem; color: var(--text-muted);"><strong>Workflow fit:</strong> Enterprise teams with API budgets who want observability and conditional routing. Less suited for individual Max subscribers — the OAuth incompatibility means double billing.</p>
    </div>
  </div>

  <div class="tool">
    <div class="tool-title">
      <span class="tool-name">OpenRouter</span>
      <span class="tool-badge low">~25%</span>
    </div>
    <p class="tool-desc">
      API gateway providing unified access to 300+ AI models from 60+ providers through a single endpoint. One integration, all models — call <code>openrouter.ai/api/v1/messages</code> with any model name (Claude, GPT-4, Gemini, Llama, Mistral). Has "Auto Router" feature powered by NotDiamond that learns which models perform best for task types. BYOK option available with 5% fee.
    </p>

    <div class="section-label works">What It Does Well</div>
    <div class="section-content works">
      <ul>
        <li>Single endpoint for 300+ models — one integration, all providers</li>
        <li>Easy model switching — just change the model name in the API call</li>
        <li>Auto Router — learned routing powered by NotDiamond, picks optimal model per task</li>
        <li>Cost comparison — see pricing across all models in one place</li>
        <li>Fallback support — if model A fails, try model B</li>
        <li>BYOK option — use your own API keys (with 5% fee)</li>
      </ul>
    </div>

    <div class="section-label why">Why It Works This Way</div>
    <div class="section-content why">
      <ul>
        <li><strong>Unified interface principle:</strong> One endpoint simplifies integration. You don't need 60 SDKs or manage 60 API keys.</li>
        <li><strong>Explicit over implicit:</strong> Auto Router doesn't auto-activate. YOU decide when to use it. This gives control to the user.</li>
        <li><strong>Learning from YOUR patterns:</strong> Auto Router needs 15+ examples to learn what works FOR YOU. Generic routing wouldn't match your specific task distribution.</li>
        <li><strong>Avoiding over-fitting:</strong> Not every team uses models the same way. The training requirement ensures personalization rather than generic one-size-fits-all routing.</li>
        <li><strong>BYOK flexibility:</strong> Some users want to pay per-token through OpenRouter, others want to use their existing API keys. Both options exist.</li>
      </ul>
    </div>

    <div class="section-label breaks">What Breaks</div>
    <div class="section-content breaks">
      <ul>
        <li><strong>Cannot use Claude Max subscriptions.</strong> Same OAuth problem as Portkey — requires API keys, not subscription auth.</li>
        <li>Auto Router requires 15+ labeled training examples. Cold start problem — you need to manually label tasks before it helps.</li>
        <li>Claude Code hardcodes model selection — Auto Router feature not exposed to CLI tools like Claude Code</li>
        <li>Stateless — no shared memory between requests, no context persistence</li>
        <li>5% BYOK fee adds cost on top of provider pricing</li>
      </ul>
    </div>

    <div class="section-label gap">The Gap</div>
    <div class="section-content gap">
      <ul>
        <li><strong>Missing capability:</strong> Zero-config routing — something that works out of the box without 15+ training examples</li>
        <li><strong>Missing capability:</strong> Integration with Claude Code — exposing routing decisions to CLI tools</li>
        <li><strong>Open question:</strong> Is the 15-example training requirement actually a feature (personalization) or a friction point? Would generic defaults be good enough?</li>
        <li><strong>Open question:</strong> Could heuristics bootstrap Auto Router faster than manual labeling?</li>
      </ul>
    </div>

    <div class="section-label journey">User Journey</div>
    <div class="section-content journey">
      <p style="margin-bottom: 0.75rem;"><strong>Mental model:</strong> "One integration, infinite model access." You call one endpoint, specify any model name, and OpenRouter handles the provider complexity. Auto Router learns your patterns — but only after you teach it.</p>

      <div class="journey-scenario">
        <p><span class="time">Week 1</span> <span class="action">— You integrate OpenRouter. You manually pick models: <code>model: "anthropic/claude-3-opus"</code> for coding, <code>model: "google/gemini-pro"</code> for research. Auto Router is available but needs 15+ labeled examples to learn.</span></p>
        <p><span class="time">Week 2</span> <span class="action">— You label 20 past requests with outcomes. Auto Router starts suggesting: "For prompts like this, Gemini performs 12% better." But Claude Code hardcodes model selection — you can't use Auto Router from the CLI.</span></p>
        <p><span class="time">Week 3</span> <span class="action">— You want to use Claude Max ($200/mo) through OpenRouter to get routing benefits. Problem: OpenRouter needs API keys, Max uses OAuth. You'd need separate API credits — defeating the purpose.</span></p>
      </div>

      <p style="margin-top: 0.75rem;"><strong>Division of responsibility:</strong></p>
      <ul>
        <li><strong>OpenRouter handles:</strong> Unified API, model switching, fallback, cost comparison, Auto Router (after training)</li>
        <li><strong>You handle:</strong> Providing 15+ training examples, managing API keys, accepting that Max subscriptions don't work here</li>
      </ul>

      <p style="margin-top: 0.75rem; color: var(--text-muted);"><strong>Workflow fit:</strong> API-first developers building apps who want model flexibility. Less suited for Claude Code power users on Max subscriptions — the integration path doesn't exist.</p>
    </div>
  </div>

  <!-- DELEGATION LAYER -->
  <div class="category">
    <div class="category-name">Delegation / MCP Layer</div>
    <div class="category-desc">Tools that let Claude delegate tasks to other AI models</div>
  </div>

  <div class="tool">
    <div class="tool-title">
      <span class="tool-name">PAL MCP</span>
      <span class="tool-badge medium">~45%</span>
    </div>
    <p class="tool-desc">
      MCP server (10K+ GitHub stars) that runs inside Claude Code, providing the <code>clink</code> tool ("CLI Link") that spawns external CLI processes. Claude can delegate to Gemini, Codex, or other Claude instances. Supports role-based prompting via pre-configured system prompts, file passing, and structured JSON output. Core idea: Claude Code has limits (200K context, no web search), PAL lets it call models with different strengths.
    </p>

    <div class="section-label works">What It Does Well</div>
    <div class="section-content works">
      <ul>
        <li>Clean delegation interface — <code>clink(cli_name="gemini", role="security-analyzer", prompt="Analyze auth.ts")</code></li>
        <li>Role-based prompting — pre-configured system prompts for different task types load automatically</li>
        <li>Result integration — Gemini's output flows naturally back into Claude's conversation as a tool result</li>
        <li>Multiple CLI support — configure Gemini, Codex, other Claude instances, anything with a CLI</li>
        <li>Delegates large context work to Gemini (1M context) while Claude orchestrates</li>
        <li>Battle-tested, production-ready with 10K+ GitHub stars</li>
      </ul>
    </div>

    <div class="section-label why">Why It Works This Way</div>
    <div class="section-content why">
      <ul>
        <li><strong>Process isolation:</strong> Fresh subprocess = fresh state = no cleanup, no corruption risk. One call's failure doesn't contaminate the next.</li>
        <li><strong>Clarity over magic:</strong> Users know exactly what Gemini sees (the prompt you sent). No hidden context injection surprises.</li>
        <li><strong>Manual control:</strong> YOU decide when to delegate via explicit clink calls. This avoids classification errors where the system misroutes your task.</li>
        <li><strong>Debuggability:</strong> "Show me what PAL sent to Gemini" is straightforward. With automatic routing, debugging "why did it choose Gemini?" is harder.</li>
        <li><strong>MCP scope:</strong> PAL is a delegation tool, not an orchestration layer. Adding persistent memory would expand scope significantly.</li>
      </ul>
    </div>

    <div class="section-label breaks">What Breaks</div>
    <div class="section-content breaks">
      <ul>
        <li><strong>No persistent memory across sessions.</strong> Day 1: Gemini finds JWT bug. Day 2: PAL spawns NEW Gemini with ZERO memory. Re-analyzes from scratch, might find different things.</li>
        <li>Delegation is MANUAL — no automatic routing. You explicitly call clink or write CLAUDE.md rules.</li>
        <li>25K token output limit — MCP tool responses capped, large analyses get truncated</li>
        <li>2-minute timeout — long-running Gemini tasks may timeout</li>
        <li>No rate limit awareness — doesn't know if Claude/Gemini approaching limits</li>
        <li>Context passed, not shared — Gemini sees files you specify, not Claude's reasoning state</li>
      </ul>
    </div>

    <div class="section-label gap">The Gap</div>
    <div class="section-content gap">
      <ul>
        <li><strong>Missing capability:</strong> Cross-session memory — Day 2 should know what Day 1 discovered without manual copy-paste</li>
        <li><strong>Missing capability:</strong> Automatic delegation — routing based on task type without explicit clink calls</li>
        <li><strong>Open question:</strong> Is manual delegation actually a feature (user control) or a friction point? Would automatic routing misroute often enough to be worse?</li>
        <li><strong>Open question:</strong> What's the right memory model? File-based (simple, grep-able) vs. vector DB (semantic search) vs. something else?</li>
      </ul>
    </div>

    <div class="section-label journey">User Journey</div>
    <div class="section-content journey">
      <p style="margin-bottom: 0.75rem;"><strong>Mental model:</strong> Each clink call spawns a fresh process. Claude maintains session continuity; Gemini resets every time. You consciously manage context handoff via files or tool parameters.</p>

      <div class="journey-scenario">
        <p><span class="time">Day 1, 10:00 AM</span> <span class="action">— You ask Claude to analyze 47 auth modules (180K tokens). CLAUDE.md says: "If >100K tokens, delegate." Claude calls <code>clink(cli_name="gemini", prompt="Analyze JWT validation")</code>. Gemini returns findings: "JWT expiration bug in auth.ts:42, missing rate limiting."</span></p>
        <p><span class="time">Day 1, 4:00 PM</span> <span class="action">— Session ends. You save key findings to <code>RESEARCH_FINDINGS.md</code> in your repo. This is manual — PAL doesn't persist Gemini's findings.</span></p>
        <p><span class="time">Day 2, 9:00 AM</span> <span class="action">— New session. You tell Claude: "Read RESEARCH_FINDINGS.md, then continue analysis." Claude has continuity (same orchestrator). But when you call clink again, Gemini is fresh — no memory of Day 1. You pass context explicitly: <code>clink(context="Yesterday found JWT bug...", prompt="Analyze new modules")</code>.</span></p>
      </div>

      <p style="margin-top: 0.75rem;"><strong>Division of responsibility:</strong></p>
      <ul>
        <li><strong>PAL handles:</strong> Spawning CLI processes, passing prompts, returning structured output, role-based system prompts</li>
        <li><strong>You handle:</strong> Deciding when to delegate (explicit clink or CLAUDE.md rules), maintaining cross-session context via files, passing context to fresh Gemini processes</li>
      </ul>

      <div class="journey-contrast">
        <div class="journey-contrast-item current">
          <div class="label">Without PAL MCP</div>
          <span style="color: var(--text-dim); font-size: 0.85rem;">Run <code>gemini -p "task" > result.md</code> manually. Read file back into Claude. Fragmented workflow.</span>
        </div>
        <div class="journey-contrast-item hypothetical">
          <div class="label">With Persistent Memory</div>
          <span style="color: var(--text-dim); font-size: 0.85rem;">Day 2: System queries memory: "What did Gemini find yesterday?" Auto-injects context into fresh process.</span>
        </div>
      </div>

      <p style="margin-top: 0.75rem; color: var(--text-muted);"><strong>Workflow fit:</strong> Clear, explicit control over delegation. You see exactly when Gemini is invoked and what context it receives. The trade-off: you manage continuity yourself via files and explicit context passing.</p>
    </div>
  </div>

  <!-- INTER-AGENT COMMUNICATION -->
  <div class="category">
    <div class="category-name">Inter-Agent Communication</div>
    <div class="category-desc">Tools that let multiple AI agents communicate with each other</div>
  </div>

  <div class="tool">
    <div class="tool-title">
      <span class="tool-name">hcom</span>
      <span class="tool-badge low">~35%</span>
    </div>
    <p class="tool-desc">
      Hook-based messaging layer using SQLite as a message bus. When Claude writes a file, a hook posts to SQLite. Gemini's hook polls SQLite and sees the event. Enables real-time inter-agent awareness through Claude Code's native hook system. Messages are structured event notifications (file_write, review_comment), not text chat.
    </p>

    <div class="section-label works">What It Does Well</div>
    <div class="section-content works">
      <ul>
        <li>Real-time inter-agent awareness — Claude writes rateLimit.ts → hook posts to SQLite → Gemini polls → reviews file → posts findings → Claude sees feedback. ~100 second loop.</li>
        <li>Collision detection — both agents notified if they touch same file</li>
        <li>Persistent event log — SQLite preserves history across polling gaps</li>
        <li>Native integration — works with Claude Code's existing hook system, no new infrastructure</li>
        <li>Lightweight — just SQLite + hook scripts</li>
        <li>Coordination without explicit orchestration — agents can react to each other without a central controller</li>
      </ul>
    </div>

    <div class="section-label why">Why It Works This Way</div>
    <div class="section-content why">
      <ul>
        <li><strong>Lightweight over heavyweight:</strong> SQLite + hooks is minimal infrastructure. No central orchestrator to run, maintain, or become a single point of failure.</li>
        <li><strong>No single point of failure:</strong> If the message bus goes down, agents still run independently. They just lose awareness, not functionality.</li>
        <li><strong>Agent autonomy:</strong> Agents decide how to react to messages. The bus doesn't prescribe behavior — it just broadcasts events.</li>
        <li><strong>Messaging ≠ Routing:</strong> hcom answers "what happened?" not "who should do this?" That's a deliberate scope boundary.</li>
        <li><strong>Native to Claude Code:</strong> Uses existing hook system. No new concepts to learn, no foreign infrastructure.</li>
      </ul>
    </div>

    <div class="section-label breaks">What Breaks</div>
    <div class="section-content breaks">
      <ul>
        <li><strong>Messaging only, NOT routing.</strong> hcom tells agents what happened, not what SHOULD happen. "Claude wrote auth.ts" is information, not a decision.</li>
        <li>Collision detected, not prevented — both agents edit same file, THEN see the collision. No locking before edit.</li>
        <li>No automatic context injection — messages are raw events, not synthesized actionable summaries</li>
        <li>Requires both agents running — Gemini must be active to receive messages</li>
        <li>Polling delays — 500ms-2sec gaps between event and reaction</li>
        <li>No failure handling — if Gemini's review times out, messages pile up unprocessed</li>
      </ul>
    </div>

    <div class="section-label gap">The Gap</div>
    <div class="section-content gap">
      <ul>
        <li><strong>Missing capability:</strong> Task routing — something that decides WHO should handle a task, not just announces what happened</li>
        <li><strong>Missing capability:</strong> Collision prevention — locking files before write, not detecting conflicts after</li>
        <li><strong>Open question:</strong> Is centralized routing better than autonomous agents? Central routing is clearer but creates a single point of failure.</li>
        <li><strong>Open question:</strong> Could hcom's event stream be enriched with routing hints without becoming a full orchestrator?</li>
      </ul>
    </div>

    <div class="section-label journey">User Journey</div>
    <div class="section-content journey">
      <p style="margin-bottom: 0.75rem;"><strong>Mental model:</strong> Agents discover each other through async event logs. hcom doesn't tell agents what to DO — it tells them what HAPPENED. Each agent decides how to react.</p>

      <div class="journey-scenario">
        <p><span class="time">10:30:45</span> <span class="action">— Claude writes <code>rateLimit.ts</code>. PostToolUse hook fires, posts to SQLite: <code>{agent: "claude", event: "file_write", file: "rateLimit.ts"}</code>.</span></p>
        <p><span class="time">10:30:48</span> <span class="action">— Gemini's hook polls SQLite (every ~1 sec), sees the event. Based on its configuration, Gemini spawns a review: "Analyze rateLimit.ts for bugs."</span></p>
        <p><span class="time">10:31:15</span> <span class="action">— Gemini finds: race condition line 42, missing error handling line 78. Posts findings back to SQLite.</span></p>
        <p><span class="time">10:31:32</span> <span class="action">— Claude's hook polls, sees Gemini's review. Claude reads feedback, fixes line 42.</span></p>
        <p><span class="time">10:32:00</span> <span class="action">— Meanwhile, you also asked Gemini to add tests. Both Claude and Gemini edit <code>rateLimit.ts</code>. hcom detects the collision AFTER both writes. You resolve the merge conflict manually.</span></p>
      </div>

      <p style="margin-top: 0.75rem;"><strong>Division of responsibility:</strong></p>
      <ul>
        <li><strong>hcom handles:</strong> Broadcasting events, persistent event log, collision detection (after the fact), native hook integration</li>
        <li><strong>You handle:</strong> Agent configuration (what events trigger what actions), resolving collisions, ensuring both agents are running, understanding that hcom is awareness not control</li>
      </ul>

      <p style="margin-top: 0.75rem; color: var(--text-muted);"><strong>Workflow fit:</strong> Parallel agents that need awareness of each other without centralized control. The trade-off: agents are autonomous (no single point of failure) but coordination is implicit (collisions detected, not prevented).</p>
    </div>
  </div>

  <!-- PARALLEL EXECUTION -->
  <div class="category">
    <div class="category-name">Parallel Execution</div>
    <div class="category-desc">Tools that run multiple AI agents simultaneously</div>
  </div>

  <div class="tool">
    <div class="tool-title">
      <span class="tool-name">Conductor Build</span>
      <span class="tool-badge low">~15%</span>
    </div>
    <p class="tool-desc">
      macOS Electron app (YC-backed, by Melty Labs) that runs multiple Claude Code instances in parallel, each isolated in its own git worktree. Visual dashboard monitors all agents simultaneously. Core idea: 5 independent features? Spin up 5 Claude instances, each on a different branch, merge when done. Conductor CREATES agents — it's the controller, not a viewer.
    </p>

    <div class="section-label works">What It Does Well</div>
    <div class="section-content works">
      <ul>
        <li>True parallel execution — 5 agents on 5 CPU cores simultaneously</li>
        <li>Git isolation via worktrees — Agent 1 modifies auth.ts without affecting Agent 2's copy. No merge conflicts during development.</li>
        <li>Visual dashboard — see all agents' progress, terminal output, file changes, diffs at a glance</li>
        <li>Merge coordination — helps combine branches when agents finish</li>
        <li>No manual git commands — worktree management is automatic</li>
        <li>YC-backed, actively developed with real support</li>
      </ul>
    </div>

    <div class="section-label why">Why It Works This Way</div>
    <div class="section-content why">
      <ul>
        <li><strong>Deep integration over broad integration:</strong> Conductor optimizes for Claude's strengths (precision coding, debugging). Supporting multiple models would dilute focus.</li>
        <li><strong>Simplicity of scope:</strong> 5 Claudes is simpler than Claude + Gemini + GPT-4. One model = consistent behavior, cost structure, performance expectations.</li>
        <li><strong>Merge strategy assumes homogeneity:</strong> Git worktree merging works because all agents produce similar code style. Mixed models would complicate merges.</li>
        <li><strong>Rate limit semantics are clear:</strong> One subscription, one pool. Easier to reason about than tracking multiple provider limits.</li>
        <li><strong>Visual-first design:</strong> A dashboard for 5 identical Claude instances is simpler than a dashboard showing heterogeneous model behaviors.</li>
      </ul>
    </div>

    <div class="section-label breaks">What Breaks</div>
    <div class="section-content breaks">
      <ul>
        <li><strong>5 agents = 5x rate limit burn.</strong> All use your single subscription. What lasts a week lasts 1-2 days. Your profile: 85% in 1-2 days. Conductor: 100% in ~8 hours.</li>
        <li>Claude-only — no Gemini, no GPT-4, no model selection for different task types</li>
        <li>No cross-agent communication — agents can't see what each other is doing</li>
        <li>No shared memory — each agent is fully isolated in its worktree</li>
        <li>No semantic task routing — you manually assign tasks to agents</li>
        <li>Silent failures — agents stall on 429 without clear notification</li>
        <li>Conductor solves "5 independent tasks, 48-hour sprint." Your problem is "continuous work across a full week."</li>
      </ul>
    </div>

    <div class="section-label gap">The Gap</div>
    <div class="section-content gap">
      <ul>
        <li><strong>Missing capability:</strong> Multi-provider support — using Claude AND Gemini to spread rate limits across independent pools</li>
        <li><strong>Missing capability:</strong> Rate limit awareness — knowing when to throttle or route to prevent 429s</li>
        <li><strong>Open question:</strong> Is parallel execution the right model for your workflow (burst work) or is sequential with intelligent routing (sustained work) better?</li>
        <li><strong>Open question:</strong> Would cross-agent communication be useful, or is isolation actually a feature (no interference)?</li>
      </ul>
    </div>

    <div class="section-label journey">User Journey</div>
    <div class="section-content journey">
      <p style="margin-bottom: 0.75rem;"><strong>Mental model:</strong> Parallelism via git worktree isolation. Each agent works in its own branch, can't interfere with others. But all 5 agents draw from your single Claude Max subscription — 5x the rate limit burn.</p>

      <div class="journey-scenario">
        <p><span class="time">Monday 9:00 AM</span> <span class="action">— You have 5 independent features to build. You open Conductor, click "Create 5 agents." Each gets its own git worktree: <code>feature-auth</code>, <code>feature-api</code>, <code>feature-tests</code>, etc.</span></p>
        <p><span class="time">Monday 11:00 AM</span> <span class="action">— All 5 agents are working simultaneously. Dashboard shows progress, terminal output, file changes. You're getting 5x the work done per hour.</span></p>
        <p><span class="time">Monday 2:00 PM</span> <span class="action">— Agent 4 stalls silently. You check — it hit 429. Your quota that normally lasts a week is now at 60% after 5 hours. Two more agents start failing.</span></p>
        <p><span class="time">Monday 4:00 PM</span> <span class="action">— Quota exhausted. All agents blocked. You've done 2 days of work in 7 hours, but now you're stuck until the weekly reset.</span></p>
      </div>

      <p style="margin-top: 0.75rem;"><strong>Division of responsibility:</strong></p>
      <ul>
        <li><strong>Conductor handles:</strong> Git worktree creation, agent spawning, visual dashboard, merge coordination</li>
        <li><strong>You handle:</strong> Rate limit pacing (Conductor doesn't throttle), manual task assignment (no semantic routing), recovering from silent 429 failures</li>
      </ul>

      <div class="journey-contrast">
        <div class="journey-contrast-item current">
          <div class="label">Your Profile</div>
          <span style="color: var(--text-dim); font-size: 0.85rem;">Already hit 85% in 1-2 days with 1 agent. Conductor would hit 100% in ~8 hours.</span>
        </div>
        <div class="journey-contrast-item hypothetical">
          <div class="label">With Multi-Provider</div>
          <span style="color: var(--text-dim); font-size: 0.85rem;">2 Claude agents + 3 Gemini agents = two independent quota pools. Sustained work across a week.</span>
        </div>
      </div>

      <p style="margin-top: 0.75rem; color: var(--text-muted);"><strong>Workflow fit:</strong> "5 independent tasks, 48-hour sprint" — burst work with merge at the end. Less suited for "continuous work across a full week" where you need to pace rate limit consumption.</p>
    </div>
  </div>

  <!-- QUOTA MONITORING -->
  <div class="category">
    <div class="category-name">Quota Monitoring</div>
    <div class="category-desc">Tools that track and display rate limit usage</div>
  </div>

  <div class="tool">
    <div class="tool-title">
      <span class="tool-name">CLIProxyAPI / CodexBar</span>
      <span class="tool-badge high">~60%</span>
    </div>
    <p class="tool-desc">
      Local proxy server with macOS menu bar widget showing real-time quota usage across AI providers. Also does round-robin load balancing if you have multiple API keys/accounts. Menu bar displays: Claude 78% | Gemini 15% | $4.23 today. Works with Claude Max subscriptions.
    </p>

    <div class="section-label works">What It Does Well</div>
    <div class="section-content works">
      <ul>
        <li>Glanceable quota visibility — menu bar always shows current usage percentages</li>
        <li>Multi-account load distribution — if you have 3 API keys, spreads load across them (3x effective quota)</li>
        <li>Post-hoc failover — if one account fails, tries the next</li>
        <li>Cost tracking — see total spend per provider per day</li>
        <li>Lightweight — just a menu bar widget, minimal overhead</li>
        <li>Works with Claude Max — tracks subscription usage, not just API</li>
      </ul>
    </div>

    <div class="section-label why">Why It Works This Way</div>
    <div class="section-content why">
      <ul>
        <li><strong>Observability vs control separation:</strong> CLIProxyAPI's job is to show you what's happening. YOUR job is to decide what to do about it.</li>
        <li><strong>User keeps agency:</strong> You see Claude at 85% and decide: "Keep going on this important debug" or "Switch to Gemini for this research." Human judgment.</li>
        <li><strong>Simplicity:</strong> Monitoring is straightforward (intercept, count tokens, display). Decision-making is hard and context-dependent.</li>
        <li><strong>Avoiding over-automation:</strong> Not every context warrants switching. You might WANT to exhaust Claude on critical debugging rather than switching to Gemini.</li>
        <li><strong>Multi-account focus:</strong> Round-robin makes sense for multiple SAME-provider accounts. Cross-provider routing is a different, more complex problem.</li>
      </ul>
    </div>

    <div class="section-label breaks">What Breaks</div>
    <div class="section-content breaks">
      <ul>
        <li><strong>Visibility without agency.</strong> 10:00 AM: Claude at 72%. 10:45 AM: Claude at 85%, still debugging. 11:15 AM: 429. You SAW quota but it didn't ACT. Display only.</li>
        <li>Reactive only — fails over AFTER 429. Request already failed, context may be lost.</li>
        <li>No semantic routing — treats all requests equally regardless of task type</li>
        <li>No proactive thresholds — can't configure "route to Gemini when Claude hits 80%"</li>
        <li>No prediction — shows current state, not "at this rate, 2.3 hours until limit"</li>
        <li>Round-robin loses caching benefits — Anthropic caches per-account, rotating breaks this</li>
      </ul>
    </div>

    <div class="section-label gap">The Gap</div>
    <div class="section-content gap">
      <ul>
        <li><strong>Missing capability:</strong> Proactive threshold routing — acting on quota data before hitting 429, not just displaying it</li>
        <li><strong>Missing capability:</strong> Rate prediction — "at current consumption rate, X hours until limit"</li>
        <li><strong>Open question:</strong> Is automatic routing at thresholds better than human judgment? User might WANT to burn Claude quota on important work.</li>
        <li><strong>Open question:</strong> What's the right threshold? 80%? 90%? Context-dependent? Should user configure it or should the system learn?</li>
      </ul>
    </div>

    <div class="section-label journey">User Journey</div>
    <div class="section-content journey">
      <p style="margin-bottom: 0.75rem;"><strong>Mental model:</strong> Real-time visibility, human decision-making. You see the numbers; you decide what to do. The tool shows you the score but doesn't play the game for you.</p>

      <div class="journey-scenario">
        <p><span class="time">10:00 AM</span> <span class="action">— Menu bar shows: Claude 72% | Gemini 15%. You're debugging a complex auth issue. You glance at the quota and think: "I have headroom, I'll keep going."</span></p>
        <p><span class="time">10:45 AM</span> <span class="action">— Menu bar: Claude 85%. You're close to a breakthrough. You think: "I should switch to Gemini for the research portion... but I don't want to break flow. I'll risk it."</span></p>
        <p><span class="time">11:15 AM</span> <span class="action">— Request fails: 429. Menu bar: Claude 100%. Your debugging session is interrupted. You manually start Gemini, copy context, re-explain the problem. 10 minutes lost.</span></p>
        <p><span class="time">11:25 AM</span> <span class="action">— You realize: "I SAW it at 85%. I could have switched. The tool showed me; I chose not to act." The visibility was there; the agency was yours.</span></p>
      </div>

      <p style="margin-top: 0.75rem;"><strong>Division of responsibility:</strong></p>
      <ul>
        <li><strong>CLIProxyAPI handles:</strong> Intercepting requests, counting tokens, displaying percentages, round-robin across accounts, post-hoc failover</li>
        <li><strong>You handle:</strong> Watching the numbers, deciding when to switch, accepting the consequences of staying on Claude at 85%</li>
      </ul>

      <div class="journey-contrast">
        <div class="journey-contrast-item current">
          <div class="label">Display-Only</div>
          <span style="color: var(--text-dim); font-size: 0.85rem;">You see "Claude 85%." You decide to keep going. You hit 429. That's your call — CLIProxyAPI showed you the data.</span>
        </div>
        <div class="journey-contrast-item hypothetical">
          <div class="label">With Proactive Routing</div>
          <span style="color: var(--text-dim); font-size: 0.85rem;">System sees 82%, sees next task is research. Routes to Gemini automatically. You keep debugging on Claude uninterrupted.</span>
        </div>
      </div>

      <p style="margin-top: 0.75rem; color: var(--text-muted);"><strong>Workflow fit:</strong> Users who want visibility but retain decision authority. The trade-off: you're informed but responsible. If you burn out at 85% on purpose because the work was worth it — that's a valid choice this tool respects.</p>
    </div>
  </div>

  <hr class="divider">

  <!-- SUMMARY TABLE -->
  <div class="summary-section">
    <div class="summary-title">Capability Matrix: What Exists Today?</div>

    <div class="capability-row">
      <span class="capability-name">Route by token count/metadata</span>
      <span class="capability-exists yes">YES</span>
      <span class="capability-who">CCProxy, Portkey, OpenRouter</span>
    </div>

    <div class="capability-row">
      <span class="capability-name">Auto-failover on 429</span>
      <span class="capability-exists yes">YES</span>
      <span class="capability-who">claude-code-mux, CLIProxyAPI</span>
    </div>

    <div class="capability-row">
      <span class="capability-name">Manual delegation to Gemini</span>
      <span class="capability-exists yes">YES</span>
      <span class="capability-who">PAL MCP (clink tool)</span>
    </div>

    <div class="capability-row">
      <span class="capability-name">Real-time agent messaging</span>
      <span class="capability-exists yes">YES</span>
      <span class="capability-who">hcom</span>
    </div>

    <div class="capability-row">
      <span class="capability-name">Parallel Claude instances</span>
      <span class="capability-exists yes">YES</span>
      <span class="capability-who">Conductor Build</span>
    </div>

    <div class="capability-row">
      <span class="capability-name">Quota visibility</span>
      <span class="capability-exists yes">YES</span>
      <span class="capability-who">CLIProxyAPI, CodexBar</span>
    </div>

    <div class="capability-row" style="margin-top: 1rem; padding-top: 1rem; border-top: 1px solid var(--border);">
      <span class="capability-name">Semantic task routing (research vs code)</span>
      <span class="capability-exists no">NO</span>
      <span class="capability-who">Nobody — requires upstream classifier</span>
    </div>

    <div class="capability-row">
      <span class="capability-name">Persistent cross-session memory</span>
      <span class="capability-exists no">NO</span>
      <span class="capability-who">Nobody — PAL has 3hr limit, hcom logs but doesn't inject</span>
    </div>

    <div class="capability-row">
      <span class="capability-name">Proactive rate prediction (before 429)</span>
      <span class="capability-exists no">NO</span>
      <span class="capability-who">Nobody — all are reactive</span>
    </div>

    <div class="capability-row">
      <span class="capability-name">Automatic mid-task context handoff</span>
      <span class="capability-exists no">NO</span>
      <span class="capability-who">Nobody — claude-code-mux loses reasoning state</span>
    </div>

    <div class="capability-row">
      <span class="capability-name">Cross-model critique orchestration</span>
      <span class="capability-exists no">NO</span>
      <span class="capability-who">Nobody — no loopback architecture</span>
    </div>
  </div>

  <hr class="divider">

  <!-- HONEST ASSESSMENT -->
  <div class="assessment">
    <div class="assessment-title">Honest Assessment: Is the Gap Worth Building For?</div>

    <p style="color: var(--text-dim); margin-bottom: 1rem;">
      The 40% gap is real, but concentrated in specific scenarios:
    </p>

    <div class="assessment-item" data-num="1.">
      <strong>If you rarely hit rate limits mid-task:</strong> CCProxy + PAL MCP + CodexBar is probably enough. Install baseline, use for 2-4 weeks, see if friction is real.
    </div>

    <div class="assessment-item" data-num="2.">
      <strong>If you hit limits mid-debug 2-3x/week:</strong> The context handoff gap is painful — you lose reasoning state, waste time re-explaining. Worth building minimal harness.
    </div>

    <div class="assessment-item" data-num="3.">
      <strong>If you want semantic routing:</strong> Nobody has this. You'd need to build the classifier regardless of other tools. But CLAUDE.md heuristics might be 80% as good.
    </div>

    <div class="assessment-item" data-num="4.">
      <strong>If you want cross-session memory:</strong> Nobody has this. You'd need to build the memory layer. But manual AGENTS.md files might work for light use.
    </div>

    <p style="color: var(--text-muted); margin-top: 1.5rem; font-size: 0.9rem;">
      Bottom line: The ONLY capability you truly can't replicate with existing tools + CLAUDE.md instructions is <strong>mid-task context handoff</strong>. Everything else has workarounds. Test the baseline before building.
    </p>
  </div>

</div>

</body>
</html>
