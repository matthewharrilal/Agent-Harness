<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Competitive Landscape Analysis</title>
  <style>
    :root {
      --bg: #0d1117;
      --bg-card: #161b22;
      --bg-elevated: #1c2128;
      --text: #e6edf3;
      --text-dim: #8b949e;
      --text-muted: #6e7681;
      --blue: #58a6ff;
      --green: #3fb950;
      --yellow: #d29922;
      --red: #f85149;
      --purple: #a371f7;
      --border: #30363d;
    }

    * { box-sizing: border-box; margin: 0; padding: 0; }

    body {
      font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif;
      font-size: 15px;
      line-height: 1.6;
      color: var(--text);
      background: var(--bg);
      padding: 3rem 2rem;
    }

    .container { max-width: 860px; margin: 0 auto; }

    h1 {
      font-size: 1.75rem;
      font-weight: 600;
      margin-bottom: 0.5rem;
    }
    .subtitle {
      color: var(--text-dim);
      margin-bottom: 2.5rem;
    }

    /* Category Headers */
    .category {
      margin: 3rem 0 1.5rem;
      padding-bottom: 0.75rem;
      border-bottom: 1px solid var(--border);
    }
    .category-name {
      font-size: 0.8rem;
      font-weight: 600;
      text-transform: uppercase;
      letter-spacing: 0.1em;
      color: var(--purple);
      margin-bottom: 0.25rem;
    }
    .category-desc {
      color: var(--text-dim);
      font-size: 0.9rem;
    }

    /* Tool Sections */
    .tool {
      background: var(--bg-card);
      border: 1px solid var(--border);
      border-radius: 8px;
      padding: 1.5rem;
      margin-bottom: 1.25rem;
    }
    .tool-title {
      display: flex;
      align-items: center;
      gap: 0.75rem;
      margin-bottom: 0.75rem;
    }
    .tool-name {
      font-size: 1.1rem;
      font-weight: 600;
    }
    .tool-badge {
      font-size: 0.75rem;
      padding: 0.2rem 0.6rem;
      border-radius: 4px;
      font-weight: 500;
    }
    .tool-badge.high { background: rgba(63,185,80,0.15); color: var(--green); }
    .tool-badge.medium { background: rgba(210,153,34,0.15); color: var(--yellow); }
    .tool-badge.low { background: rgba(248,81,73,0.15); color: var(--red); }

    .tool-desc {
      color: var(--text-dim);
      margin-bottom: 1.25rem;
      line-height: 1.7;
    }

    /* Section labels */
    .section-label {
      font-size: 0.7rem;
      font-weight: 600;
      text-transform: uppercase;
      letter-spacing: 0.08em;
      margin-bottom: 0.5rem;
    }
    .section-label.works { color: var(--green); }
    .section-label.why { color: var(--purple); }
    .section-label.breaks { color: var(--red); }
    .section-label.gap { color: var(--yellow); }
    .section-label.journey { color: var(--blue); }

    .section-content.journey { border-color: rgba(88,166,255,0.4); }

    .journey-scenario {
      background: var(--bg);
      border: 1px solid var(--border);
      border-radius: 6px;
      padding: 1rem;
      margin: 0.75rem 0;
      font-size: 0.9rem;
      line-height: 1.7;
    }
    .journey-scenario .time {
      color: var(--blue);
      font-weight: 600;
      font-family: 'SF Mono', Monaco, monospace;
      font-size: 0.85rem;
    }
    .journey-scenario .action {
      color: var(--text-dim);
    }
    .journey-contrast {
      display: grid;
      grid-template-columns: 1fr 1fr;
      gap: 1rem;
      margin: 0.75rem 0;
    }
    .journey-contrast-item {
      background: var(--bg);
      border: 1px solid var(--border);
      border-radius: 6px;
      padding: 0.75rem;
      font-size: 0.85rem;
    }
    .journey-contrast-item .label {
      font-weight: 600;
      font-size: 0.75rem;
      text-transform: uppercase;
      letter-spacing: 0.05em;
      margin-bottom: 0.5rem;
    }
    .journey-contrast-item.current .label { color: var(--text-muted); }
    .journey-contrast-item.hypothetical .label { color: var(--purple); }

    .section-content {
      color: var(--text-dim);
      margin-bottom: 1.25rem;
      padding-left: 0.75rem;
      border-left: 2px solid var(--border);
    }
    .section-content.works { border-color: rgba(63,185,80,0.4); }
    .section-content.why { border-color: rgba(163,113,247,0.4); }
    .section-content.breaks { border-color: rgba(248,81,73,0.4); }
    .section-content.gap { border-color: rgba(210,153,34,0.4); }

    .section-content ul {
      list-style: none;
      padding: 0;
    }
    .section-content li {
      margin: 0.4rem 0;
      padding-left: 1rem;
      position: relative;
    }
    .section-content li::before {
      content: "•";
      position: absolute;
      left: 0;
      color: var(--text-muted);
    }

    /* Divider */
    .divider {
      border: none;
      border-top: 1px solid var(--border);
      margin: 3rem 0;
    }

    /* Summary Table */
    .summary-section {
      margin: 3rem 0;
    }
    .summary-title {
      font-size: 1.25rem;
      font-weight: 600;
      margin-bottom: 1.5rem;
    }

    .capability-row {
      display: grid;
      grid-template-columns: 1fr auto 1fr;
      gap: 1rem;
      padding: 0.75rem 0;
      border-bottom: 1px solid var(--border);
      align-items: start;
    }
    .capability-row:last-child { border-bottom: none; }

    .capability-name {
      color: var(--text);
      font-weight: 500;
    }
    .capability-exists {
      text-align: center;
      font-size: 0.85rem;
      font-weight: 600;
      min-width: 50px;
    }
    .capability-exists.yes { color: var(--green); }
    .capability-exists.no { color: var(--red); }
    .capability-who {
      color: var(--text-muted);
      font-size: 0.9rem;
    }

    /* Assessment Box */
    .assessment {
      background: var(--bg-card);
      border: 1px solid var(--border);
      border-radius: 8px;
      padding: 1.5rem;
      margin: 2rem 0;
    }
    .assessment-title {
      font-size: 1.1rem;
      font-weight: 600;
      margin-bottom: 1rem;
      color: var(--yellow);
    }
    .assessment-item {
      margin: 0.75rem 0;
      padding-left: 1.5rem;
      position: relative;
      color: var(--text-dim);
    }
    .assessment-item::before {
      content: attr(data-num);
      position: absolute;
      left: 0;
      color: var(--text-muted);
      font-size: 0.85rem;
    }
    .assessment-item strong {
      color: var(--text);
    }

    /* Context box */
    .context {
      background: var(--bg-elevated);
      border: 1px solid var(--border);
      border-radius: 8px;
      padding: 1.25rem;
      margin-bottom: 2rem;
    }
    .context-row {
      display: flex;
      margin: 0.4rem 0;
    }
    .context-key {
      color: var(--text-muted);
      min-width: 140px;
      font-size: 0.9rem;
    }
    .context-value {
      color: var(--text);
      font-size: 0.9rem;
    }

    /* Code/scenario blocks */
    code {
      background: var(--bg-elevated);
      padding: 0.15rem 0.4rem;
      border-radius: 4px;
      font-family: 'SF Mono', Monaco, monospace;
      font-size: 0.9em;
      color: var(--blue);
    }

    .scenario {
      background: var(--bg);
      border: 1px solid var(--border);
      border-radius: 6px;
      padding: 1rem;
      margin: 0.75rem 0;
      font-family: 'SF Mono', Monaco, monospace;
      font-size: 0.8rem;
      line-height: 1.6;
      color: var(--text-muted);
      white-space: pre-wrap;
      overflow-x: auto;
    }
    /* Tab Navigation */
    .tab-nav {
      display: flex;
      gap: 0;
      border-bottom: 1px solid var(--border);
      margin-bottom: 2.5rem;
      position: sticky;
      top: 0;
      background: var(--bg);
      z-index: 100;
      padding-top: 1rem;
    }
    .tab-btn {
      padding: 0.75rem 1.5rem;
      background: none;
      border: none;
      border-bottom: 2px solid transparent;
      color: var(--text-dim);
      font-size: 0.9rem;
      font-weight: 500;
      cursor: pointer;
      transition: all 0.2s;
      font-family: inherit;
    }
    .tab-btn:hover { color: var(--text); }
    .tab-btn.active {
      color: var(--purple);
      border-bottom-color: var(--purple);
    }
    .tab-panel { display: none; }
    .tab-panel.active { display: block; }

    /* Investigation card section types */
    .section-label.tension { color: var(--purple); }
    .section-content.tension { border-color: rgba(163,113,247,0.4); }
    .section-label.realize { color: #56d4dd; }
    .section-content.realize { border-color: rgba(86,212,221,0.4); }
    .section-label.threads { color: var(--text-muted); }
    .section-content.threads { border-color: var(--border); }

    /* Wave badges */
    .wave-badge {
      font-size: 0.7rem;
      padding: 0.15rem 0.5rem;
      border-radius: 4px;
      font-weight: 600;
      background: rgba(163,113,247,0.15);
      color: var(--purple);
    }
    .layer-badge {
      font-size: 0.7rem;
      padding: 0.15rem 0.5rem;
      border-radius: 4px;
      font-weight: 500;
      background: rgba(88,166,255,0.1);
      color: var(--blue);
    }

    /* Investigation header */
    .iq-status {
      display: inline-flex;
      align-items: center;
      gap: 0.5rem;
      font-size: 0.8rem;
      color: var(--text-muted);
      margin-bottom: 1.5rem;
    }
    .iq-status .dot {
      width: 8px;
      height: 8px;
      border-radius: 50%;
      background: var(--green);
    }

    /* Dependency graph box */
    .dep-graph {
      background: var(--bg-card);
      border: 1px solid var(--border);
      border-radius: 8px;
      padding: 1.5rem;
      margin: 2rem 0;
      overflow-x: auto;
    }
    .dep-graph pre {
      font-family: 'SF Mono', Monaco, monospace;
      font-size: 0.75rem;
      line-height: 1.5;
      color: var(--text-dim);
      margin: 0;
    }
  </style>
</head>
<body>

<div class="container">

  <nav class="tab-nav">
    <button class="tab-btn active" data-tab="landscape">Competitive Landscape</button>
    <button class="tab-btn" data-tab="investigation">Investigation Questions</button>
  </nav>

  <div id="tab-landscape" class="tab-panel active">

  <h1>Competitive Landscape Analysis</h1>
  <p class="subtitle">8 tools analyzed for hybrid Claude + Gemini orchestration</p>

  <div class="context">
    <div class="context-row">
      <span class="context-key">Problem</span>
      <span class="context-value">85% of Claude Max rate limits hit in 1-2 days</span>
    </div>
    <div class="context-row">
      <span class="context-key">Proposed Solution</span>
      <span class="context-value">Invisible middleware routing between Claude + Gemini</span>
    </div>
    <div class="context-row">
      <span class="context-key">Key Question</span>
      <span class="context-value">Is the ~40% gap between existing tools and the vision worth building?</span>
    </div>
  </div>

  <!-- TERMINOLOGY SECTION -->
  <div class="tool" style="margin-top: 2rem; border-color: var(--purple);">
    <div class="tool-title">
      <span class="tool-name">What Is An Agent Harness?</span>
    </div>

    <p class="tool-desc">
      The 2026 industry consensus (Philipp Schmid/HuggingFace, Sequoia Capital, Aakash Gupta) is that <strong>"2026 Is Agent Harnesses"</strong> — the harness, not the model, is where differentiation happens. Manus rewrote their harness 5 times with the same underlying models; each rewrite improved reliability. The infrastructure matters more than the weights.
    </p>

    <div class="section-label" style="color: var(--purple);">Industry Definition</div>
    <div class="section-content" style="border-color: rgba(163,113,247,0.4);">
      <p style="margin-bottom: 0.75rem;">An <strong>agent harness</strong> is the complete architectural system surrounding an LLM that manages what the model can't do itself:</p>
      <ul>
        <li><strong>Execute tool calls</strong> — Model outputs intent, harness performs the action</li>
        <li><strong>Manage memory</strong> — Persist context beyond conversation window</li>
        <li><strong>Structure workflows</strong> — Coordinate multi-step operations</li>
        <li><strong>Handle long-running tasks</strong> — Checkpointing, recovery, continuation</li>
        <li><strong>Enforce rules</strong> — Guardrails, permissions, validation</li>
      </ul>
      <p style="margin-top: 0.75rem; color: var(--text-muted);">The harness is scaffolding — not the model, not the business logic, but the infrastructure that makes agents reliable.</p>
    </div>

    <div class="section-label" style="color: var(--yellow);">Your Concept: A Hybrid</div>
    <div class="section-content" style="border-color: rgba(210,153,34,0.4);">
      <p style="margin-bottom: 0.75rem;">What you're building is <strong>more than a harness</strong> — it combines three architectural components:</p>
      <ul>
        <li><strong>Harness component</strong> — Memory management, tool execution, context persistence across sessions</li>
        <li><strong>Orchestrator component</strong> — Routing logic deciding Claude vs Gemini per task based on semantics</li>
        <li><strong>Proxy/Router component</strong> — Intercepts requests, directs traffic, handles failover</li>
      </ul>
      <p style="margin-top: 0.75rem; color: var(--text-muted);">If you only call it a "harness," people expect infrastructure for ONE model. Your vision includes orchestration BETWEEN models — deciding which chef handles which dish, sharing prep work between kitchens.</p>
    </div>

    <div class="section-label" style="color: var(--blue);">Better Terminology Options</div>
    <div class="section-content" style="border-color: rgba(88,166,255,0.4);">
      <ul>
        <li><strong>Agent Orchestration Layer</strong> — Clearest, industry-standard. Emphasizes coordination between providers.</li>
        <li><strong>Multi-Model Harness</strong> — Extends the harness definition to explicitly cover cross-provider work.</li>
        <li><strong>Agentic Router</strong> — If emphasizing the routing decision logic over infrastructure.</li>
      </ul>
    </div>

    <div class="section-label" style="color: var(--text-dim);">Why Existing Tools Aren't This</div>
    <div class="section-content" style="border-color: var(--border);">
      <p style="margin-bottom: 0.75rem;">The 8 tools analyzed each handle ONE piece:</p>
      <ul>
        <li><strong>CCProxy, Portkey, OpenRouter</strong> — Proxies/gateways that route, but don't manage state</li>
        <li><strong>claude-code-mux</strong> — Failover proxy, but loses reasoning state during handoff</li>
        <li><strong>PAL MCP</strong> — Delegation tool, but no persistent memory or automatic routing</li>
        <li><strong>hcom</strong> — Inter-agent messaging, but passive (tells what happened, not what should happen)</li>
        <li><strong>Conductor Build</strong> — Parallel execution, but Claude-only, no cross-provider orchestration</li>
        <li><strong>CLIProxyAPI</strong> — Quota visibility, but display-only, doesn't act on thresholds</li>
      </ul>
      <p style="margin-top: 0.75rem; color: var(--text-muted);"><strong>None combine:</strong> semantic routing + persistent memory + proactive rate prediction + mid-task context handoff. That's the ~40% gap.</p>
    </div>

    <div class="section-label" style="color: var(--blue);">User Journey: What Might Change (Speculative)</div>
    <div class="section-content" style="border-color: rgba(88,166,255,0.4);">
      <p style="margin-bottom: 0.75rem;"><strong>Today's mental model:</strong> You manage the orchestration. You track rate limits, decide when to delegate, copy context between sessions, and choose which provider handles which task.</p>

      <div class="journey-contrast">
        <div class="journey-contrast-item current">
          <div class="label">Current Reality</div>
          <ul style="margin: 0; padding-left: 1rem; color: var(--text-dim); font-size: 0.85rem;">
            <li>You watch CodexBar: "Claude 82%"</li>
            <li>You decide: "This research can go to Gemini"</li>
            <li>You invoke clink manually</li>
            <li>Tomorrow, you copy findings to new session</li>
          </ul>
        </div>
        <div class="journey-contrast-item hypothetical">
          <div class="label">Hypothetical Harness</div>
          <ul style="margin: 0; padding-left: 1rem; color: var(--text-dim); font-size: 0.85rem;">
            <li>System sees: "Claude 82%, task is research"</li>
            <li>System routes to Gemini automatically</li>
            <li>You see routing decision in ambient log</li>
            <li>Tomorrow, memory auto-injects context</li>
          </ul>
        </div>
      </div>

      <p style="margin-top: 0.75rem;"><strong>What shifts TO the system:</strong> Rate limit tracking, routing decisions, context persistence, provider selection.</p>
      <p><strong>What stays WITH you:</strong> High-level task decomposition, veto authority over routing, approval of plans, understanding why decisions were made.</p>
      <p><strong>New responsibilities:</strong> Trusting routing you didn't choose. Debugging when delegation fails. Understanding why the system chose Gemini over Claude. Handling conflicts if memory is stale.</p>
      <p style="color: var(--text-muted); margin-top: 0.75rem;"><strong>Honest uncertainty:</strong> We don't know if this is worth building until we test the baseline. Does automatic routing actually save enough quota to justify the complexity? Does the mental model shift feel like relief or loss of control?</p>
    </div>
  </div>

  <!-- ROUTING/PROXY LAYER -->
  <div class="category">
    <div class="category-name">Routing / Proxy Layer</div>
    <div class="category-desc">Tools that sit between your client and AI providers, intercepting and routing requests</div>
  </div>

  <div class="tool">
    <div class="tool-title">
      <span class="tool-name">CCProxy</span>
      <span class="tool-badge high">~65%</span>
    </div>
    <p class="tool-desc">
      Python-based proxy using LiteLLM that intercepts Claude Code API requests. Routes based on 4 rule types: <code>TokenCountRule</code> (if tokens > threshold), <code>MatchModelRule</code> (model name matching), <code>ThinkingRule</code> (thinking parameter presence), and <code>MatchToolRule</code> (tool invocation detection). Design philosophy is zero overhead, transparent proxy — rules evaluate in &lt;1ms.
    </p>

    <div class="section-label works">What It Does Well</div>
    <div class="section-content works">
      <ul>
        <li>Token-based routing works perfectly for large context scenarios — request with 180K tokens automatically routes to Gemini's 1M context window</li>
        <li>Tool-based routing enables WebSearch → Perplexity scenarios without manual intervention</li>
        <li>Zero latency overhead since rules are simple conditionals, not model calls</li>
        <li>Works with Claude Max subscriptions (OAuth compatible)</li>
        <li>Transparent to Claude Code — it doesn't know the proxy exists</li>
      </ul>
    </div>

    <div class="section-label why">Why It Works This Way</div>
    <div class="section-content why">
      <ul>
        <li><strong>Architectural constraint:</strong> A proxy sees the request envelope (tokens, model name, tools), not the semantic intent inside. It operates at the HTTP boundary.</li>
        <li><strong>Speed over intelligence:</strong> Rules evaluate in &lt;1ms. Adding semantic classification would add 50-200ms latency per request — that's no longer a "transparent proxy."</li>
        <li><strong>Simplicity over accuracy:</strong> 4 clear rule types vs. a classifier that needs training data, maintenance, and can misroute 10-15% of requests unpredictably.</li>
        <li><strong>Fails predictably:</strong> If token-count routing is wrong, it's wrong consistently. Classifier errors are unpredictable and harder to debug.</li>
        <li><strong>This is a valid trade-off:</strong> CCProxy chose transparency and speed. Not every tool needs to be intelligent — some should just be fast and reliable.</li>
      </ul>
    </div>

    <div class="section-label breaks">What Breaks</div>
    <div class="section-content breaks">
      <ul>
        <li>Cannot understand task semantics — "Research JWT vulnerabilities" and "Debug auth.ts" both look like ~10 tokens with no tools, route identically</li>
        <li>No rate limit awareness — doesn't know you're at 85% Claude quota, routes next request to Claude anyway</li>
        <li>No failover — if Anthropic fails, request fails. No retry logic.</li>
        <li>No shared state between requests — each request is independent, no learning from patterns</li>
        <li>Mixed-intent requests route entirely to one provider — "Research X then code Y" goes 100% to Claude, wasting quota on research</li>
      </ul>
    </div>

    <div class="section-label gap">The Gap</div>
    <div class="section-content gap">
      <ul>
        <li><strong>Missing capability:</strong> Task-intent classification — distinguishing research from coding from debugging based on the prompt content</li>
        <li><strong>Missing capability:</strong> Rate limit awareness — knowing quota state and routing accordingly</li>
        <li><strong>Open question:</strong> Is adding 50-200ms latency for semantic routing worth the quota savings? Depends on how often misrouting actually happens in your workflow.</li>
        <li><strong>Open question:</strong> Could heuristics (keyword matching) get 80% of semantic routing value without the latency cost?</li>
      </ul>
    </div>

    <div class="section-label journey">User Journey</div>
    <div class="section-content journey">
      <p style="margin-bottom: 0.75rem;"><strong>Mental model:</strong> CCProxy is a fast, transparent gatekeeper. It's not intelligent — it's predictable. You understand what it can see (tokens, tools, model name) and what it can't (intent, semantics, task type).</p>

      <div class="journey-scenario">
        <p><span class="time">9:47 AM</span> <span class="action">— You configured CCProxy: "if tokens > 100K, route to Gemini." You type <code>claude</code> normally. CCProxy sits at localhost:4000, invisible.</span></p>
        <p><span class="time">9:50 AM</span> <span class="action">— You ask: "Research latest XSS attack vectors and fix auth.ts." CCProxy sees: 12 tokens, no tools. Routes to Claude (default). You realize: this is research, but CCProxy can't tell. You accept the trade-off — research runs on Claude.</span></p>
        <p><span class="time">10:15 AM</span> <span class="action">— Research done (15K tokens). You pivot to coding. CCProxy still under 100K threshold. No routing change. The research→coding semantic boundary was invisible to it.</span></p>
      </div>

      <p style="margin-top: 0.75rem;"><strong>Division of responsibility:</strong></p>
      <ul>
        <li><strong>CCProxy handles:</strong> Exact token counting, &lt;1ms rule evaluation, forwarding to correct endpoint</li>
        <li><strong>You handle:</strong> Understanding your task is research vs coding, knowing when the threshold should have triggered, recognizing token count ≠ semantic complexity</li>
      </ul>

      <div class="journey-contrast">
        <div class="journey-contrast-item current">
          <div class="label">Without CCProxy</div>
          <span style="color: var(--text-dim); font-size: 0.85rem;">Manually stop Claude, start Gemini for large analysis, copy-paste context between CLI instances.</span>
        </div>
        <div class="journey-contrast-item hypothetical">
          <div class="label">With Orchestration Layer</div>
          <span style="color: var(--text-dim); font-size: 0.85rem;">System detects "research task" → routes to Gemini. Detects "coding phase" → switches back to Claude mid-conversation.</span>
        </div>
      </div>

      <p style="margin-top: 0.75rem; color: var(--text-muted);"><strong>Workflow fit:</strong> Token-aware load distribution when you accept manual decomposition of semantic intent. Works beautifully for "large audits → Gemini, code gen → Claude." Less useful for the research-then-code pattern where the semantic boundary matters more than token count.</p>
    </div>
  </div>

  <div class="tool">
    <div class="tool-title">
      <span class="tool-name">claude-code-mux</span>
      <span class="tool-badge high">~60%</span>
    </div>
    <p class="tool-desc">
      Rust-based HTTP proxy providing automatic failover between AI providers. When Anthropic returns 429, it immediately tries your backup providers (OpenRouter, Vertex AI, etc.). Supports 18+ providers, OAuth authentication, &lt;1ms routing overhead, ~5MB memory footprint. Designed as a stateless proxy — each request is independent.
    </p>

    <div class="section-label works">What It Does Well</div>
    <div class="section-content works">
      <ul>
        <li>Seamless failover on 429 errors — you send a request, Anthropic fails, mux catches it, tries OpenRouter, returns response. You never know the switch happened.</li>
        <li>Priority-based provider configuration — set Anthropic as priority 1, OpenRouter as priority 2, Vertex as priority 3</li>
        <li>OAuth authentication support — works with Claude Max subscriptions</li>
        <li>Minimal resource usage — Rust binary, &lt;1ms routing, ~5MB memory</li>
        <li>Fault isolation — one bad request doesn't affect subsequent requests</li>
      </ul>
    </div>

    <div class="section-label why">Why It Works This Way</div>
    <div class="section-content why">
      <ul>
        <li><strong>Stateless by design:</strong> Each request is independent. No database, no storage, no persistence. This means zero infrastructure requirements and no state corruption risks.</li>
        <li><strong>Proxy, not orchestrator:</strong> Adding memory would change the tool's nature entirely. It would need a database, state extraction logic, injection logic — a different product.</li>
        <li><strong>The hard problem:</strong> Extracting "reasoning state" from Claude's session is non-trivial. You can't just dump the conversation — Claude's internal hypothesis chain isn't in the text.</li>
        <li><strong>Failure mode simplicity:</strong> Stateless means easy debugging (each request reproducible), horizontal scaling (spin up more proxies), and crash recovery (restart, zero data loss).</li>
        <li><strong>This is a valid trade-off:</strong> For a failover proxy, reliability beats intelligence. Memory adds complexity and new failure modes.</li>
      </ul>
    </div>

    <div class="section-label breaks">What Breaks</div>
    <div class="section-content breaks">
      <ul>
        <li><strong>Context continuity breaks during failover.</strong> Turn 1-5: debugging a race condition, Claude eliminated 2 hypotheses, testing the third. Turn 6: 429, switches to OpenRouter. Fresh instance re-reads messages, might re-investigate eliminated hypotheses. You lose 5-10 minutes.</li>
        <li>No semantic routing — doesn't understand task type, just priority-based failover</li>
        <li>No rate limit prediction — reacts to 429 after it happens, doesn't predict or prevent it</li>
        <li>Message history preserved, reasoning state lost — the text is there but the "mental model" isn't</li>
      </ul>
    </div>

    <div class="section-label gap">The Gap</div>
    <div class="section-content gap">
      <ul>
        <li><strong>Missing capability:</strong> Reasoning state preservation during provider handoff — some way to capture "where Claude was" in its debugging process</li>
        <li><strong>Missing capability:</strong> Proactive rate prediction — routing before hitting limits, not after</li>
        <li><strong>Open question:</strong> How do you capture reasoning state? Is it in the message history (could be summarized)? Or is it truly internal to the inference session (fundamentally lost)?</li>
        <li><strong>Open question:</strong> Would a simple "here's what we've tried so far" summary injected into the new session be enough? Or do you need something more sophisticated?</li>
      </ul>
    </div>

    <div class="section-label journey">User Journey</div>
    <div class="section-content journey">
      <p style="margin-bottom: 0.75rem;"><strong>Mental model:</strong> claude-code-mux is a stateless message router. It preserves your conversation text but not the model's reasoning state. When failover happens, the new instance re-reads messages but builds its own understanding from scratch.</p>

      <div class="journey-scenario">
        <p><span class="time">10:00 AM</span> <span class="action">— You're 25 minutes into debugging a race condition. Claude has eliminated 2 hypotheses, is testing a third involving async initialization order.</span></p>
        <p><span class="time">10:25 AM</span> <span class="action">— Turn 6: You ask "Can you trace the initialization sequence?" Anthropic returns 429. Mux catches it, switches to OpenRouter (3-5 second delay).</span></p>
        <p><span class="time">10:26 AM</span> <span class="action">— OpenRouter Claude receives all 6 turns of message history. But it's a fresh instance — no memory of the hypothesis chain. It might re-investigate the eliminated hypotheses. You notice the response feels like it's "starting over."</span></p>
        <p><span class="time">10:35 AM</span> <span class="action">— You manually re-orient: "We already ruled out timing and lock scope. Focus on async init." The debugging continues, but you lost ~10 minutes.</span></p>
      </div>

      <p style="margin-top: 0.75rem;"><strong>Division of responsibility:</strong></p>
      <ul>
        <li><strong>Mux handles:</strong> 429 detection, automatic failover, provider priority, message forwarding</li>
        <li><strong>You handle:</strong> Recognizing when failover happened, validating response quality, re-orienting the new instance if needed, deciding when to switch back</li>
      </ul>

      <div class="journey-contrast">
        <div class="journey-contrast-item current">
          <div class="label">Without mux</div>
          <span style="color: var(--text-dim); font-size: 0.85rem;">429 error. You manually open Gemini, copy conversation, paste, explain "we were debugging X." 3-5 minute interruption.</span>
        </div>
        <div class="journey-contrast-item hypothetical">
          <div class="label">With Persistent Memory</div>
          <span style="color: var(--text-dim); font-size: 0.85rem;">Mux fails over + injects: "Prior reasoning: eliminated hypotheses A, B. Testing C." New instance continues from the exact point.</span>
        </div>
      </div>

      <p style="margin-top: 0.75rem; color: var(--text-muted);"><strong>Workflow fit:</strong> Short, stateless requests where reasoning continuity doesn't matter. Less suited for long debugging chains where the model's "mental model" is critical to progress.</p>
    </div>
  </div>

  <div class="tool">
    <div class="tool-title">
      <span class="tool-name">Portkey</span>
      <span class="tool-badge low">~30%</span>
    </div>
    <p class="tool-desc">
      Cloud AI gateway for enterprise use. Every request goes through Portkey's servers where it can be routed based on metadata, logged, cached, and monitored. Supports complex conditional routing rules, excellent observability (costs, latency, tokens), request caching, failover/retries, and per-team spending limits. SOC 2 compliant.
    </p>

    <div class="section-label works">What It Does Well</div>
    <div class="section-content works">
      <ul>
        <li>Metadata-based conditional routing — <code>IF metadata.task_type == "research" THEN route to Gemini</code></li>
        <li>Excellent observability — every request logged with costs, latency, token counts</li>
        <li>Caching — identical requests return cached responses, saves money</li>
        <li>Failover and retries — if Provider A fails, try Provider B</li>
        <li>Enterprise features — audit logs, cost controls per team/project, SOC 2 compliance</li>
      </ul>
    </div>

    <div class="section-label why">Why It Works This Way</div>
    <div class="section-content why">
      <ul>
        <li><strong>Gateway principle:</strong> The routing layer shouldn't own business logic. Your application understands your tasks better than a generic gateway ever could.</li>
        <li><strong>Separation of concerns:</strong> Portkey's job is to forward, log, and cache. Classification is YOUR responsibility — you know what "research" means in your context.</li>
        <li><strong>Flexibility:</strong> You can use ANY classifier upstream (keyword matching, embeddings, LLM calls, heuristics). Portkey doesn't constrain your approach.</li>
        <li><strong>Stateless simplicity:</strong> Gateway stays simple, fast, and reliable. Complexity lives upstream where it belongs.</li>
        <li><strong>API keys by design:</strong> Enterprise gateways work with API keys because that's how providers bill enterprises. Subscription OAuth is a consumer pattern.</li>
      </ul>
    </div>

    <div class="section-label breaks">What Breaks</div>
    <div class="section-content breaks">
      <ul>
        <li><strong>Cannot use Claude Max subscriptions.</strong> Anthropic banned third-party OAuth access in Jan 2026. Portkey requires API keys. You'd pay for Max AND API credits — double cost.</li>
        <li>No semantic understanding — routes based on metadata YOU provide. "Upstream logic" problem: YOU classify, then tell Portkey via metadata.</li>
        <li>Stateless — no memory between requests, no context persistence</li>
        <li>Cloud dependency — every request routes through Portkey's servers, adding latency and requiring trust</li>
      </ul>
    </div>

    <div class="section-label gap">The Gap</div>
    <div class="section-content gap">
      <ul>
        <li><strong>Missing capability:</strong> Automatic semantic classification — something needs to decide "this is research" without requiring your code to do it</li>
        <li><strong>Missing capability:</strong> Subscription authentication — using Max/$200mo through routing infrastructure</li>
        <li><strong>Open question:</strong> Should the classifier live in the gateway (simpler for user, less flexible) or upstream (more flexible, more work for user)?</li>
        <li><strong>Open question:</strong> Is Portkey's enterprise focus incompatible with individual Max subscriptions, or is there a bridge?</li>
      </ul>
    </div>

    <div class="section-label journey">User Journey</div>
    <div class="section-content journey">
      <p style="margin-bottom: 0.75rem;"><strong>Mental model:</strong> "I am the classifier; Portkey is the enforcer." You determine task type upstream, attach it as metadata, and Portkey routes based on your classification. The gateway doesn't understand your task — it trusts your labels.</p>

      <div class="journey-scenario">
        <p><span class="time">Setup</span> <span class="action">— You build a wrapper that classifies tasks. When you type "research JWT security," your code tags it <code>metadata.task_type = "research"</code> before calling Portkey.</span></p>
        <p><span class="time">10:00 AM</span> <span class="action">— You debug OAuth implementation. Your classifier sees "debug" keyword, tags it <code>task_type = "coding"</code>. Portkey routes to Claude.</span></p>
        <p><span class="time">10:30 AM</span> <span class="action">— You ask "What's the latest OAuth 2.1 spec?" Your classifier tags <code>"research"</code>. Portkey routes to Gemini. But wait — you're using Claude Max, which Portkey can't use. You'd need a separate Gemini API key.</span></p>
      </div>

      <p style="margin-top: 0.75rem;"><strong>Division of responsibility:</strong></p>
      <ul>
        <li><strong>Portkey handles:</strong> Conditional routing based on metadata, logging, caching, failover, cost tracking</li>
        <li><strong>You handle:</strong> Building the classifier, maintaining it, handling edge cases where classification is wrong, managing separate API keys for each provider</li>
      </ul>

      <p style="margin-top: 0.75rem; color: var(--text-muted);"><strong>Workflow fit:</strong> Enterprise teams with API budgets who want observability and conditional routing. Less suited for individual Max subscribers — the OAuth incompatibility means double billing.</p>
    </div>
  </div>

  <div class="tool">
    <div class="tool-title">
      <span class="tool-name">OpenRouter</span>
      <span class="tool-badge low">~25%</span>
    </div>
    <p class="tool-desc">
      API gateway providing unified access to 300+ AI models from 60+ providers through a single endpoint. One integration, all models — call <code>openrouter.ai/api/v1/messages</code> with any model name (Claude, GPT-4, Gemini, Llama, Mistral). Has "Auto Router" feature powered by NotDiamond that learns which models perform best for task types. BYOK option available with 5% fee.
    </p>

    <div class="section-label works">What It Does Well</div>
    <div class="section-content works">
      <ul>
        <li>Single endpoint for 300+ models — one integration, all providers</li>
        <li>Easy model switching — just change the model name in the API call</li>
        <li>Auto Router — learned routing powered by NotDiamond, picks optimal model per task</li>
        <li>Cost comparison — see pricing across all models in one place</li>
        <li>Fallback support — if model A fails, try model B</li>
        <li>BYOK option — use your own API keys (with 5% fee)</li>
      </ul>
    </div>

    <div class="section-label why">Why It Works This Way</div>
    <div class="section-content why">
      <ul>
        <li><strong>Unified interface principle:</strong> One endpoint simplifies integration. You don't need 60 SDKs or manage 60 API keys.</li>
        <li><strong>Explicit over implicit:</strong> Auto Router doesn't auto-activate. YOU decide when to use it. This gives control to the user.</li>
        <li><strong>Learning from YOUR patterns:</strong> Auto Router needs 15+ examples to learn what works FOR YOU. Generic routing wouldn't match your specific task distribution.</li>
        <li><strong>Avoiding over-fitting:</strong> Not every team uses models the same way. The training requirement ensures personalization rather than generic one-size-fits-all routing.</li>
        <li><strong>BYOK flexibility:</strong> Some users want to pay per-token through OpenRouter, others want to use their existing API keys. Both options exist.</li>
      </ul>
    </div>

    <div class="section-label breaks">What Breaks</div>
    <div class="section-content breaks">
      <ul>
        <li><strong>Cannot use Claude Max subscriptions.</strong> Same OAuth problem as Portkey — requires API keys, not subscription auth.</li>
        <li>Auto Router requires 15+ labeled training examples. Cold start problem — you need to manually label tasks before it helps.</li>
        <li>Claude Code hardcodes model selection — Auto Router feature not exposed to CLI tools like Claude Code</li>
        <li>Stateless — no shared memory between requests, no context persistence</li>
        <li>5% BYOK fee adds cost on top of provider pricing</li>
      </ul>
    </div>

    <div class="section-label gap">The Gap</div>
    <div class="section-content gap">
      <ul>
        <li><strong>Missing capability:</strong> Zero-config routing — something that works out of the box without 15+ training examples</li>
        <li><strong>Missing capability:</strong> Integration with Claude Code — exposing routing decisions to CLI tools</li>
        <li><strong>Open question:</strong> Is the 15-example training requirement actually a feature (personalization) or a friction point? Would generic defaults be good enough?</li>
        <li><strong>Open question:</strong> Could heuristics bootstrap Auto Router faster than manual labeling?</li>
      </ul>
    </div>

    <div class="section-label journey">User Journey</div>
    <div class="section-content journey">
      <p style="margin-bottom: 0.75rem;"><strong>Mental model:</strong> "One integration, infinite model access." You call one endpoint, specify any model name, and OpenRouter handles the provider complexity. Auto Router learns your patterns — but only after you teach it.</p>

      <div class="journey-scenario">
        <p><span class="time">Week 1</span> <span class="action">— You integrate OpenRouter. You manually pick models: <code>model: "anthropic/claude-3-opus"</code> for coding, <code>model: "google/gemini-pro"</code> for research. Auto Router is available but needs 15+ labeled examples to learn.</span></p>
        <p><span class="time">Week 2</span> <span class="action">— You label 20 past requests with outcomes. Auto Router starts suggesting: "For prompts like this, Gemini performs 12% better." But Claude Code hardcodes model selection — you can't use Auto Router from the CLI.</span></p>
        <p><span class="time">Week 3</span> <span class="action">— You want to use Claude Max ($200/mo) through OpenRouter to get routing benefits. Problem: OpenRouter needs API keys, Max uses OAuth. You'd need separate API credits — defeating the purpose.</span></p>
      </div>

      <p style="margin-top: 0.75rem;"><strong>Division of responsibility:</strong></p>
      <ul>
        <li><strong>OpenRouter handles:</strong> Unified API, model switching, fallback, cost comparison, Auto Router (after training)</li>
        <li><strong>You handle:</strong> Providing 15+ training examples, managing API keys, accepting that Max subscriptions don't work here</li>
      </ul>

      <p style="margin-top: 0.75rem; color: var(--text-muted);"><strong>Workflow fit:</strong> API-first developers building apps who want model flexibility. Less suited for Claude Code power users on Max subscriptions — the integration path doesn't exist.</p>
    </div>
  </div>

  <!-- DELEGATION LAYER -->
  <div class="category">
    <div class="category-name">Delegation / MCP Layer</div>
    <div class="category-desc">Tools that let Claude delegate tasks to other AI models</div>
  </div>

  <div class="tool">
    <div class="tool-title">
      <span class="tool-name">PAL MCP</span>
      <span class="tool-badge medium">~45%</span>
    </div>
    <p class="tool-desc">
      MCP server (10K+ GitHub stars) that runs inside Claude Code, providing the <code>clink</code> tool ("CLI Link") that spawns external CLI processes. Claude can delegate to Gemini, Codex, or other Claude instances. Supports role-based prompting via pre-configured system prompts, file passing, and structured JSON output. Core idea: Claude Code has limits (200K context, no web search), PAL lets it call models with different strengths.
    </p>

    <div class="section-label works">What It Does Well</div>
    <div class="section-content works">
      <ul>
        <li>Clean delegation interface — <code>clink(cli_name="gemini", role="security-analyzer", prompt="Analyze auth.ts")</code></li>
        <li>Role-based prompting — pre-configured system prompts for different task types load automatically</li>
        <li>Result integration — Gemini's output flows naturally back into Claude's conversation as a tool result</li>
        <li>Multiple CLI support — configure Gemini, Codex, other Claude instances, anything with a CLI</li>
        <li>Delegates large context work to Gemini (1M context) while Claude orchestrates</li>
        <li>Battle-tested, production-ready with 10K+ GitHub stars</li>
      </ul>
    </div>

    <div class="section-label why">Why It Works This Way</div>
    <div class="section-content why">
      <ul>
        <li><strong>Process isolation:</strong> Fresh subprocess = fresh state = no cleanup, no corruption risk. One call's failure doesn't contaminate the next.</li>
        <li><strong>Clarity over magic:</strong> Users know exactly what Gemini sees (the prompt you sent). No hidden context injection surprises.</li>
        <li><strong>Manual control:</strong> YOU decide when to delegate via explicit clink calls. This avoids classification errors where the system misroutes your task.</li>
        <li><strong>Debuggability:</strong> "Show me what PAL sent to Gemini" is straightforward. With automatic routing, debugging "why did it choose Gemini?" is harder.</li>
        <li><strong>MCP scope:</strong> PAL is a delegation tool, not an orchestration layer. Adding persistent memory would expand scope significantly.</li>
      </ul>
    </div>

    <div class="section-label breaks">What Breaks</div>
    <div class="section-content breaks">
      <ul>
        <li><strong>No persistent memory across sessions.</strong> Day 1: Gemini finds JWT bug. Day 2: PAL spawns NEW Gemini with ZERO memory. Re-analyzes from scratch, might find different things.</li>
        <li>Delegation is MANUAL — no automatic routing. You explicitly call clink or write CLAUDE.md rules.</li>
        <li>25K token output limit — MCP tool responses capped, large analyses get truncated</li>
        <li>2-minute timeout — long-running Gemini tasks may timeout</li>
        <li>No rate limit awareness — doesn't know if Claude/Gemini approaching limits</li>
        <li>Context passed, not shared — Gemini sees files you specify, not Claude's reasoning state</li>
      </ul>
    </div>

    <div class="section-label gap">The Gap</div>
    <div class="section-content gap">
      <ul>
        <li><strong>Missing capability:</strong> Cross-session memory — Day 2 should know what Day 1 discovered without manual copy-paste</li>
        <li><strong>Missing capability:</strong> Automatic delegation — routing based on task type without explicit clink calls</li>
        <li><strong>Open question:</strong> Is manual delegation actually a feature (user control) or a friction point? Would automatic routing misroute often enough to be worse?</li>
        <li><strong>Open question:</strong> What's the right memory model? File-based (simple, grep-able) vs. vector DB (semantic search) vs. something else?</li>
      </ul>
    </div>

    <div class="section-label journey">User Journey</div>
    <div class="section-content journey">
      <p style="margin-bottom: 0.75rem;"><strong>Mental model:</strong> Each clink call spawns a fresh process. Claude maintains session continuity; Gemini resets every time. You consciously manage context handoff via files or tool parameters.</p>

      <div class="journey-scenario">
        <p><span class="time">Day 1, 10:00 AM</span> <span class="action">— You ask Claude to analyze 47 auth modules (180K tokens). CLAUDE.md says: "If >100K tokens, delegate." Claude calls <code>clink(cli_name="gemini", prompt="Analyze JWT validation")</code>. Gemini returns findings: "JWT expiration bug in auth.ts:42, missing rate limiting."</span></p>
        <p><span class="time">Day 1, 4:00 PM</span> <span class="action">— Session ends. You save key findings to <code>RESEARCH_FINDINGS.md</code> in your repo. This is manual — PAL doesn't persist Gemini's findings.</span></p>
        <p><span class="time">Day 2, 9:00 AM</span> <span class="action">— New session. You tell Claude: "Read RESEARCH_FINDINGS.md, then continue analysis." Claude has continuity (same orchestrator). But when you call clink again, Gemini is fresh — no memory of Day 1. You pass context explicitly: <code>clink(context="Yesterday found JWT bug...", prompt="Analyze new modules")</code>.</span></p>
      </div>

      <p style="margin-top: 0.75rem;"><strong>Division of responsibility:</strong></p>
      <ul>
        <li><strong>PAL handles:</strong> Spawning CLI processes, passing prompts, returning structured output, role-based system prompts</li>
        <li><strong>You handle:</strong> Deciding when to delegate (explicit clink or CLAUDE.md rules), maintaining cross-session context via files, passing context to fresh Gemini processes</li>
      </ul>

      <div class="journey-contrast">
        <div class="journey-contrast-item current">
          <div class="label">Without PAL MCP</div>
          <span style="color: var(--text-dim); font-size: 0.85rem;">Run <code>gemini -p "task" > result.md</code> manually. Read file back into Claude. Fragmented workflow.</span>
        </div>
        <div class="journey-contrast-item hypothetical">
          <div class="label">With Persistent Memory</div>
          <span style="color: var(--text-dim); font-size: 0.85rem;">Day 2: System queries memory: "What did Gemini find yesterday?" Auto-injects context into fresh process.</span>
        </div>
      </div>

      <p style="margin-top: 0.75rem; color: var(--text-muted);"><strong>Workflow fit:</strong> Clear, explicit control over delegation. You see exactly when Gemini is invoked and what context it receives. The trade-off: you manage continuity yourself via files and explicit context passing.</p>
    </div>
  </div>

  <!-- INTER-AGENT COMMUNICATION -->
  <div class="category">
    <div class="category-name">Inter-Agent Communication</div>
    <div class="category-desc">Tools that let multiple AI agents communicate with each other</div>
  </div>

  <div class="tool">
    <div class="tool-title">
      <span class="tool-name">hcom</span>
      <span class="tool-badge low">~35%</span>
    </div>
    <p class="tool-desc">
      Hook-based messaging layer using SQLite as a message bus. When Claude writes a file, a hook posts to SQLite. Gemini's hook polls SQLite and sees the event. Enables real-time inter-agent awareness through Claude Code's native hook system. Messages are structured event notifications (file_write, review_comment), not text chat.
    </p>

    <div class="section-label works">What It Does Well</div>
    <div class="section-content works">
      <ul>
        <li>Real-time inter-agent awareness — Claude writes rateLimit.ts → hook posts to SQLite → Gemini polls → reviews file → posts findings → Claude sees feedback. ~100 second loop.</li>
        <li>Collision detection — both agents notified if they touch same file</li>
        <li>Persistent event log — SQLite preserves history across polling gaps</li>
        <li>Native integration — works with Claude Code's existing hook system, no new infrastructure</li>
        <li>Lightweight — just SQLite + hook scripts</li>
        <li>Coordination without explicit orchestration — agents can react to each other without a central controller</li>
      </ul>
    </div>

    <div class="section-label why">Why It Works This Way</div>
    <div class="section-content why">
      <ul>
        <li><strong>Lightweight over heavyweight:</strong> SQLite + hooks is minimal infrastructure. No central orchestrator to run, maintain, or become a single point of failure.</li>
        <li><strong>No single point of failure:</strong> If the message bus goes down, agents still run independently. They just lose awareness, not functionality.</li>
        <li><strong>Agent autonomy:</strong> Agents decide how to react to messages. The bus doesn't prescribe behavior — it just broadcasts events.</li>
        <li><strong>Messaging ≠ Routing:</strong> hcom answers "what happened?" not "who should do this?" That's a deliberate scope boundary.</li>
        <li><strong>Native to Claude Code:</strong> Uses existing hook system. No new concepts to learn, no foreign infrastructure.</li>
      </ul>
    </div>

    <div class="section-label breaks">What Breaks</div>
    <div class="section-content breaks">
      <ul>
        <li><strong>Messaging only, NOT routing.</strong> hcom tells agents what happened, not what SHOULD happen. "Claude wrote auth.ts" is information, not a decision.</li>
        <li>Collision detected, not prevented — both agents edit same file, THEN see the collision. No locking before edit.</li>
        <li>No automatic context injection — messages are raw events, not synthesized actionable summaries</li>
        <li>Requires both agents running — Gemini must be active to receive messages</li>
        <li>Polling delays — 500ms-2sec gaps between event and reaction</li>
        <li>No failure handling — if Gemini's review times out, messages pile up unprocessed</li>
      </ul>
    </div>

    <div class="section-label gap">The Gap</div>
    <div class="section-content gap">
      <ul>
        <li><strong>Missing capability:</strong> Task routing — something that decides WHO should handle a task, not just announces what happened</li>
        <li><strong>Missing capability:</strong> Collision prevention — locking files before write, not detecting conflicts after</li>
        <li><strong>Open question:</strong> Is centralized routing better than autonomous agents? Central routing is clearer but creates a single point of failure.</li>
        <li><strong>Open question:</strong> Could hcom's event stream be enriched with routing hints without becoming a full orchestrator?</li>
      </ul>
    </div>

    <div class="section-label journey">User Journey</div>
    <div class="section-content journey">
      <p style="margin-bottom: 0.75rem;"><strong>Mental model:</strong> Agents discover each other through async event logs. hcom doesn't tell agents what to DO — it tells them what HAPPENED. Each agent decides how to react.</p>

      <div class="journey-scenario">
        <p><span class="time">10:30:45</span> <span class="action">— Claude writes <code>rateLimit.ts</code>. PostToolUse hook fires, posts to SQLite: <code>{agent: "claude", event: "file_write", file: "rateLimit.ts"}</code>.</span></p>
        <p><span class="time">10:30:48</span> <span class="action">— Gemini's hook polls SQLite (every ~1 sec), sees the event. Based on its configuration, Gemini spawns a review: "Analyze rateLimit.ts for bugs."</span></p>
        <p><span class="time">10:31:15</span> <span class="action">— Gemini finds: race condition line 42, missing error handling line 78. Posts findings back to SQLite.</span></p>
        <p><span class="time">10:31:32</span> <span class="action">— Claude's hook polls, sees Gemini's review. Claude reads feedback, fixes line 42.</span></p>
        <p><span class="time">10:32:00</span> <span class="action">— Meanwhile, you also asked Gemini to add tests. Both Claude and Gemini edit <code>rateLimit.ts</code>. hcom detects the collision AFTER both writes. You resolve the merge conflict manually.</span></p>
      </div>

      <p style="margin-top: 0.75rem;"><strong>Division of responsibility:</strong></p>
      <ul>
        <li><strong>hcom handles:</strong> Broadcasting events, persistent event log, collision detection (after the fact), native hook integration</li>
        <li><strong>You handle:</strong> Agent configuration (what events trigger what actions), resolving collisions, ensuring both agents are running, understanding that hcom is awareness not control</li>
      </ul>

      <p style="margin-top: 0.75rem; color: var(--text-muted);"><strong>Workflow fit:</strong> Parallel agents that need awareness of each other without centralized control. The trade-off: agents are autonomous (no single point of failure) but coordination is implicit (collisions detected, not prevented).</p>
    </div>
  </div>

  <!-- PARALLEL EXECUTION -->
  <div class="category">
    <div class="category-name">Parallel Execution</div>
    <div class="category-desc">Tools that run multiple AI agents simultaneously</div>
  </div>

  <div class="tool">
    <div class="tool-title">
      <span class="tool-name">Conductor Build</span>
      <span class="tool-badge low">~15%</span>
    </div>
    <p class="tool-desc">
      macOS Electron app (YC-backed, by Melty Labs) that runs multiple Claude Code instances in parallel, each isolated in its own git worktree. Visual dashboard monitors all agents simultaneously. Core idea: 5 independent features? Spin up 5 Claude instances, each on a different branch, merge when done. Conductor CREATES agents — it's the controller, not a viewer.
    </p>

    <div class="section-label works">What It Does Well</div>
    <div class="section-content works">
      <ul>
        <li>True parallel execution — 5 agents on 5 CPU cores simultaneously</li>
        <li>Git isolation via worktrees — Agent 1 modifies auth.ts without affecting Agent 2's copy. No merge conflicts during development.</li>
        <li>Visual dashboard — see all agents' progress, terminal output, file changes, diffs at a glance</li>
        <li>Merge coordination — helps combine branches when agents finish</li>
        <li>No manual git commands — worktree management is automatic</li>
        <li>YC-backed, actively developed with real support</li>
      </ul>
    </div>

    <div class="section-label why">Why It Works This Way</div>
    <div class="section-content why">
      <ul>
        <li><strong>Deep integration over broad integration:</strong> Conductor optimizes for Claude's strengths (precision coding, debugging). Supporting multiple models would dilute focus.</li>
        <li><strong>Simplicity of scope:</strong> 5 Claudes is simpler than Claude + Gemini + GPT-4. One model = consistent behavior, cost structure, performance expectations.</li>
        <li><strong>Merge strategy assumes homogeneity:</strong> Git worktree merging works because all agents produce similar code style. Mixed models would complicate merges.</li>
        <li><strong>Rate limit semantics are clear:</strong> One subscription, one pool. Easier to reason about than tracking multiple provider limits.</li>
        <li><strong>Visual-first design:</strong> A dashboard for 5 identical Claude instances is simpler than a dashboard showing heterogeneous model behaviors.</li>
      </ul>
    </div>

    <div class="section-label breaks">What Breaks</div>
    <div class="section-content breaks">
      <ul>
        <li><strong>5 agents = 5x rate limit burn.</strong> All use your single subscription. What lasts a week lasts 1-2 days. Your profile: 85% in 1-2 days. Conductor: 100% in ~8 hours.</li>
        <li>Claude-only — no Gemini, no GPT-4, no model selection for different task types</li>
        <li>No cross-agent communication — agents can't see what each other is doing</li>
        <li>No shared memory — each agent is fully isolated in its worktree</li>
        <li>No semantic task routing — you manually assign tasks to agents</li>
        <li>Silent failures — agents stall on 429 without clear notification</li>
        <li>Conductor solves "5 independent tasks, 48-hour sprint." Your problem is "continuous work across a full week."</li>
      </ul>
    </div>

    <div class="section-label gap">The Gap</div>
    <div class="section-content gap">
      <ul>
        <li><strong>Missing capability:</strong> Multi-provider support — using Claude AND Gemini to spread rate limits across independent pools</li>
        <li><strong>Missing capability:</strong> Rate limit awareness — knowing when to throttle or route to prevent 429s</li>
        <li><strong>Open question:</strong> Is parallel execution the right model for your workflow (burst work) or is sequential with intelligent routing (sustained work) better?</li>
        <li><strong>Open question:</strong> Would cross-agent communication be useful, or is isolation actually a feature (no interference)?</li>
      </ul>
    </div>

    <div class="section-label journey">User Journey</div>
    <div class="section-content journey">
      <p style="margin-bottom: 0.75rem;"><strong>Mental model:</strong> Parallelism via git worktree isolation. Each agent works in its own branch, can't interfere with others. But all 5 agents draw from your single Claude Max subscription — 5x the rate limit burn.</p>

      <div class="journey-scenario">
        <p><span class="time">Monday 9:00 AM</span> <span class="action">— You have 5 independent features to build. You open Conductor, click "Create 5 agents." Each gets its own git worktree: <code>feature-auth</code>, <code>feature-api</code>, <code>feature-tests</code>, etc.</span></p>
        <p><span class="time">Monday 11:00 AM</span> <span class="action">— All 5 agents are working simultaneously. Dashboard shows progress, terminal output, file changes. You're getting 5x the work done per hour.</span></p>
        <p><span class="time">Monday 2:00 PM</span> <span class="action">— Agent 4 stalls silently. You check — it hit 429. Your quota that normally lasts a week is now at 60% after 5 hours. Two more agents start failing.</span></p>
        <p><span class="time">Monday 4:00 PM</span> <span class="action">— Quota exhausted. All agents blocked. You've done 2 days of work in 7 hours, but now you're stuck until the weekly reset.</span></p>
      </div>

      <p style="margin-top: 0.75rem;"><strong>Division of responsibility:</strong></p>
      <ul>
        <li><strong>Conductor handles:</strong> Git worktree creation, agent spawning, visual dashboard, merge coordination</li>
        <li><strong>You handle:</strong> Rate limit pacing (Conductor doesn't throttle), manual task assignment (no semantic routing), recovering from silent 429 failures</li>
      </ul>

      <div class="journey-contrast">
        <div class="journey-contrast-item current">
          <div class="label">Your Profile</div>
          <span style="color: var(--text-dim); font-size: 0.85rem;">Already hit 85% in 1-2 days with 1 agent. Conductor would hit 100% in ~8 hours.</span>
        </div>
        <div class="journey-contrast-item hypothetical">
          <div class="label">With Multi-Provider</div>
          <span style="color: var(--text-dim); font-size: 0.85rem;">2 Claude agents + 3 Gemini agents = two independent quota pools. Sustained work across a week.</span>
        </div>
      </div>

      <p style="margin-top: 0.75rem; color: var(--text-muted);"><strong>Workflow fit:</strong> "5 independent tasks, 48-hour sprint" — burst work with merge at the end. Less suited for "continuous work across a full week" where you need to pace rate limit consumption.</p>
    </div>
  </div>

  <!-- QUOTA MONITORING -->
  <div class="category">
    <div class="category-name">Quota Monitoring</div>
    <div class="category-desc">Tools that track and display rate limit usage</div>
  </div>

  <div class="tool">
    <div class="tool-title">
      <span class="tool-name">CLIProxyAPI / CodexBar</span>
      <span class="tool-badge high">~60%</span>
    </div>
    <p class="tool-desc">
      Local proxy server with macOS menu bar widget showing real-time quota usage across AI providers. Also does round-robin load balancing if you have multiple API keys/accounts. Menu bar displays: Claude 78% | Gemini 15% | $4.23 today. Works with Claude Max subscriptions.
    </p>

    <div class="section-label works">What It Does Well</div>
    <div class="section-content works">
      <ul>
        <li>Glanceable quota visibility — menu bar always shows current usage percentages</li>
        <li>Multi-account load distribution — if you have 3 API keys, spreads load across them (3x effective quota)</li>
        <li>Post-hoc failover — if one account fails, tries the next</li>
        <li>Cost tracking — see total spend per provider per day</li>
        <li>Lightweight — just a menu bar widget, minimal overhead</li>
        <li>Works with Claude Max — tracks subscription usage, not just API</li>
      </ul>
    </div>

    <div class="section-label why">Why It Works This Way</div>
    <div class="section-content why">
      <ul>
        <li><strong>Observability vs control separation:</strong> CLIProxyAPI's job is to show you what's happening. YOUR job is to decide what to do about it.</li>
        <li><strong>User keeps agency:</strong> You see Claude at 85% and decide: "Keep going on this important debug" or "Switch to Gemini for this research." Human judgment.</li>
        <li><strong>Simplicity:</strong> Monitoring is straightforward (intercept, count tokens, display). Decision-making is hard and context-dependent.</li>
        <li><strong>Avoiding over-automation:</strong> Not every context warrants switching. You might WANT to exhaust Claude on critical debugging rather than switching to Gemini.</li>
        <li><strong>Multi-account focus:</strong> Round-robin makes sense for multiple SAME-provider accounts. Cross-provider routing is a different, more complex problem.</li>
      </ul>
    </div>

    <div class="section-label breaks">What Breaks</div>
    <div class="section-content breaks">
      <ul>
        <li><strong>Visibility without agency.</strong> 10:00 AM: Claude at 72%. 10:45 AM: Claude at 85%, still debugging. 11:15 AM: 429. You SAW quota but it didn't ACT. Display only.</li>
        <li>Reactive only — fails over AFTER 429. Request already failed, context may be lost.</li>
        <li>No semantic routing — treats all requests equally regardless of task type</li>
        <li>No proactive thresholds — can't configure "route to Gemini when Claude hits 80%"</li>
        <li>No prediction — shows current state, not "at this rate, 2.3 hours until limit"</li>
        <li>Round-robin loses caching benefits — Anthropic caches per-account, rotating breaks this</li>
      </ul>
    </div>

    <div class="section-label gap">The Gap</div>
    <div class="section-content gap">
      <ul>
        <li><strong>Missing capability:</strong> Proactive threshold routing — acting on quota data before hitting 429, not just displaying it</li>
        <li><strong>Missing capability:</strong> Rate prediction — "at current consumption rate, X hours until limit"</li>
        <li><strong>Open question:</strong> Is automatic routing at thresholds better than human judgment? User might WANT to burn Claude quota on important work.</li>
        <li><strong>Open question:</strong> What's the right threshold? 80%? 90%? Context-dependent? Should user configure it or should the system learn?</li>
      </ul>
    </div>

    <div class="section-label journey">User Journey</div>
    <div class="section-content journey">
      <p style="margin-bottom: 0.75rem;"><strong>Mental model:</strong> Real-time visibility, human decision-making. You see the numbers; you decide what to do. The tool shows you the score but doesn't play the game for you.</p>

      <div class="journey-scenario">
        <p><span class="time">10:00 AM</span> <span class="action">— Menu bar shows: Claude 72% | Gemini 15%. You're debugging a complex auth issue. You glance at the quota and think: "I have headroom, I'll keep going."</span></p>
        <p><span class="time">10:45 AM</span> <span class="action">— Menu bar: Claude 85%. You're close to a breakthrough. You think: "I should switch to Gemini for the research portion... but I don't want to break flow. I'll risk it."</span></p>
        <p><span class="time">11:15 AM</span> <span class="action">— Request fails: 429. Menu bar: Claude 100%. Your debugging session is interrupted. You manually start Gemini, copy context, re-explain the problem. 10 minutes lost.</span></p>
        <p><span class="time">11:25 AM</span> <span class="action">— You realize: "I SAW it at 85%. I could have switched. The tool showed me; I chose not to act." The visibility was there; the agency was yours.</span></p>
      </div>

      <p style="margin-top: 0.75rem;"><strong>Division of responsibility:</strong></p>
      <ul>
        <li><strong>CLIProxyAPI handles:</strong> Intercepting requests, counting tokens, displaying percentages, round-robin across accounts, post-hoc failover</li>
        <li><strong>You handle:</strong> Watching the numbers, deciding when to switch, accepting the consequences of staying on Claude at 85%</li>
      </ul>

      <div class="journey-contrast">
        <div class="journey-contrast-item current">
          <div class="label">Display-Only</div>
          <span style="color: var(--text-dim); font-size: 0.85rem;">You see "Claude 85%." You decide to keep going. You hit 429. That's your call — CLIProxyAPI showed you the data.</span>
        </div>
        <div class="journey-contrast-item hypothetical">
          <div class="label">With Proactive Routing</div>
          <span style="color: var(--text-dim); font-size: 0.85rem;">System sees 82%, sees next task is research. Routes to Gemini automatically. You keep debugging on Claude uninterrupted.</span>
        </div>
      </div>

      <p style="margin-top: 0.75rem; color: var(--text-muted);"><strong>Workflow fit:</strong> Users who want visibility but retain decision authority. The trade-off: you're informed but responsible. If you burn out at 85% on purpose because the work was worth it — that's a valid choice this tool respects.</p>
    </div>
  </div>

  <hr class="divider">

  <!-- SUMMARY TABLE -->
  <div class="summary-section">
    <div class="summary-title">Capability Matrix: What Exists Today?</div>

    <div class="capability-row">
      <span class="capability-name">Route by token count/metadata</span>
      <span class="capability-exists yes">YES</span>
      <span class="capability-who">CCProxy, Portkey, OpenRouter</span>
    </div>

    <div class="capability-row">
      <span class="capability-name">Auto-failover on 429</span>
      <span class="capability-exists yes">YES</span>
      <span class="capability-who">claude-code-mux, CLIProxyAPI</span>
    </div>

    <div class="capability-row">
      <span class="capability-name">Manual delegation to Gemini</span>
      <span class="capability-exists yes">YES</span>
      <span class="capability-who">PAL MCP (clink tool)</span>
    </div>

    <div class="capability-row">
      <span class="capability-name">Real-time agent messaging</span>
      <span class="capability-exists yes">YES</span>
      <span class="capability-who">hcom</span>
    </div>

    <div class="capability-row">
      <span class="capability-name">Parallel Claude instances</span>
      <span class="capability-exists yes">YES</span>
      <span class="capability-who">Conductor Build</span>
    </div>

    <div class="capability-row">
      <span class="capability-name">Quota visibility</span>
      <span class="capability-exists yes">YES</span>
      <span class="capability-who">CLIProxyAPI, CodexBar</span>
    </div>

    <div class="capability-row" style="margin-top: 1rem; padding-top: 1rem; border-top: 1px solid var(--border);">
      <span class="capability-name">Semantic task routing (research vs code)</span>
      <span class="capability-exists no">NO</span>
      <span class="capability-who">Nobody — requires upstream classifier</span>
    </div>

    <div class="capability-row">
      <span class="capability-name">Persistent cross-session memory</span>
      <span class="capability-exists no">NO</span>
      <span class="capability-who">Nobody — PAL has 3hr limit, hcom logs but doesn't inject</span>
    </div>

    <div class="capability-row">
      <span class="capability-name">Proactive rate prediction (before 429)</span>
      <span class="capability-exists no">NO</span>
      <span class="capability-who">Nobody — all are reactive</span>
    </div>

    <div class="capability-row">
      <span class="capability-name">Automatic mid-task context handoff</span>
      <span class="capability-exists no">NO</span>
      <span class="capability-who">Nobody — claude-code-mux loses reasoning state</span>
    </div>

    <div class="capability-row">
      <span class="capability-name">Cross-model critique orchestration</span>
      <span class="capability-exists no">NO</span>
      <span class="capability-who">Nobody — no loopback architecture</span>
    </div>
  </div>

  <hr class="divider">

  <!-- HONEST ASSESSMENT -->
  <div class="assessment">
    <div class="assessment-title">Honest Assessment: Is the Gap Worth Building For?</div>

    <p style="color: var(--text-dim); margin-bottom: 1rem;">
      The 40% gap is real, but concentrated in specific scenarios:
    </p>

    <div class="assessment-item" data-num="1.">
      <strong>If you rarely hit rate limits mid-task:</strong> CCProxy + PAL MCP + CodexBar is probably enough. Install baseline, use for 2-4 weeks, see if friction is real.
    </div>

    <div class="assessment-item" data-num="2.">
      <strong>If you hit limits mid-debug 2-3x/week:</strong> The context handoff gap is painful — you lose reasoning state, waste time re-explaining. Worth building minimal harness.
    </div>

    <div class="assessment-item" data-num="3.">
      <strong>If you want semantic routing:</strong> Nobody has this. You'd need to build the classifier regardless of other tools. But CLAUDE.md heuristics might be 80% as good.
    </div>

    <div class="assessment-item" data-num="4.">
      <strong>If you want cross-session memory:</strong> Nobody has this. You'd need to build the memory layer. But manual AGENTS.md files might work for light use.
    </div>

    <p style="color: var(--text-muted); margin-top: 1.5rem; font-size: 0.9rem;">
      Bottom line: The ONLY capability you truly can't replicate with existing tools + CLAUDE.md instructions is <strong>mid-task context handoff</strong>. Everything else has workarounds. Test the baseline before building.
    </p>
  </div>

  <hr class="divider">

  <!-- 5-LAYER ORCHESTRATION MODEL -->
  <div class="category">
    <div class="category-name">The 5-Layer Orchestration Model</div>
    <div class="category-desc">Where the harness sits in the stack — and why that matters</div>
  </div>

  <div class="tool" style="border-color: var(--purple); margin-bottom: 2rem;">
    <div class="tool-title">
      <span class="tool-name">The Layering Insight</span>
    </div>
    <p class="tool-desc">
      Research clarified WHERE the harness fits. It's not competing with PAL MCP or CCProxy — it sits <strong>above</strong> them at the Control Plane layer. The stack has 5 distinct layers, each with different market density.
    </p>

    <div class="scenario" style="font-size: 0.75rem; line-height: 1.5;">┌─────────────────────────────────────────────────────────────────┐
│ CONTROL PLANE (Who/Why/When)              ← THE HARNESS LIVES HERE
│ ├─ Semantic task-type routing             ← NOBODY does this yet
│ ├─ Cost optimization logic                ← NOBODY does this yet
│ ├─ Cross-model critique orchestration     ← NOBODY does this yet
│ ├─ Proactive rate limit prediction        ← NOBODY does this yet
│ └─ Unified approval policies              ← NOBODY does this yet
├─────────────────────────────────────────────────────────────────┤
│ ROUTING/DISPATCH PLANE                    ← CROWDED (PAL MCP, CCProxy)
│ ├─ Provider selection (which API)
│ ├─ Context serialization for hand-off
│ └─ Concurrent invocation / spawning
├─────────────────────────────────────────────────────────────────┤
│ PERSISTENCE PLANE                         ← SPARSE
│ ├─ Conductor (.md files) — single-provider only
│ └─ Cross-session memory — EMPTY
├─────────────────────────────────────────────────────────────────┤
│ OBSERVATION PLANE                         ← PARTIAL
│ ├─ CodexBar (rate limits)
│ └─ Routing analytics — EMPTY
├─────────────────────────────────────────────────────────────────┤
│ CLI/EXECUTION LAYER
│ ├─ Claude Code + PAL MCP
│ ├─ Gemini CLI + Conductor
│ └─ CCProxy interception
└─────────────────────────────────────────────────────────────────┘</div>

    <div class="section-label why">Key Insight: Conductor vs Harness</div>
    <div class="section-content why">
      <p style="margin-bottom: 0.75rem;"><strong>Gemini Conductor</strong> sits at the Persistence + Planning layer for a <strong>single provider</strong>:</p>
      <ul>
        <li>Context-as-code (specs, plans in markdown)</li>
        <li>Workflow structure (Spec → Plan → Implement)</li>
        <li>Brownfield project support</li>
      </ul>
      <p style="margin-top: 0.75rem;"><strong>The Harness</strong> sits at the <strong>Control Plane — above Conductor</strong>:</p>
      <ul>
        <li>Decides Claude vs Gemini for each task</li>
        <li>Manages persistent memory across BOTH providers</li>
        <li>Orchestrates cross-model critique</li>
        <li>Predicts rate limit exhaustion</li>
      </ul>
      <p style="margin-top: 0.75rem; color: var(--yellow);"><strong>They don't conflict. They stack.</strong> Conductor could be a component the harness uses when delegating to Gemini.</p>
    </div>
  </div>

  <!-- LAYER 1: CONTROL PLANE -->
  <div class="tool">
    <div class="tool-title">
      <span class="tool-name">Layer 1: Control Plane</span>
      <span class="tool-badge" style="background: rgba(248,81,73,0.15); color: var(--red);">EMPTY</span>
    </div>
    <p class="tool-desc">
      The harness brain. Makes strategic routing decisions <strong>before</strong> execution begins. Checks rate limits, analyzes task type, consults memory. Decides: Claude handles this? Delegate to Gemini? Trigger critique?
    </p>

    <div class="section-label journey">In The Journey (9:00 AM)</div>
    <div class="section-content journey">
      <div class="journey-scenario">
        <p><span class="action">Jane asks: "Add GitHub OAuth to our Express API. Our auth uses username/password with PostgreSQL. Don't break existing auth."</span></p>
        <p style="margin-top: 0.75rem;"><strong>Control Plane thinks:</strong></p>
        <p><span class="time">Rate check:</span> <span class="action">Claude 68%, Gemini 12%</span></p>
        <p><span class="time">Task type:</span> <span class="action">FEATURE_WITH_SECURITY, complexity 0.78</span></p>
        <p><span class="time">Token estimate:</span> <span class="action">70K (fits Claude's window)</span></p>
        <p style="margin-top: 0.5rem;"><span class="time">Decision:</span> <span class="action">CLAUDE_PRIMARY + GEMINI_CRITIQUE</span></p>
        <p><span class="time">Confidence:</span> <span class="action">92%</span></p>
        <p><span class="time">Logged:</span> <span class="action">route-001, reasoning captured</span></p>
      </div>
    </div>

    <div class="section-label gap">Contrast: Today vs With Harness</div>
    <div class="section-content gap">
      <div class="journey-contrast">
        <div class="journey-contrast-item current">
          <div class="label">Today</div>
          <ul style="margin: 0; padding-left: 1rem; color: var(--text-dim); font-size: 0.85rem;">
            <li>Manual: "Do I have quota left?"</li>
            <li>Guess: "Maybe use Gemini for research?"</li>
            <li>No record of why you chose</li>
          </ul>
        </div>
        <div class="journey-contrast-item hypothetical">
          <div class="label">With Harness</div>
          <ul style="margin: 0; padding-left: 1rem; color: var(--text-dim); font-size: 0.85rem;">
            <li>Ambient: Menu bar shows 68%</li>
            <li>FSM: "Task C ideal for Gemini (0.92 confidence)"</li>
            <li>Logged: route-001, reasoning captured</li>
          </ul>
        </div>
      </div>
    </div>

    <div class="section-label" style="color: var(--yellow);">Why This Matters</div>
    <div class="section-content" style="border-color: rgba(210,153,34,0.4);">
      <p><strong>Control Plane is where the 40% gap lives.</strong> Nobody does semantic routing, proactive rate prediction, or automated critique orchestration. This layer is <strong>EMPTY</strong> in the market. The routing/dispatch tools (CCProxy, PAL MCP) execute decisions — they don't make them.</p>
    </div>
  </div>

  <!-- LAYER 2: ROUTING/DISPATCH -->
  <div class="tool">
    <div class="tool-title">
      <span class="tool-name">Layer 2: Routing/Dispatch</span>
      <span class="tool-badge high">CROWDED</span>
    </div>
    <p class="tool-desc">
      The hands. Executes the Control Plane's decision. Spawns Gemini subprocess via MCP tool, serializes context for hand-off, captures streaming output. This is what PAL MCP and CCProxy do.
    </p>

    <div class="section-label journey">In The Journey (9:28 AM)</div>
    <div class="section-content journey">
      <div class="journey-scenario">
        <p><span class="action">Control Plane decided: "Delegate testing to Gemini"</span></p>
        <p style="margin-top: 0.75rem;"><strong>Routing executes:</strong></p>
        <p><span class="time">MCP Tool:</span> <span class="action">delegate_to_gemini</span></p>
        <p style="padding-left: 1rem; color: var(--text-muted);">task: "Write tests for OAuth flow"</p>
        <p style="padding-left: 1rem; color: var(--text-muted);">context: [file list, requirements, style guide]</p>
        <p style="margin-top: 0.5rem;"><span class="time">Spawns:</span> <span class="action"><code>gemini -p "..." --output-format stream-json</code></span></p>
        <p><span class="time">Captures:</span> <span class="action">Streaming JSON events</span></p>
        <p><span class="time">Returns:</span> <span class="action">tests/oauth.test.js to Claude</span></p>
      </div>
    </div>

    <div class="section-label gap">Contrast: Today vs With Harness</div>
    <div class="section-content gap">
      <div class="journey-contrast">
        <div class="journey-contrast-item current">
          <div class="label">Today</div>
          <ul style="margin: 0; padding-left: 1rem; color: var(--text-dim); font-size: 0.85rem;">
            <li>Manual: Open second terminal, run <code>gemini -p</code></li>
            <li>Manual: Copy Gemini output, paste to Claude</li>
            <li>User is the coordinator</li>
          </ul>
        </div>
        <div class="journey-contrast-item hypothetical">
          <div class="label">With Harness</div>
          <ul style="margin: 0; padding-left: 1rem; color: var(--text-dim); font-size: 0.85rem;">
            <li>Automatic: MCP tool handles subprocess</li>
            <li>Automatic: Context flows back via MCP</li>
            <li>Harness is the coordinator</li>
          </ul>
        </div>
      </div>
    </div>

    <div class="section-label" style="color: var(--yellow);">Why This Matters</div>
    <div class="section-content" style="border-color: rgba(210,153,34,0.4);">
      <p>This layer is <strong>CROWDED</strong> (PAL MCP, CCProxy). The routing/dispatch problem is mostly solved. <strong>The gap is in the layer ABOVE</strong> — deciding WHAT to route, not HOW to route it.</p>
    </div>
  </div>

  <!-- LAYER 3: EXECUTION -->
  <div class="tool">
    <div class="tool-title">
      <span class="tool-name">Layer 3: Execution</span>
      <span class="tool-badge" style="background: rgba(88,166,255,0.15); color: var(--blue);">SOLID</span>
    </div>
    <p class="tool-desc">
      Actually runs the task — reads files, writes code, runs commands. Claude uses native tools (Read, Edit, Bash). Gemini runs as headless subprocess, returns structured output.
    </p>

    <div class="section-label journey">In The Journey (9:05 - 9:30 AM)</div>
    <div class="section-content journey">
      <div class="journey-scenario">
        <p><strong>Claude Execution:</strong></p>
        <p><span class="time">9:05</span> <span class="action">— Reads auth.js, User.js, authRoutes.js (8K tokens)</span></p>
        <p><span class="time">9:10</span> <span class="action">— Creates OAUTH_DESIGN.md</span></p>
        <p><span class="time">9:15</span> <span class="action">— Writes src/oauth/github.js (280 lines)</span></p>
        <p><span class="time">9:25</span> <span class="action">— Writes src/routes/oauthRoutes.js (120 lines)</span></p>
        <p><span class="time">9:28</span> <span class="action">— Context window 77% → Warning triggered</span></p>
        <p style="margin-top: 0.75rem;"><strong>Gemini Execution (delegated):</strong></p>
        <p><span class="time">9:30</span> <span class="action">— Receives test requirements from Claude</span></p>
        <p><span class="time">9:32</span> <span class="action">— Generates tests/oauth.test.js (12K Gemini tokens)</span></p>
        <p><span class="time">9:32</span> <span class="action">— Returns via stream-json to Claude</span></p>
      </div>
    </div>

    <div class="section-label gap">Contrast: Today vs With Harness</div>
    <div class="section-content gap">
      <div class="journey-contrast">
        <div class="journey-contrast-item current">
          <div class="label">Today</div>
          <ul style="margin: 0; padding-left: 1rem; color: var(--text-dim); font-size: 0.85rem;">
            <li>Claude alone until it runs out</li>
            <li>Hit 429 → stop and wait</li>
            <li>Context window fills → stuck</li>
          </ul>
        </div>
        <div class="journey-contrast-item hypothetical">
          <div class="label">With Harness</div>
          <ul style="margin: 0; padding-left: 1rem; color: var(--text-dim); font-size: 0.85rem;">
            <li>Claude + Gemini seamlessly</li>
            <li>Approach limit → auto-delegate</li>
            <li>Context fills → Gemini continues</li>
          </ul>
        </div>
      </div>
    </div>

    <div class="section-label" style="color: var(--yellow);">Why This Matters</div>
    <div class="section-content" style="border-color: rgba(210,153,34,0.4);">
      <p><strong>Conductor sits HERE</strong> — it makes Gemini smarter about existing codebases via spec/plan workflow. The harness sits ABOVE, deciding <strong>when to USE Conductor</strong> vs other execution patterns.</p>
    </div>
  </div>

  <!-- LAYER 4: PERSISTENCE -->
  <div class="tool">
    <div class="tool-title">
      <span class="tool-name">Layer 4: Persistence (Memory)</span>
      <span class="tool-badge" style="background: rgba(248,81,73,0.15); color: var(--red);">SPARSE</span>
    </div>
    <p class="tool-desc">
      Stores what happened so future sessions know. Session memory tracks routing decisions and token usage. Cross-session memory captures key learnings, security notes, architectural decisions.
    </p>

    <div class="section-label journey">In The Journey (After Completion)</div>
    <div class="section-content journey">
      <div class="journey-scenario">
        <p><strong>Session Memory saved:</strong></p>
        <p style="padding-left: 1rem; color: var(--text-dim);">• Routing: 2 decisions (primary + delegation)</p>
        <p style="padding-left: 1rem; color: var(--text-dim);">• Tokens: Claude 72K, Gemini 12K</p>
        <p style="padding-left: 1rem; color: var(--text-dim);">• Files: 3 created</p>
        <p style="padding-left: 1rem; color: var(--text-dim);">• Issues: 1 HIGH fixed, 2 MEDIUM deferred</p>
        <p style="margin-top: 0.75rem;"><strong>Cross-Session Memory updated:</strong></p>
        <p style="padding-left: 1rem; color: var(--text-dim);">• PKCE flow implemented in this codebase</p>
        <p style="padding-left: 1rem; color: var(--text-dim);">• Rate limiting critical for OAuth callbacks (S001)</p>
        <p style="padding-left: 1rem; color: var(--text-dim);">• GitHub API needs 10s timeout (S003)</p>
      </div>
    </div>

    <div class="section-label gap">Contrast: Today vs With Harness</div>
    <div class="section-content gap">
      <div class="journey-contrast">
        <div class="journey-contrast-item current">
          <div class="label">Today</div>
          <ul style="margin: 0; padding-left: 1rem; color: var(--text-dim); font-size: 0.85rem;">
            <li>User remembers (or forgets)</li>
            <li>Tell Claude same thing twice</li>
            <li>Gemini doesn't know Claude's decisions</li>
          </ul>
        </div>
        <div class="journey-contrast-item hypothetical">
          <div class="label">With Harness</div>
          <ul style="margin: 0; padding-left: 1rem; color: var(--text-dim); font-size: 0.85rem;">
            <li>System remembers for you</li>
            <li>Claude reads from shared memory</li>
            <li>Both providers read/write same store</li>
          </ul>
        </div>
      </div>
    </div>

    <div class="section-label" style="color: var(--yellow);">Why This Matters</div>
    <div class="section-content" style="border-color: rgba(210,153,34,0.4);">
      <p>This layer is <strong>SPARSE</strong>. Conductor stores specs/plans in files (single provider). Nobody has <strong>cross-provider semantic memory</strong>. This is part of the 40% gap — work discovered by Gemini should be available to Claude tomorrow without copy-paste.</p>
    </div>
  </div>

  <!-- LAYER 5: OBSERVATION -->
  <div class="tool">
    <div class="tool-title">
      <span class="tool-name">Layer 5: Observation (Dashboard)</span>
      <span class="tool-badge medium">PARTIAL</span>
    </div>
    <p class="tool-desc">
      Shows what happened and why. Rate limit gauges, cost tracking, routing analytics, activity timeline, critique history. CodexBar does part of this — nobody does routing analytics.
    </p>

    <div class="section-label journey">In The Journey (Always Visible)</div>
    <div class="section-content journey">
      <div class="scenario" style="font-size: 0.75rem; line-height: 1.4;">Menu Bar Widget:
  Claude: 76% │ Gemini: 22% │ Cost: $0.035

Dashboard (localhost:3200):
┌─ Activity ─────────────────────┐
│ 9:00  Session start            │
│ 9:03  Task → Claude (code)     │
│ 9:28  Task → Gemini (tests)    │
│ 9:33  Critique requested       │
│ 9:36  APPROVED                 │
└────────────────────────────────┘

┌─ Routing Analytics ────────────┐
│ Why Claude primary?            │
│ • Task type: code (95% match)  │
│ • Context: 70K (fits window)   │
│ • Rate limit: 68% (safe)       │
│ Confidence: 92%                │
└────────────────────────────────┘</div>
    </div>

    <div class="section-label gap">Contrast: Today vs With Harness</div>
    <div class="section-content gap">
      <div class="journey-contrast">
        <div class="journey-contrast-item current">
          <div class="label">Today</div>
          <ul style="margin: 0; padding-left: 1rem; color: var(--text-dim); font-size: 0.85rem;">
            <li>Check CodexBar for rate limits</li>
            <li>No idea why decisions happened</li>
            <li>Cost scattered across providers</li>
          </ul>
        </div>
        <div class="journey-contrast-item hypothetical">
          <div class="label">With Harness</div>
          <ul style="margin: 0; padding-left: 1rem; color: var(--text-dim); font-size: 0.85rem;">
            <li>Rate limits + routing + cost in one view</li>
            <li>Full routing rationale logged</li>
            <li>Unified cost + "would have cost"</li>
          </ul>
        </div>
      </div>
    </div>

    <div class="section-label" style="color: var(--yellow);">Why This Matters</div>
    <div class="section-content" style="border-color: rgba(210,153,34,0.4);">
      <p>CodexBar shows rate limits. Nobody shows <strong>routing analytics</strong> or "this cost $0.45 on Claude, would have cost $0.12 on Gemini." Understanding WHY decisions were made is part of the 40% gap.</p>
    </div>
  </div>

  <hr class="divider">

  <!-- CROSS-MODEL CRITIQUE -->
  <div class="category">
    <div class="category-name">Cross-Model Critique: Team of Rivals</div>
    <div class="category-desc">The pattern that turns 60% accuracy into 90%</div>
  </div>

  <div class="tool">
    <div class="tool-title">
      <span class="tool-name">Critique Orchestration</span>
      <span class="tool-badge" style="background: rgba(248,81,73,0.15); color: var(--red);">NOBODY DOES THIS</span>
    </div>
    <p class="tool-desc">
      arxiv 2601.14351 demonstrates that cross-model critique (having a different model review work) improves accuracy from 60% to 90% on critical tasks. The harness would automate this — security tasks trigger Gemini review, architecture decisions trigger second-opinion.
    </p>

    <div class="section-label journey">In The Journey (9:33 AM - Security Review)</div>
    <div class="section-content journey">
      <div class="journey-scenario">
        <p><span class="action">Claude finished implementing OAuth. Control Plane triggers critique based on task type.</span></p>
        <p style="margin-top: 0.75rem;"><span class="time">MCP Tool:</span> <span class="action">delegate_to_gemini_for_critique</span></p>
        <p style="padding-left: 1rem; color: var(--text-muted);">mode: SECURITY_REVIEW</p>
        <p style="padding-left: 1rem; color: var(--text-muted);">files: [github.js, oauthRoutes.js]</p>
        <p style="margin-top: 0.75rem;"><strong>Gemini finds 4 issues:</strong></p>
        <p style="padding-left: 1rem;"><span style="color: var(--red);">S001 (HIGH):</span> <span class="action">Missing rate limiting on callback endpoint</span></p>
        <p style="padding-left: 1rem;"><span style="color: var(--yellow);">S002 (MEDIUM):</span> <span class="action">Session storage not durable</span></p>
        <p style="padding-left: 1rem;"><span style="color: var(--yellow);">S003 (MEDIUM):</span> <span class="action">No timeout on GitHub API calls</span></p>
        <p style="padding-left: 1rem;"><span style="color: var(--text-muted);">S004 (LOW):</span> <span class="action">Missing error logging</span></p>
        <p style="margin-top: 0.75rem;"><span class="time">9:35 AM</span> <span class="action">— Claude fixes S001 + S003 (HIGH + easy MEDIUM)</span></p>
        <p><span class="time">9:36 AM</span> <span class="action">— Gemini re-reviews: <span style="color: var(--green);">APPROVED</span></span></p>
      </div>
    </div>

    <div class="section-label gap">Contrast: Today vs With Harness</div>
    <div class="section-content gap">
      <div class="journey-contrast">
        <div class="journey-contrast-item current">
          <div class="label">Today</div>
          <ul style="margin: 0; padding-left: 1rem; color: var(--text-dim); font-size: 0.85rem;">
            <li>Manual: "Can you review for security?"</li>
            <li>Self-review: Same model misses same bugs</li>
            <li>60% accuracy on critical issues</li>
          </ul>
        </div>
        <div class="journey-contrast-item hypothetical">
          <div class="label">With Harness</div>
          <ul style="margin: 0; padding-left: 1rem; color: var(--text-dim); font-size: 0.85rem;">
            <li>Automatic: Security tasks trigger cross-review</li>
            <li>Different model = different blind spots</li>
            <li>90% accuracy (arxiv 2601.14351)</li>
          </ul>
        </div>
      </div>
    </div>

    <div class="section-label" style="color: var(--yellow);">Why This Matters</div>
    <div class="section-content" style="border-color: rgba(210,153,34,0.4);">
      <p><strong>Nobody automates cross-model critique.</strong> You can do it manually (ask Gemini to review Claude's work), but the harness would: (1) detect security-sensitive tasks, (2) automatically trigger critique, (3) feed findings back for fixes, (4) re-validate. The 60%→90% accuracy gain is real but currently requires manual coordination.</p>
    </div>
  </div>

  <hr class="divider">

  <!-- FAILURE SCENARIOS -->
  <div class="category">
    <div class="category-name">Failure Scenarios</div>
    <div class="category-desc">What breaks and how recovery works</div>
  </div>

  <div class="tool">
    <div class="tool-title">
      <span class="tool-name">Failure Mode Playbook</span>
    </div>
    <p class="tool-desc">
      The harness must handle failures gracefully. These are the scenarios the journey accounts for.
    </p>

    <div style="margin: 1rem 0;">
      <div class="capability-row">
        <span class="capability-name">Context window full</span>
        <span class="capability-exists" style="color: var(--yellow); min-width: 120px;">93% utilization</span>
        <span class="capability-who">Auto-delegate mid-phase to Gemini</span>
      </div>

      <div class="capability-row">
        <span class="capability-name">Gemini subprocess timeout</span>
        <span class="capability-exists" style="color: var(--yellow); min-width: 120px;">MCP 60s limit</span>
        <span class="capability-who">Kill + retry with smaller context</span>
      </div>

      <div class="capability-row">
        <span class="capability-name">Security critique rejects</span>
        <span class="capability-exists" style="color: var(--red); min-width: 120px;">CRITICAL issue</span>
        <span class="capability-who">Escalate to user, get design decision</span>
      </div>

      <div class="capability-row">
        <span class="capability-name">Both providers exhausted</span>
        <span class="capability-exists" style="color: var(--red); min-width: 120px;">95% + 100%</span>
        <span class="capability-who">Checkpoint work, notify, pause</span>
      </div>

      <div class="capability-row">
        <span class="capability-name">Memory stale/conflicting</span>
        <span class="capability-exists" style="color: var(--yellow); min-width: 120px;">Old data</span>
        <span class="capability-who">Flag to user, ask for refresh</span>
      </div>

      <div class="capability-row">
        <span class="capability-name">Routing confidence low</span>
        <span class="capability-exists" style="color: var(--text-muted); min-width: 120px;">&lt;70%</span>
        <span class="capability-who">Ask user which provider to use</span>
      </div>
    </div>
  </div>

  <hr class="divider">

  <!-- 40% GAP SUMMARY -->
  <div class="summary-section">
    <div class="summary-title">The 40% Gap: Final Synthesis</div>

    <p style="color: var(--text-dim); margin-bottom: 1.5rem;">
      Existing tools get 55-60%. The remaining 40% is genuine and multiplicative — each gap compounds the others.
    </p>

    <div class="capability-row" style="background: var(--bg-card); padding: 0.75rem; border-radius: 6px;">
      <span class="capability-name"><strong>Gap</strong></span>
      <span class="capability-exists" style="color: var(--text); font-weight: 600; min-width: 140px;">Today</span>
      <span class="capability-who"><strong>What Harness Adds</strong></span>
    </div>

    <div class="capability-row">
      <span class="capability-name">Semantic Routing</span>
      <span class="capability-exists" style="color: var(--text-muted); min-width: 140px;">Rule-based (tokens)</span>
      <span class="capability-who">Task-type classification (code/research/architecture)</span>
    </div>

    <div class="capability-row">
      <span class="capability-name">Cross-Provider Memory</span>
      <span class="capability-exists" style="color: var(--text-muted); min-width: 140px;">None (user is memory)</span>
      <span class="capability-who">Shared AGENTS.md + optional Mem0</span>
    </div>

    <div class="capability-row">
      <span class="capability-name">Rate Prediction</span>
      <span class="capability-exists" style="color: var(--text-muted); min-width: 140px;">Reactive (429 → notice)</span>
      <span class="capability-who">Proactive (approaching → auto-route)</span>
    </div>

    <div class="capability-row">
      <span class="capability-name">Critique Orchestration</span>
      <span class="capability-exists" style="color: var(--text-muted); min-width: 140px;">Manual (ask both)</span>
      <span class="capability-who">Automated (security → auto-trigger)</span>
    </div>

    <div class="capability-row">
      <span class="capability-name">Integration</span>
      <span class="capability-exists" style="color: var(--text-muted); min-width: 140px;">4 separate tools</span>
      <span class="capability-who">One invisible system</span>
    </div>
  </div>

  <div class="assessment">
    <div class="assessment-title">The Honest Question</div>
    <p style="color: var(--text-dim); margin-bottom: 1rem;">
      Is the 40% gap worth months of building? The answer depends on how often you hit the friction:
    </p>

    <div class="assessment-item" data-num="→">
      <strong>If rate limits rarely interrupt mid-task:</strong> The baseline (PAL MCP + CCProxy + CodexBar) is probably enough.
    </div>

    <div class="assessment-item" data-num="→">
      <strong>If you lose context 2-3x/week to 429s:</strong> The harness pays for itself in recovered time.
    </div>

    <div class="assessment-item" data-num="→">
      <strong>If security bugs slip through:</strong> Cross-model critique (60%→90%) is the highest-ROI feature.
    </div>

    <div class="assessment-item" data-num="→">
      <strong>If you re-explain context constantly:</strong> Persistent memory is the highest-ROI feature.
    </div>

    <p style="color: var(--yellow); margin-top: 1.5rem; font-weight: 500;">
      Two paths forward:
    </p>
    <p style="color: var(--text-dim); margin-top: 0.5rem;">
      <strong>Path A:</strong> Validate with zero-code baseline first. Install existing tools, use for 1 week of real work. Does the 40% gap matter in practice?
    </p>
    <p style="color: var(--text-dim); margin-top: 0.5rem;">
      <strong>Path B:</strong> Design the Control Plane layer. What does semantic routing actually look like? What triggers cross-model critique? How does memory unify across providers?
    </p>
  </div>

  </div><!-- end tab-landscape -->

  <div id="tab-investigation" class="tab-panel">

  <h1>Investigation Questions</h1>
  <p class="subtitle">7 primary questions mapped to the 5-layer architecture, researched in parallel</p>

  <div class="context">
    <div class="context-row">
      <span class="context-key">Questions</span>
      <span class="context-value">7 primary (IQ-1 through IQ-7) + 4 auxiliary</span>
    </div>
    <div class="context-row">
      <span class="context-key">Research</span>
      <span class="context-value">7 parallel agents, 90+ sources across frameworks, papers, and production systems</span>
    </div>
    <div class="context-row">
      <span class="context-key">Card Dimensions</span>
      <span class="context-value">Core Tension / Research Findings / What Could Go Wrong / Design Implications / User Journey / What This Makes You Realize / Open Threads</span>
    </div>
  </div>

  <!-- DEPENDENCY GRAPH -->
  <div class="dep-graph">
    <div style="font-weight: 600; color: var(--purple); margin-bottom: 1rem; font-size: 0.8rem; text-transform: uppercase; letter-spacing: 0.08em;">Investigation Dependency Graph</div>
    <pre>
                    ┌─────────────┐
                    │    IQ-1     │  DECOMPOSITION BEFORE CLASSIFICATION
                    │ (Layer 1,2) │  [ROOT NODE — no dependencies]
                    └──────┬──────┘
                           │
              ┌────────────┼────────────┐
              │            │            │
              ▼            ▼            ▼
     ┌────────────┐  ┌──────────┐  ┌──────────┐
     │   IQ-7     │  │   IQ-3   │  │   IQ-4   │
     │ Semantic   │  │ Routing  │  │ LLM      │
     │ vs Heur.   │  │ Nuance   │  │ Costs    │
     └─────┬──────┘  └────┬─────┘  └─────▲────┘
           │              │               │
           │              ├───────────────┘
           │              │
           │         ┌────▼─────┐
           └────────►│   IQ-5   │  VISUAL INTERFACE
                     │ (Layer 4)│
                     └──────────┘

   PARALLEL TRACKS (independent):

     ┌─────────────┐        ┌─────────────┐
     │    IQ-2     │        │    IQ-6     │
     │  Conductor  │        │  MCP Server │
     └─────────────┘        └─────────────┘</pre>
  </div>

  <!-- WAVE 1 -->
  <div class="category">
    <div class="category-name">Wave 1 — Parallel Start</div>
    <div class="category-desc">Root node + two independent tracks. No dependencies. Research these simultaneously.</div>
  </div>

  <!-- IQ-1 -->
  <div class="tool">
    <div class="tool-title">
      <span class="tool-name">IQ-1: Task Decomposition Before Classification</span>
      <span class="wave-badge">Wave 1</span>
      <span class="layer-badge">Layer 1, 2</span>
    </div>
    <p class="tool-desc">
      Should we decompose tasks into subtasks BEFORE classification and routing? <code>prompt → decompose → classify each → route each</code> instead of <code>prompt → classify → route</code>. Example: "refactor auth module to use JWT" contains research, architecture, code, bulk editing, and testing subtasks — each potentially routed to a different model. This is the root investigation node: the answer reshapes every other question.
    </p>

    <div class="section-label tension">The Core Tension</div>
    <div class="section-content tension">
      <p style="margin-bottom: 0.75rem;">A recursive paradox on three levels:</p>
      <ul>
        <li><strong>The meta-cognition cost:</strong> Decomposition itself requires intelligence (an LLM call). But the harness exists to SAVE LLM calls. You spend Claude tokens to decide how to save Claude tokens. A single decomposition costs 500-2,000 tokens — if it routes 3/5 subtasks to Gemini saving 30K-150K Claude tokens, ROI is positive. For "fix typo in README"? Catastrophically negative.</li>
        <li><strong>The chicken-and-egg:</strong> You can't know whether to decompose without classifying complexity. But you can't accurately classify a compound task without decomposing it. "Refactor auth module" is classified as "code" by surface-level routing — but it CONTAINS research, architecture, code, bulk editing, and testing.</li>
        <li><strong>The quality variance:</strong> A good decomposition produces 5 well-scoped subtasks. A bad one produces "understand auth, do JWT stuff, test it." Bad decomposition → bad routing → worse results than no decomposition. Who validates decomposition quality without triggering the same self-review trap (arxiv 2601.14351)?</li>
      </ul>
    </div>

    <div class="section-label works">Research Findings</div>
    <div class="section-content works">
      <p style="margin-bottom: 0.75rem;"><strong>Decompose-before-execute is the dominant pattern in agent frameworks — but multi-model routing after decomposition is rare.</strong></p>
      <ul>
        <li><strong>LangGraph LLMCompiler (ICML 2024):</strong> Generates a DAG of tasks with dependencies. <strong>3.7x latency speedup, 6.7x cost savings, ~9% accuracy improvement</strong> over ReAct. Decomposes to parallelize within a single provider.</li>
        <li><strong>Manus AI:</strong> The closest production system to the harness vision. Three agent types: planning → execution → verification. "Stitches multiple models for specialized roles, including Claude 3.5/3.7, Qwen, and dynamic routing to modern GPT/Gemini families." Decomposes AND routes across models.</li>
        <li><strong>Google ADK:</strong> Treats decomposition and routing as independent choices. Does NOT prescribe "decompose before routing." Recommends: "Start with a sequential chain, debug it, then add complexity."</li>
        <li><strong>RouteLLM / Aurelio:</strong> Do NOT decompose. Classify the whole prompt, route to one model. Designed for atomic queries, not compound tasks.</li>
        <li><strong>The two-stage solution (AWS + vLLM):</strong> Lightweight complexity estimation (~20ms, no LLM) → if compound, decompose with LLM (~1-2s); if simple, route directly. This resolves the chicken-and-egg.</li>
      </ul>
      <div class="scenario">Framework         | Decompose First? | Multi-Model? | Dynamic Replan?
LangGraph         | Yes (plan-exec)  | No           | Yes
LLMCompiler       | Yes (DAG)        | No           | Yes (streaming)
CrewAI            | Implicit (roles) | No           | No
Manus AI          | Yes (explicit)   | YES          | Yes
RouteLLM/Aurelio  | No               | Yes (route)  | No
Google ADK        | Both available   | No           | Partial</div>
    </div>

    <div class="section-label breaks">What Could Go Wrong</div>
    <div class="section-content breaks">
      <ul>
        <li><strong>Over-decomposition:</strong> "Fix typo in README" becomes 5 subtasks. Adds 500+ token overhead and 200ms+ latency for a 2-second task. If 70%+ of daily tasks are simple, constant unnecessary overhead.</li>
        <li><strong>Error propagation (MAST study, 1,600+ traces):</strong> "A slightly off-target decomposition results in irrelevant search queries, feeding unusable data to a summarizer." One bad decomposition cascades through all subtasks.</li>
        <li><strong>Deeply interdependent subtasks:</strong> "Debug why auth returns 403 for valid tokens" — answer to subtask 1 determines what subtasks 2 and 3 even mean. Forces sequential execution, negating parallelism benefit.</li>
        <li><strong>Latency penalty:</strong> 1-2s decomposition delay before work starts. For a tool promising to be "invisible," a perceivable delay on every interaction is destructive.</li>
        <li><strong>Category escalation:</strong> Adding decomposition transforms the harness from "intelligent router" to "agent orchestration framework." Competes with LangGraph, not just RouteLLM. Build timeline expands from weeks to months.</li>
      </ul>
    </div>

    <div class="section-label gap">Design Implications</div>
    <div class="section-content gap">
      <ul>
        <li><strong>Layer 1 becomes two-stage:</strong> <code>prompt → complexity_estimate (embedding, ~20ms) → simple? classify+route : decompose → classify each → route each</code>. No longer a single classification step — a conditional pipeline with a complexity gate.</li>
        <li><strong>Layer 2 makes N decisions:</strong> For compound tasks, N routing decisions instead of 1. Needs subtask dependency tracking (DAG), cross-subtask routing optimization ("subtask 1 on Claude, should subtask 2 also stay on Claude for continuity?"), and parallel execution orchestration.</li>
        <li><strong>Layer 3 needs richer schema:</strong> Decomposition templates, subtask dependency graphs, per-subtask routing outcomes. This argues for structured storage (SQLite) over flat files — can't efficiently query "what decomposition pattern worked for refactoring tasks?" from markdown.</li>
        <li><strong>Layer 4 shows decomposition trees:</strong> Not "Task → Claude" but a tree showing each subtask's routing, progress, and outcomes. Significantly more informative but more complex to build.</li>
        <li><strong>Layer 5 — invisible or approved?:</strong> Three options: fully invisible (risk: user doesn't understand delays), approval required (adds friction), or <strong>invisible by default, visible on demand</strong> (aligns with "silent in experience, visual for granularity").</li>
      </ul>
    </div>

    <div class="section-label journey">User Journey Impact</div>
    <div class="section-content journey">
      <div class="journey-contrast">
        <div class="journey-contrast-item current">
          <div class="label">Classify-First (Current)</div>
          <span style="color: var(--text-dim); font-size: 0.85rem;">"refactor auth to JWT" → classified as "code" (0.91) → Claude handles everything. ~100ms to first action. Claude may waste tokens on research subtasks Gemini could handle.</span>
        </div>
        <div class="journey-contrast-item hypothetical">
          <div class="label">Decompose-First (Proposed)</div>
          <span style="color: var(--text-dim); font-size: 0.85rem;">"refactor auth to JWT" → compound → decomposed into 5 subtasks → search→Gemini, design→Claude, modify→Claude, update→Gemini, test→Claude. ~1.5-2s to first action but better utilization.</span>
        </div>
      </div>
      <p style="margin-top: 0.75rem; color: var(--text-muted);"><strong>Simple tasks are unaffected</strong> — the complexity gate routes them through the fast path (~100ms). The decomposition delay only applies to genuinely compound tasks. Power users can force decomposition (<code>/decompose</code>) or prevent it (<code>/simple</code>).</p>
    </div>

    <div class="section-label realize">What This Makes You Realize</div>
    <div class="section-content realize">
      <ul>
        <li><strong>Recomposition is the harder problem:</strong> 5 subtask results from 2 models need synthesis. Who synthesizes? Claude. But Claude only has Gemini's summarized output, not full context. Synthesis quality depends on Layer 3 memory quality. Decomposition → recomposition → memory → schema standardization — cascading requirements.</li>
        <li><strong>This makes the harness an agent framework:</strong> Decompose + track dependencies + parallel execute + handle per-subtask failures + synthesize = full agent runtime. Category shift from "intelligent proxy" to "agent orchestration." Competes with LangGraph/ADK, not RouteLLM. Build scope expands significantly.</li>
        <li><strong>Emergent subtasks are unavoidable:</strong> During subtask 2 (design JWT), Claude discovers OAuth flows need updating → subtask 6 emerges. Static plans break. Need dynamic replanning (like Manus). More infrastructure complexity.</li>
        <li><strong>The chicken-and-egg has a known solution:</strong> Two-stage hybrid routing. Embedding-based complexity estimation (fast, no LLM) → conditional decomposition. Only compound tasks pay the cost.</li>
        <li><strong>Memory makes decomposition better over time:</strong> Store which decomposition patterns worked, which subtask types route well to which model. First week: conservative, occasionally wrong. Month two: calibrated to actual patterns. <strong>Decomposition should NOT be in MVP</strong> — it should come after memory has data.</li>
        <li><strong>Option B may be simpler:</strong> Instead of harness-level decomposition, leverage Claude's native decomposition (Task tool/subagents). When Claude decides to delegate, the harness routes the delegation. Claude does what it already does; harness just gets smart about WHERE each piece runs.</li>
      </ul>
    </div>

    <div class="section-label threads">Open Threads</div>
    <div class="section-content threads">
      <ul>
        <li>Could a lighter model (Gemini Flash) handle decomposition, saving Claude tokens? Decomposition is planning, not execution — some research suggests lighter models suffice.</li>
        <li>What is the complexity threshold for the two-stage gate? Confidence score? Token count? Keyword patterns? Empirical question needing real-world testing.</li>
        <li>Does Claude's native decomposition (Option B) make harness-level decomposition unnecessary? Test the zero-code baseline first.</li>
        <li>How does decomposition interact with the veto pattern? Per-subtask critique? Or whole-task only?</li>
        <li>How deep should decomposition go? One level for routing, let executing model handle further decomposition internally.</li>
      </ul>
    </div>
  </div>

  <!-- IQ-6 -->
  <div class="tool">
    <div class="tool-title">
      <span class="tool-name">IQ-6: MCP Server Reliability, Tools & Robustness</span>
      <span class="wave-badge">Wave 1</span>
      <span class="layer-badge">Layer 5</span>
    </div>
    <p class="tool-desc">
      The delivery mechanism for the entire 5-layer architecture. LLM-based tool selection is unreliable — Claude DECIDES whether to call MCP tools, with 12-50% failure rates depending on conditions. How do you make an invisible harness that's also reliable? Research uncovered a critical hooks + MCP hybrid approach that reframes the entire delivery architecture.
    </p>

    <div class="section-label tension">The Core Tension</div>
    <div class="section-content tension">
      <ul>
        <li><strong>Inference-based invocation is inherently unreliable.</strong> MCP tools are suggestions, not guarantees. Claude decides whether to call <code>delegate_to_gemini</code> based on its reasoning. If Claude doesn't think to call your tool, the harness doesn't activate. Native tools (Read, Edit, Bash) always win in discoverability because they're hardcoded.</li>
        <li><strong>Tool Search compounds the problem.</strong> When context contains >10% tool definitions, Claude uses a discovery step (Tool Search) that further reduces reliability. More harness tools = less reliable invocation of any individual tool.</li>
        <li><strong>Invisibility requires reliability.</strong> If the harness is invisible but unreliable, it's worse than no harness — the user doesn't know when it's working vs when it silently failed. An invisible system that works 70% of the time is an invisible system you can't trust.</li>
      </ul>
    </div>

    <div class="section-label works">Research Findings</div>
    <div class="section-content works">
      <p style="margin-bottom: 0.75rem;"><strong>The breakthrough: Hooks (guaranteed) + MCP tools (optional) = reliability without fragility.</strong></p>
      <ul>
        <li><strong>Claude Code hooks</strong> fire on lifecycle events — always, deterministically, regardless of what Claude decides to do. They are shell commands executed by the runtime, not LLM-chosen tool calls.</li>
        <li><strong>UserPromptSubmit</strong> is the linchpin — fires on EVERY prompt. Can inject context that shapes Claude's reasoning: "You are at 80% Claude usage. Consider delegating research tasks to Gemini." Claude's natural reasoning then aligns with what the harness needs. No tool invocation required.</li>
        <li><strong>PreToolUse</strong> can block tool calls — if Claude tries to use a tool when rate-limited, the hook can intercept and prevent it. Hard enforcement.</li>
        <li><strong>PostToolUse</strong> records every tool call — tracking, counters, telemetry. Zero LLM cost.</li>
        <li><strong>Stop hook</strong> fires at session end — write session summary to memory. Guaranteed persistence.</li>
      </ul>

      <div class="scenario">RELIABILITY MODEL: THREE TIERS

Tier 1 — CLAUDE.md (Soft Steering)
  "When task > 100K tokens, use delegate_to_gemini"
  Always read. Claude decides whether to follow.
  Reliability: ~60-80%

Tier 2 — Hooks (Hard Enforcement)
  UserPromptSubmit: inject rate state + memory
  PreToolUse: block if rate-limited
  PostToolUse: track usage
  Stop: persist session to memory
  Reliability: 100% (they always fire)

Tier 3 — MCP Tools (Capability Extension)
  delegate_to_gemini, request_critique, query_memory
  Claude chooses when to invoke.
  Reliability: ~50-88% (depends on context)

CRITICAL PATH = Tier 2. Optional enhancements = Tier 3.
System doesn't break if MCP tools aren't called.</div>
    </div>

    <div class="section-label breaks">What Could Go Wrong</div>
    <div class="section-content breaks">
      <ul>
        <li><strong>Claude never calls <code>delegate_to_gemini</code></strong> — even with CLAUDE.md instructions, native tool preference wins. Mitigation: hooks inject context that makes Claude want to delegate naturally.</li>
        <li><strong>Hook injection changes reasoning unexpectedly</strong> — injecting "You're at 80% usage" might make Claude overly conservative, refusing to do work it should handle. The injection must be informational, not directive.</li>
        <li><strong>Too many MCP tools trigger Tool Search</strong> — exposing 8+ harness tools alongside other MCP servers pushes past the threshold. Keep harness tools minimal (3-5 max).</li>
        <li><strong>MCP server crashes mid-session</strong> — hooks are separate from MCP; they keep working. But MCP-dependent features (delegation, critique) stop. Need graceful degradation.</li>
        <li><strong>Hook latency</strong> — UserPromptSubmit adds processing time before Claude sees the prompt. If hook script takes 500ms (reading memory, checking rates), the user feels it on every interaction.</li>
      </ul>
    </div>

    <div class="section-label gap">Design Implications</div>
    <div class="section-content gap">
      <p style="margin-bottom: 0.75rem;"><strong>The hooks + MCP split for each harness function:</strong></p>
      <div class="scenario">FUNCTION              │ HOOK (guaranteed)              │ MCP TOOL (optional)
──────────────────────┼────────────────────────────────┼──────────────────────
Rate checking         │ UserPromptSubmit: inject state  │ check_rate_status
Memory injection      │ UserPromptSubmit: inject context│ query_memory (deep)
Rate enforcement      │ PreToolUse: block if exhausted  │ —
Usage tracking        │ PostToolUse: update counters    │ —
Delegation            │ —                               │ delegate_to_gemini
Critique              │ —                               │ request_critique
Session persistence   │ Stop: write session to memory   │ —</div>
      <ul>
        <li><strong>Hooks handle the critical path.</strong> Rate awareness, memory injection, usage tracking, and session persistence are ALL guaranteed via hooks. The harness works even if Claude never calls a single MCP tool.</li>
        <li><strong>MCP tools handle optional capabilities.</strong> Active delegation and critique are enhancements Claude can choose when appropriate. If Claude doesn't call them, the system degrades gracefully — tasks run on Claude (the default) instead of being routed.</li>
        <li><strong>CLAUDE.md provides behavioral context.</strong> Not enforcement, but guidance: "You have these tools available. Here's when they're useful." Keeps Claude informed without constraining.</li>
      </ul>
    </div>

    <div class="section-label journey">User Journey Impact</div>
    <div class="section-content journey">
      <div class="journey-scenario">
        <p><span class="time">9:00 AM</span> <span class="action">— User types <code>claude</code>. MCP server spawns from .mcp.json. Hooks register from settings.json. User sees nothing different.</span></p>
        <p><span class="time">9:01 AM</span> <span class="action">— User types: "Research latest OAuth 2.1 spec changes." UserPromptSubmit hook fires → checks rate state (Claude at 72%), injects: "[Harness: Claude 72% weekly usage. Research tasks can be delegated to Gemini.]" → Claude reads injected context, decides to call <code>delegate_to_gemini</code>.</span></p>
        <p><span class="time">9:15 AM</span> <span class="action">— User types: "Now implement the changes in auth.ts." UserPromptSubmit fires → injects rate state + memory: "[Harness: Claude 73%. Previous task found 3 spec changes relevant to auth.ts.]" → Claude handles directly (coding task, memory context).</span></p>
        <p><span class="time">12:00 PM</span> <span class="action">— Session ends. Stop hook fires → writes session summary to memory. Tomorrow's session will have today's context automatically.</span></p>
      </div>
      <p style="margin-top: 0.75rem; color: var(--text-muted);"><strong>Key:</strong> The user never explicitly invoked the harness. Hooks injected context invisibly. Claude made routing decisions based on that context. The system worked even though MCP tool invocation was optional — hooks guaranteed the critical path.</p>
    </div>

    <div class="section-label realize">What This Makes You Realize</div>
    <div class="section-content realize">
      <ul>
        <li><strong>Hooks ARE the harness.</strong> MCP tools are optional power-ups. The entire critical path (rate awareness, memory injection, tracking, persistence) runs on hooks. This reframes the architecture: the harness is a hooks system with an optional MCP enhancement layer.</li>
        <li><strong>UserPromptSubmit rewrites the invisible routing problem.</strong> Instead of hoping Claude calls <code>route_task</code>, you inject the routing context directly. Claude's natural reasoning handles the rest. This is how you make routing invisible: not by building a router Claude must invoke, but by shaping the information Claude reasons with.</li>
        <li><strong>Could the entire harness be hooks-only?</strong> If hooks inject rate state + memory + routing suggestion on every prompt, and Claude's natural reasoning handles delegation (using PAL MCP's existing <code>clink</code> tool), do we need a custom MCP server at all? The harness might be: hooks + CLAUDE.md + PAL MCP. Zero custom MCP code.</li>
        <li><strong>The 3-tier model maps to failure tolerance:</strong> CLAUDE.md fails silently (Claude ignores instruction), MCP fails visibly (tool call fails), hooks can't fail silently (they either fire or error). Design for: hooks handle must-work, MCP handles nice-to-have, CLAUDE.md handles behavioral nudges.</li>
        <li><strong>Hook limitations matter:</strong> Can hooks see conversation history? Can they make HTTP calls? What's their latency budget? If UserPromptSubmit can't read a SQLite database in <100ms, the memory injection strategy breaks. These are implementation questions that determine feasibility.</li>
      </ul>
    </div>

    <div class="section-label threads">Open Threads</div>
    <div class="section-content threads">
      <ul>
        <li>What exactly can UserPromptSubmit inject? Plain text? JSON? System-level messages? This determines how sophisticated the context injection can be.</li>
        <li>What happens if a hook script crashes? Does Claude Code hang? Error gracefully? This determines how defensive hook implementations need to be.</li>
        <li>Hook latency budget: if every prompt adds 100-500ms from hook processing, is that acceptable? Can hooks run async?</li>
        <li>Could the harness be ENTIRELY hooks + CLAUDE.md + PAL MCP, with zero custom MCP server? This would massively simplify implementation.</li>
        <li>How many MCP tools is the sweet spot? Research suggests <5 for reliable invocation. The harness should expose the minimum viable tool set.</li>
      </ul>
    </div>
  </div>

  <!-- IQ-2 -->
  <div class="tool">
    <div class="tool-title">
      <span class="tool-name">IQ-2: Gemini Conductor Layer Mapping</span>
      <span class="wave-badge">Wave 1</span>
      <span class="layer-badge">All 5 Layers</span>
    </div>
    <p class="tool-desc">
      If Gemini Conductor wasn't limited to Gemini CLI, what parts of the 5 layers does it tackle? Conductor is NOT what it sounds like — it is a context-driven development workflow extension, not a multi-model orchestrator. Understanding what it actually does (and doesn't do) clarifies where the genuine gap lives.
    </p>

    <div class="section-label tension">The Core Tension</div>
    <div class="section-content tension">
      <ul>
        <li><strong>Conductor is narrower than the name suggests.</strong> It manages markdown-based specs, plans, and tracks. It does NOT route between models, select providers, persist semantic memory, or provide dashboards. It's structured project management for Gemini CLI.</li>
        <li><strong>"Unbounding" is a thought experiment, not a product comparison.</strong> The real question isn't "what if Conductor could use Claude?" — it's "what if Conductor's workflow philosophy were extended to a multi-provider world?" Those are very different questions.</li>
        <li><strong>As-is, Conductor covers ~15-20% of the harness vision.</strong> Almost all in Layer 3 (markdown persistence) and Layer 5 (CLI extension pattern). Zero coverage of Layers 1, 2, or 4.</li>
      </ul>
    </div>

    <div class="section-label works">Research Findings</div>
    <div class="section-content works">
      <p style="margin-bottom: 0.75rem;"><strong>Conductor (Dec 2025 preview) provides 6 commands:</strong> <code>/conductor:setup</code>, <code>newTrack</code>, <code>implement</code>, <code>status</code>, <code>revert</code>, <code>review</code>. Context → Spec & Plan → Implement lifecycle. Persistent markdown in <code>conductor/</code> directory. Git-aware rollback. Parallel tracks.</p>
      <div class="scenario">LAYER-BY-LAYER MAPPING:

Layer 1 (Router):     Conductor: 0%.  Gemini CLI Auto mode: ~15% (Flash/Pro intra-family)
Layer 2 (Decision):   Conductor: 0%.  Nothing makes cross-provider decisions.
Layer 3 (Memory):     Conductor: ~30%. Markdown files persist across sessions. No semantic search.
Layer 4 (Visual):     Conductor: 0%.  Text-only /status command.
Layer 5 (CLI):        Conductor: ~20%. Demonstrates CLI extension pattern. Not invisible — requires /conductor: prefix.</div>
      <ul>
        <li><strong>Gemini CLI Auto mode (separate from Conductor):</strong> Lightweight classifier (~1.6s) scores complexity 0-100. Below 50% → Flash, above → Pro. Keyword analysis. Intra-family only — no cross-provider awareness.</li>
        <li><strong>Google explicitly rejected multi-provider PR</strong> for Gemini CLI. Cross-provider support is outside Google's product incentive. The harness is structurally protected from Google shipping a "Claude+Gemini" solution.</li>
        <li><strong>The real competitive threats are NOT Conductor:</strong> Conductor Build (goforgeit.com) covers Layer 4 at ~60-70%. CCProxy covers Layers 1-2 (rule-based). Claude-Flow covers cross-provider orchestration. These are the actual competitors.</li>
      </ul>
    </div>

    <div class="section-label breaks">What Could Go Wrong</div>
    <div class="section-content breaks">
      <ul>
        <li><strong>Google ecosystem convergence (12-18 month risk):</strong> Conductor + Auto mode + monitoring dashboards + save_memory + ADK = maybe 60-70% of the vision for Gemini-only. Not a 3-month threat, but a trajectory to watch.</li>
        <li><strong>Third-party cross-provider tools are already shipping:</strong> Conductor Build (multi-engine visual dashboard, macOS app), CCProxy (Claude-to-Gemini routing), Claude-Flow (multi-provider orchestration). The gap is narrowing from below.</li>
        <li><strong>MCP as the great equalizer:</strong> The Agentic AI Foundation (AAIF, Dec 2025) makes MCP universal — adopted by Claude, Gemini, Cursor, VS Code, ChatGPT. Any MCP-based tool is inherently cross-provider. Competitors can build MCP-based harnesses just as easily.</li>
      </ul>
    </div>

    <div class="section-label gap">Design Implications</div>
    <div class="section-content gap">
      <p style="margin-bottom: 0.75rem;"><strong>Clear build-vs-adopt split emerges from the mapping:</strong></p>
      <ul>
        <li><strong>ADOPT Layer 4 (dashboard):</strong> Conductor Build already ships multi-engine visual dashboard. Don't build from scratch.</li>
        <li><strong>ADOPT/EXTEND Layer 5 (CLI proxy):</strong> CCProxy already ships Claude-to-Gemini routing. Extend, don't rebuild.</li>
        <li><strong>BUILD Layer 1 (semantic router):</strong> Nothing ships semantic cross-provider task classification.</li>
        <li><strong>BUILD Layer 2 (decision engine):</strong> Nothing ships the 4-signal fusion (route + rate + memory + subtask context). Most novel and hardest layer.</li>
        <li><strong>BUILD or EXTEND Layer 3 (memory):</strong> Conductor's markdown approach validates Phase 1 (files). Add semantic capabilities later.</li>
      </ul>
    </div>

    <div class="section-label journey">User Journey Impact</div>
    <div class="section-content journey">
      <div class="journey-contrast">
        <div class="journey-contrast-item current">
          <div class="label">Conductor-Only Experience</div>
          <span style="color: var(--text-dim); font-size: 0.85rem;">Gemini only. Structured planning. Persistent context. BUT: no Claude, no rate management, no semantic routing, explicit /conductor: commands, no dashboard. User abandons their most capable tool.</span>
        </div>
        <div class="journey-contrast-item hypothetical">
          <div class="label">Harness Experience</div>
          <span style="color: var(--text-dim); font-size: 0.85rem;">Claude + Gemini. Invisible routing. Semantic memory. Rate prediction. Type <code>claude</code> normally. "What did we learn about auth?" works. Cross-model critique for security.</span>
        </div>
      </div>
      <p style="margin-top: 0.75rem; color: var(--text-muted);"><strong>The experience gap:</strong> Conductor forces full Gemini commitment. The harness preserves Claude as the primary tool while adding Gemini as intelligent overflow. The user's $200/mo Claude Max investment is leveraged, not abandoned.</p>
    </div>

    <div class="section-label realize">What This Makes You Realize</div>
    <div class="section-content realize">
      <ul>
        <li><strong>The value prop is cross-provider orchestration, not technical novelty.</strong> Even if Conductor covered Layers 1-2 for Gemini-only, the harness's value is intact because no single vendor will ship Claude+Gemini integration. Google won't add Claude. Anthropic won't add Gemini. The cross-provider gap is structurally permanent.</li>
        <li><strong>The real moat might be "works with Claude Max subscription."</strong> Not a technical advantage, but a distribution one. Users don't want $200/mo for Claude Max AND separate API fees. The harness works within existing economics.</li>
        <li><strong>Conductor's markdown approach validates Phase 1 memory.</strong> Persistent markdown files in the repo, committed to git, readable by both humans and AI. Not sophisticated, but production-proven for context persistence. Start here, add semantic capabilities in Phase 2-3.</li>
        <li><strong>The 40% gap might shrink to 30-35%.</strong> If Conductor Build (goforgeit.com) is adopted for Layer 4, existing tool coverage rises from ~60% to ~65-70%. The remaining gap is smaller but harder and more valuable: semantic routing + decision engine + semantic memory.</li>
        <li><strong>Zero-code baseline should include Conductor.</strong> Expand the baseline test to: PAL MCP + CCProxy + CodexBar + Gemini CLI with Conductor. Test the full available ecosystem before deciding what to build.</li>
      </ul>
    </div>

    <div class="section-label threads">Open Threads</div>
    <div class="section-content threads">
      <ul>
        <li>Could Conductor Build (goforgeit.com) evolve into the harness? They already wrap Claude + Gemini + Codex. If they add semantic routing and memory, they become a commercial version of this project. Monitor their roadmap.</li>
        <li>Gemini 3's "Dynamic View" (generative UI from prompts) — could it partially address Layer 4 without building anything? Speculative but worth watching.</li>
        <li>If Google merges Conductor into Gemini CLI core, the bar for "better than just using Gemini" rises. Doesn't threaten the harness (still single-provider) but raises expectations.</li>
      </ul>
    </div>
  </div>

  <!-- WAVE 2 -->
  <div class="category">
    <div class="category-name">Wave 2 — After IQ-1</div>
    <div class="category-desc">Depends on decomposition answer. If decomposition changes what the router receives, these questions must account for that.</div>
  </div>

  <!-- IQ-7 -->
  <div class="tool">
    <div class="tool-title">
      <span class="tool-name">IQ-7: Semantic Routing vs Heuristics Consistency</span>
      <span class="wave-badge">Wave 2</span>
      <span class="layer-badge">Layer 1, 2</span>
    </div>
    <p class="tool-desc">
      The architecture says "semantic routing" (RouteLLM, Aurelio) but the diagrams show heuristic-style examples: "audit → Claude, 140K tokens → Gemini." Is this a consistency bug or the correct end state? Maybe heuristics are enough — and maybe decomposition (IQ-1) makes them even more sufficient.
    </p>

    <div class="section-label tension">The Core Tension</div>
    <div class="section-content tension">
      <ul>
        <li><strong>This might be a false dichotomy.</strong> RouteLLM's "pre-trained routers" ARE learned heuristics — just more sophisticated ones. The real question isn't "semantic vs heuristic" but how COMPLEX the heuristics need to be. Simple keyword rules? Embedding similarity? Trained classifiers?</li>
        <li><strong>The user's task distribution may be simpler than assumed.</strong> If 80% of tasks fall into obvious categories ("fix this bug" → code → Claude, "analyze this 150K codebase" → large context → Gemini), keyword matching gets 80% accuracy at zero cost. Is the 15% accuracy improvement from semantic routing worth 50-100ms + embedding costs per request?</li>
        <li><strong>The diagrams might be correct.</strong> Heuristic examples in diagrams because those ARE the common cases. Semantic routing exists only for the 10-15% ambiguous edge cases. The diagram inconsistency might reflect reality, not a documentation bug.</li>
      </ul>
    </div>

    <div class="section-label works">Research Findings</div>
    <div class="section-content works">
      <ul>
        <li><strong>RouteLLM is learned heuristics at scale.</strong> Trained on GPT-4 vs Mixtral routing decisions. The "pre-trained routers" are statistical models that learned complexity signals from millions of examples. They ARE heuristics — just far more nuanced than keyword matching.</li>
        <li><strong>CCProxy's pure heuristic approach (token count, tool match, model name):</strong> Works perfectly for "large analysis → Gemini" but cannot distinguish "research JWT vulnerabilities" from "debug auth.ts" — both look like ~10 tokens with no tools, route identically.</li>
        <li><strong>Hybrid approach exists:</strong> Fast path (heuristics for obvious cases: token count > 100K → Gemini, explicit delegation → PAL MCP) + slow path (semantic classification for ambiguous cases). How to decide which path? The complexity gate from IQ-1.</li>
        <li><strong>If IQ-1 decomposition happens first:</strong> Decomposed subtasks like "search codebase for auth patterns" are easily classified by keyword heuristics ("search" → research → Gemini). Decomposition may ELIMINATE the need for semantic routing by making routing inputs simple enough for rules.</li>
      </ul>
    </div>

    <div class="section-label breaks">What Could Go Wrong</div>
    <div class="section-content breaks">
      <ul>
        <li><strong>Semantic routing = over-engineering</strong> if heuristics work 90% of the time. Added cost and latency for marginal improvement. The user doesn't notice 5% more accuracy; they DO notice 100ms more latency.</li>
        <li><strong>Heuristics = under-engineering</strong> if the user frequently has ambiguous tasks ("research JWT vulnerabilities then implement fixes"). Misrouting research to Claude wastes tokens. Misrouting coding to Gemini produces lower quality.</li>
        <li><strong>Building semantic routing before measuring heuristic accuracy:</strong> Assumption-driven, not data-driven. The zero-code baseline test would reveal how often heuristic routing actually misroutes.</li>
      </ul>
    </div>

    <div class="section-label gap">Design Implications</div>
    <div class="section-content gap">
      <ul>
        <li><strong>If heuristics suffice:</strong> Layer 1 is a simple rule engine. Fast, free, deterministic. The diagrams accurately represent the production system.</li>
        <li><strong>If semantic needed:</strong> Layer 1 is a probabilistic classifier. 50-100ms, embedding cost, requires evaluation. Diagrams need to show classification output, not task-type labels.</li>
        <li><strong>Minimum Viable Router:</strong> Start with heuristics (token count, keyword matching, explicit delegation commands). Measure misroute rate. Add semantic classification only for task types that consistently misroute. Data-driven, not assumption-driven.</li>
        <li><strong>The diagrams should evolve:</strong> Phase 1 diagrams show heuristic routing (accurate for that phase). Phase 2 diagrams show semantic routing (if needed). The inconsistency resolves when the implementation exists.</li>
      </ul>
    </div>

    <div class="section-label journey">User Journey Impact</div>
    <div class="section-content journey">
      <p style="margin-bottom: 0.75rem;"><strong>The user doesn't care HOW routing happens — only that it's accurate and fast.</strong></p>
      <ul>
        <li>If heuristics misroute 1 in 10 tasks: the user corrects occasionally, barely notices.</li>
        <li>If heuristics misroute 3 in 10 tasks: the user learns not to trust routing, starts manually delegating. Harness becomes useless overhead.</li>
        <li>The threshold is somewhere around 85% accuracy. Below that, the user's manual routing judgment is better and faster.</li>
      </ul>
      <p style="margin-top: 0.75rem; color: var(--text-muted);">The zero-code baseline test answers this empirically: use heuristic routing for a week, measure how often the user disagrees with routing decisions.</p>
    </div>

    <div class="section-label realize">What This Makes You Realize</div>
    <div class="section-content realize">
      <ul>
        <li><strong>Adaptive routing is the actual answer.</strong> Start with heuristics. Log every routing decision and outcome. Identify task types that consistently misroute. Add semantic classification ONLY for those types. The system improves based on data, not assumptions.</li>
        <li><strong>Decomposition (IQ-1) may make this question moot.</strong> If compound tasks are decomposed into clear subtasks ("search codebase" → research, "modify auth.ts" → code), heuristics handle the decomposed inputs easily. The need for semantic routing depends on the answer to IQ-1.</li>
        <li><strong>The user's workflow distribution determines the answer.</strong> If 90% of tasks are obviously code/research/analysis, heuristics win. If 50% are ambiguous compound tasks, semantic routing wins. This isn't a technical question — it's an empirical one about THIS user's task patterns.</li>
        <li><strong>RouteLLM/Aurelio evaluation should wait.</strong> No point evaluating semantic routers before knowing whether heuristics are sufficient. Test heuristics first → measure → decide. This saves research time and implementation effort.</li>
      </ul>
    </div>

    <div class="section-label threads">Open Threads</div>
    <div class="section-content threads">
      <ul>
        <li>What's the actual misroute rate for heuristic routing on this user's task distribution? Only the zero-code baseline test can answer this.</li>
        <li>If decomposition (IQ-1) happens, does it simplify routing inputs enough to make semantic routing unnecessary?</li>
        <li>Can the harness log routing decisions from day one (even with heuristics) to build the dataset needed for future semantic routing evaluation?</li>
      </ul>
    </div>
  </div>

  <!-- IQ-3 -->
  <div class="tool">
    <div class="tool-title">
      <span class="tool-name">IQ-3: Layer 2 Routing Nuance — Forward-Thinking Conservation</span>
      <span class="wave-badge">Wave 2</span>
      <span class="layer-badge">Layer 2, 3</span>
    </div>
    <p class="tool-desc">
      Beyond task classification and current rate limits: if task classification says "Claude is best," but you're burning through capacity and have a complex architecture review Friday, should the system conserve Claude and route to Gemini now? This is <strong>predictive resource management</strong> — the hardest dimension of the hardest layer.
    </p>

    <div class="section-label tension">The Core Tension</div>
    <div class="section-content tension">
      <ul>
        <li><strong>Conservation implies value judgment.</strong> "This task is less important than a hypothetical future task." Who decides? The harness is an AI making resource allocation decisions on behalf of the user. That is a significant autonomy question.</li>
        <li><strong>Prediction requires data the system doesn't have.</strong> The user doesn't announce their schedule. How do you predict Friday's architecture review? Historical patterns ("Fridays are architecture-heavy")? Statistical burn rate projection? Or just: ask the user?</li>
        <li><strong>Quality degradation compounds invisibly.</strong> Routing a research task to Gemini when Claude would have been better is a single quality loss. But if conservation routes 10 tasks to Gemini over two days, the accumulated quality gap becomes noticeable — and the user can't pinpoint when or why things got worse.</li>
      </ul>
    </div>

    <div class="section-label works">Research Findings</div>
    <div class="section-content works">
      <ul>
        <li><strong>Cloud autoscaler analogy:</strong> Kubernetes uses predictive autoscaling with historical patterns (HPA + VPA). Reserves capacity for expected peak. Applicable concept: track weekly usage curves, predict remaining capacity, adjust routing aggressiveness.</li>
        <li><strong>RouteLLM's cost-quality trade-off:</strong> RouteLLM achieves 95% of GPT-4 quality using only 14% of GPT-4 calls by routing easy tasks to cheaper models. The mechanism: a learned threshold that balances quality vs cost. Directly applicable to conservation.</li>
        <li><strong>Burn rate is more useful than current state.</strong> "Claude at 60%" means nothing without context. "Claude at 60% on Wednesday, burning 30%/day, 2 days remaining" means conservation is critical. "Claude at 60% on Friday afternoon" means spend freely.</li>
        <li><strong>User-declared priorities exist in many systems:</strong> Kubernetes allows priority classes for pods. The harness equivalent: user says "/priority high" for critical tasks or "/conserve" to enable aggressive conservation mode.</li>
      </ul>
    </div>

    <div class="section-label breaks">What Could Go Wrong</div>
    <div class="section-content breaks">
      <ul>
        <li><strong>Conserving when not needed = unnecessary quality degradation.</strong> If the user doesn't have a Friday review, conservation wasted Claude capacity that expires unused at week reset.</li>
        <li><strong>NOT conserving when needed = hitting the wall.</strong> The exact problem the harness exists to solve.</li>
        <li><strong>"Rationing" feeling erodes trust.</strong> User: "Why is the system giving me Gemini when I need Claude?" If the answer is "because we're saving Claude for later" and the user disagrees, trust breaks.</li>
        <li><strong>Gaming:</strong> User learns "if I want Claude, phrase it as architecture" to bypass conservation. The system becomes adversarial.</li>
        <li><strong>Novel weeks break predictions.</strong> Historical patterns fail when the user works on something new. Monday-Thursday pattern doesn't predict a surprise Friday deadline.</li>
      </ul>
    </div>

    <div class="section-label gap">Design Implications</div>
    <div class="section-content gap">
      <p style="margin-bottom: 0.75rem;"><strong>Signals beyond the current 4 that Layer 2 should consume:</strong></p>
      <ul>
        <li><strong>Burn rate trajectory:</strong> Not just "Claude at 60%" but "burning 30%/day, 2 days to reset." This is the single most important conservation signal.</li>
        <li><strong>Day-of-week patterns:</strong> Historical task distribution. If Fridays are architecture-heavy, conserve Thursday afternoon.</li>
        <li><strong>Quality sensitivity:</strong> Some tasks tolerate B-tier model (research, bulk analysis). Others demand A-tier (security audit, architecture). Route by sensitivity, not just type.</li>
        <li><strong>User overrides and frequency:</strong> If the user overrides conservation 5x this week, reduce conservation aggressiveness. The system learns from disagreement.</li>
        <li><strong>Explicit declarations:</strong> <code>/conserve</code> and <code>/spend</code> modes. <code>/priority high</code> for specific tasks. User retains final authority.</li>
      </ul>
    </div>

    <div class="section-label journey">User Journey Impact</div>
    <div class="section-content journey">
      <div class="journey-scenario">
        <p><span class="time">Wednesday 2 PM</span> <span class="action">— Claude at 58%. Burn rate: 28%/day. Dashboard shows: "At current rate, Claude exhausts by Thursday evening." UserPromptSubmit injects: "[Harness: Conservation mode active. Research/analysis tasks routing to Gemini.]"</span></p>
        <p><span class="time">Wednesday 3 PM</span> <span class="action">— User: "Research JWT best practices." System routes to Gemini (research + conservation). User doesn't notice — research quality is comparable.</span></p>
        <p><span class="time">Thursday 9 AM</span> <span class="action">— User: "Review the auth security implementation." System routes to Claude (security-critical, high quality sensitivity). Conservation preserved capacity for this.</span></p>
      </div>
      <p style="margin-top: 0.75rem; color: var(--text-muted);"><strong>Override mechanism:</strong> User can type <code>/spend</code> to disable conservation for the current session, or <code>/priority high</code> to force Claude for a specific task. The system respects overrides and learns from them.</p>
    </div>

    <div class="section-label realize">What This Makes You Realize</div>
    <div class="section-content realize">
      <ul>
        <li><strong>There may be a simpler framing:</strong> "Always route to the cheapest model that meets a quality threshold." No prediction needed. Just: if Gemini can handle this at acceptable quality, use Gemini. If not, use Claude. Quality threshold is the only parameter, not capacity prediction.</li>
        <li><strong>This turns the harness into a planning tool.</strong> Predictive conservation means the harness thinks about the user's week, not just the current task. That's planning, not routing. Scope expansion.</li>
        <li><strong>The inverse is also valid:</strong> SPENDING aggressively early because models improve over time. Today's hard task is tomorrow's easy task as model capabilities advance. Conservation might be wrong if next week's models are better.</li>
        <li><strong>User-declared priorities are probably sufficient for MVP.</strong> Instead of predictive conservation: let the user say "I have a big review Friday." System notes it, adjusts routing. Simple, transparent, no prediction needed. Add automatic prediction in Phase 2 once patterns accumulate.</li>
        <li><strong>The quality threshold approach subsumes conservation:</strong> If you define "quality threshold per task type" (security: Claude only, research: either, bulk edit: either), conservation happens naturally. You never route security to Gemini regardless of capacity. You always route research to Gemini when it's cheaper. No prediction needed.</li>
      </ul>
    </div>

    <div class="section-label threads">Open Threads</div>
    <div class="section-content threads">
      <ul>
        <li>Is the "quality threshold per task type" approach sufficient, or does the user genuinely need predictive conservation?</li>
        <li>How many task-cycles before weekly patterns become reliable? If the user works differently every week, pattern-based prediction never converges.</li>
        <li>What does the override UX look like? CLI commands? Dashboard toggle? CLAUDE.md declarations?</li>
        <li>Break-even analysis: how accurate does prediction need to be to be worth the complexity? If random conservation performs 70% as well as perfect prediction, just use the simpler approach.</li>
      </ul>
    </div>
  </div>

  <!-- WAVE 3 -->
  <div class="category">
    <div class="category-name">Wave 3 — After IQ-1 + IQ-3</div>
    <div class="category-desc">Requires knowing pipeline shape (IQ-1) and routing factors (IQ-3) to map the complete cost surface.</div>
  </div>

  <!-- IQ-4 -->
  <div class="tool">
    <div class="tool-title">
      <span class="tool-name">IQ-4: Overarching LLM Costs Across All Layers</span>
      <span class="wave-badge">Wave 3</span>
      <span class="layer-badge">All Layers</span>
    </div>
    <p class="tool-desc">
      The harness exists to SAVE model usage. But it consumes LLM tokens for its own operations: classification, decomposition, memory reads/writes, confidence scoring. If the overhead is 15% per task, the harness needs to save MORE than 15% to break even. This is the ROI question for the entire project.
    </p>

    <div class="section-label tension">The Core Tension</div>
    <div class="section-content tension">
      <ul>
        <li><strong>The harness is a tax on every interaction.</strong> CLAUDE.md instructions consume context window tokens on every turn. Classification (if LLM-based) costs tokens per prompt. Memory operations (Mem0) cost tokens per read/write. The question isn't whether there's overhead — it's whether the overhead is acceptable.</li>
        <li><strong>The ROI equation:</strong> (Claude tokens saved by routing to Gemini) minus (harness overhead tokens) = net savings. If the harness saves 40% of Claude usage but consumes 20% in overhead, net savings is 20%. If overhead is 45%, the harness is net negative.</li>
        <li><strong>Cold start is the worst case.</strong> Day one: maximum overhead (classification, memory initialization, CLAUDE.md processing) with zero benefit (no memory data, no routing history). The harness must justify itself from the beginning or the user won't trust it.</li>
      </ul>
    </div>

    <div class="section-label works">Research Findings</div>
    <div class="section-content works">
      <p style="margin-bottom: 0.75rem;"><strong>Cost map by layer and frequency:</strong></p>
      <div class="scenario">PER-PROMPT COSTS (every interaction):
  CLAUDE.md context injection:  ~500-2,000 tokens (fixed tax per turn)
  UserPromptSubmit hook:        0 tokens (process execution, not LLM)
  Hooks (Pre/PostToolUse):      0 tokens (process execution)

PER-TASK COSTS (when task is routed):
  RouteLLM classification:      ~0 tokens (pre-trained classifier, local inference)
  Aurelio classification:       ~minimal (embedding comparison, not LLM call)
  Task decomposition (if IQ-1): ~500-2,000 tokens (LLM call for decomposition)
  Memory read (Mem0):           ~200-500 tokens (LLM extracts relevant memories)
  Memory read (files):          0 LLM tokens (Claude reads .md file naturally)
  Memory write (Mem0):          ~300-800 tokens (LLM extracts + stores)
  Memory write (files):         0 LLM tokens (file append)

TRIGGERED COSTS (occasional):
  Cross-model critique:         1.5-2x task cost (Gemini reviews Claude's work)
  Deep memory query:            ~500-1,000 tokens (semantic search + retrieval)
  Session summary (Stop hook):  ~500-1,500 tokens (if LLM summarizes)

NON-LLM COSTS:
  MCP server process:           ~10-50MB RAM, negligible CPU
  Hook execution:               ~1-10ms per hook
  Dashboard/widget:             ~20-50MB RAM for Electron/web process</div>
    </div>

    <div class="section-label breaks">What Could Go Wrong</div>
    <div class="section-content breaks">
      <ul>
        <li><strong>CLAUDE.md bloat:</strong> Harness instructions in CLAUDE.md eat context window on EVERY turn. At 200K context limit, 2,000 tokens of harness instructions is 1% — small but compounding over long sessions where context fills up.</li>
        <li><strong>Mem0 LLM calls on every interaction:</strong> If memory reads use an LLM for extraction, that's a cost per prompt. Over a day of heavy use (50-100 prompts), that's 10K-50K tokens just for memory operations.</li>
        <li><strong>Decomposition overhead on compound tasks:</strong> If IQ-1 says "yes, decompose," each compound task adds 500-2,000 tokens of decomposition cost. Plus per-subtask classification. The overhead scales with task complexity.</li>
        <li><strong>Critique is expensive when triggered:</strong> Cross-model critique (1.5-2x task cost) is justified for security-critical work but devastating for routine tasks if triggered incorrectly.</li>
      </ul>
    </div>

    <div class="section-label gap">Design Implications</div>
    <div class="section-content gap">
      <ul>
        <li><strong>File-based memory has ZERO LLM cost.</strong> Vs Mem0 which uses LLM calls for extraction. For overhead-sensitive design, files win. The trade-off is semantic search capability — but maybe that's a Phase 2 addition.</li>
        <li><strong>Hooks have ZERO LLM cost.</strong> This is why the hooks-first architecture (IQ-6) is critical. Every function moved from "MCP tool Claude must invoke" to "hook that fires automatically" reduces both reliability risk AND LLM overhead.</li>
        <li><strong>RouteLLM has near-zero cost.</strong> Pre-trained classifiers run local inference. No API call. This favors RouteLLM over an LLM-based classification approach.</li>
        <li><strong>Keep CLAUDE.md minimal.</strong> Every token in harness instructions is a tax on every turn. Write tight, not comprehensive. 500 tokens of well-crafted instructions beats 2,000 tokens of verbose ones.</li>
        <li><strong>Use the cheapest model for harness operations.</strong> If memory extraction uses an LLM, use Gemini Flash or Haiku — not Claude Opus. Harness overhead should use the cheapest capable model.</li>
      </ul>
    </div>

    <div class="section-label journey">User Journey Impact</div>
    <div class="section-content journey">
      <p style="margin-bottom: 0.75rem;"><strong>The invisible tax:</strong> The user doesn't see overhead, but they feel it in two ways:</p>
      <ul>
        <li><strong>Context window pressure:</strong> CLAUDE.md instructions + injected memory reduce available context. In a long session, the user hits context limits slightly earlier than without the harness.</li>
        <li><strong>Latency:</strong> Classification (50-100ms) + memory read (100-500ms) + decomposition (1-2s for compound) adds up. Simple tasks should feel instant; the harness should add <200ms to simple interactions.</li>
      </ul>
      <p style="margin-top: 0.75rem; color: var(--text-muted);"><strong>The dashboard should show ROI:</strong> "This week: harness overhead 8,500 tokens. Claude tokens saved by routing: 142,000. Net savings: 133,500 tokens (94% efficiency)." This builds trust that the overhead is worth it.</p>
    </div>

    <div class="section-label realize">What This Makes You Realize</div>
    <div class="section-content realize">
      <ul>
        <li><strong>The zero-cost architecture is: hooks + files + CLAUDE.md + PAL MCP.</strong> Hooks fire for free. File reads are free (Claude reads .md natively). CLAUDE.md is a fixed cost. PAL MCP is existing infrastructure. Total LLM overhead: ~500-2,000 tokens for CLAUDE.md per session. Everything else is zero.</li>
        <li><strong>There's a "break-even tasks per day" number.</strong> Below some threshold, the harness isn't worth running. If the user processes 5 tasks/day, overhead might exceed savings. At 20+ tasks/day, the math is clearly positive. Understanding this number is critical for the zero-code baseline test.</li>
        <li><strong>The cheapest harness is the best harness.</strong> Every design decision should minimize overhead. Prefer file-based memory over Mem0 (until semantic search proves necessary). Prefer hooks over MCP tools. Prefer RouteLLM over LLM-based classification. The overhead budget is tight.</li>
        <li><strong>Critique is the luxury feature.</strong> At 1.5-2x task cost, cross-model critique should trigger rarely and deliberately. The current design (user-triggered or complexity-triggered) is correct — never auto-trigger for routine work.</li>
      </ul>
    </div>

    <div class="section-label threads">Open Threads</div>
    <div class="section-content threads">
      <ul>
        <li>What's the actual CLAUDE.md token count for harness instructions? Measure against different instruction verbosity levels.</li>
        <li>What's Mem0's per-operation LLM cost in practice? Benchmark with real tasks before choosing between Mem0 and files.</li>
        <li>Is there a break-even chart: tasks/day vs. net token savings? This would help the user decide when to enable/disable the harness.</li>
        <li>Can the dashboard show real-time ROI? "Harness overhead this session: X tokens. Tokens saved: Y tokens. Net: Z."</li>
      </ul>
    </div>
  </div>

  <!-- WAVE 4 -->
  <div class="category">
    <div class="category-name">Wave 4 — After All Previous Waves</div>
    <div class="category-desc">Downstream of everything. The visual interface displays what the system produces — must know what's worth showing.</div>
  </div>

  <!-- IQ-5 -->
  <div class="tool">
    <div class="tool-title">
      <span class="tool-name">IQ-5: Visual Interface — What Should Actually Surface?</span>
      <span class="wave-badge">Wave 4</span>
      <span class="layer-badge">Layer 4</span>
    </div>
    <p class="tool-desc">
      Beyond just "Claude 42% | Gemini 88%" — what would a power user actually want to see? The interface is downstream of all logic decisions. It's not an information display — it's a <strong>trust mechanism</strong>. The question is: what builds trust in automated routing decisions you didn't make?
    </p>

    <div class="section-label tension">The Core Tension</div>
    <div class="section-content tension">
      <ul>
        <li><strong>Too much information = cognitive overload.</strong> Defeats the "invisible" goal. The user starts monitoring the dashboard instead of working.</li>
        <li><strong>Too little information = CodexBar clone.</strong> Already exists. "Claude: 42%" is not worth building.</li>
        <li><strong>The right amount shows what CodexBar CAN'T:</strong> WHY things were routed where they were, what the system learned, what it's predicting, what it would do differently if you changed parameters.</li>
      </ul>
    </div>

    <div class="section-label works">Research Findings</div>
    <div class="section-content works">
      <p style="margin-bottom: 0.75rem;"><strong>Four tiers of information, from ambient to deep:</strong></p>
      <ul>
        <li><strong>Tier 1 — Ambient (menu bar, always visible):</strong> Rate limits both providers, burn rate arrow (trending up/down), conservation mode indicator, session cost. Glanceable in <1 second.</li>
        <li><strong>Tier 2 — Activity (dashboard, recent):</strong> Last 10 routing decisions with 1-sentence justifications. Active tasks and which provider is handling them. Real-time token counts per provider.</li>
        <li><strong>Tier 3 — Analytics (dashboard, historical):</strong> Routing accuracy over time. Model performance comparison for YOUR task types. Weekly token savings from routing. Task distribution charts. Patterns the system has learned.</li>
        <li><strong>Tier 4 — Diagnostic (dashboard, rare):</strong> Full decision trace for any routing decision ("Route signal said Claude, rate limits said Gemini, memory said Gemini failed this last time, result: Claude with conservation note"). Memory contents browser. Signal weight visualization.</li>
      </ul>
      <p style="margin-top: 0.75rem;"><strong>From observability research (Langfuse, W&B):</strong> The most valued feature is trace visualization — seeing the chain of decisions that led to an outcome. Applied here: seeing the chain of signals that led to a routing decision.</p>
    </div>

    <div class="section-label breaks">What Could Go Wrong</div>
    <div class="section-content breaks">
      <ul>
        <li><strong>Dashboard shows routing user disagrees with → trust erosion.</strong> "Why did it send my security audit to Gemini?!" If the explanation is "because Claude was at 78%," the user might not accept the trade-off. Override mechanism is essential.</li>
        <li><strong>Predictions that are wrong are worse than no predictions.</strong> Showing "Claude exhausts by 3 PM Thursday" when it actually lasts until Friday evening makes the system feel unreliable.</li>
        <li><strong>Information asymmetry:</strong> Dashboard shows things the user can't act on. "Memory has 47 stored patterns" — so what? If the user can't edit, validate, or understand these patterns, the information breeds anxiety, not trust.</li>
        <li><strong>Dashboard addiction:</strong> The user compulsively checks routing decisions, losing the "invisible" benefit. The dashboard should be boring when things work — interesting only when things are unexpected.</li>
      </ul>
    </div>

    <div class="section-label gap">Design Implications</div>
    <div class="section-content gap">
      <ul>
        <li><strong>Data pipeline:</strong> Hooks → HTTP POST → SQLite → SSE → UI. Hooks capture events. SQLite stores them. Server-sent events push to browser. React/Svelte renders.</li>
        <li><strong>Menu bar widget (Swift/Electron):</strong> Minimal footprint. Shows: <code>● Claude: 42% ↑ │ Gemini: 88% │ $0.50</code>. The ↑ arrow indicates burn rate direction. Click to open dashboard.</li>
        <li><strong>Dashboard panels (informed by all IQs):</strong>
          <ul>
            <li>Activity Feed (from IQ-1): Shows decomposition trees if decomposition is used</li>
            <li>Routing Analytics (from IQ-7): Heuristic vs semantic accuracy comparison</li>
            <li>Cost Tracker (from IQ-4): Harness overhead vs savings, ROI visualization</li>
            <li>Rate Limits (from IQ-3): Burn rate projection, conservation status</li>
            <li>Memory Browser (from Layer 3): What the system has learned, pattern explorer</li>
            <li>Decision Traces (from IQ-3/AQ-1): Full signal breakdown for any routing decision</li>
          </ul>
        </li>
      </ul>
    </div>

    <div class="section-label journey">User Journey Impact</div>
    <div class="section-content journey">
      <div class="journey-scenario">
        <p><span class="time">Morning</span> <span class="action">— Glance at menu bar: "Claude: 87% | Gemini: 92%". Fresh week, full capacity. Start working normally.</span></p>
        <p><span class="time">Mid-day</span> <span class="action">— Notice menu bar says "Claude: 52% ↓". Hmm, burning fast. Click to open dashboard. See: "15 tasks today, 9 routed to Claude, 6 to Gemini. Projected exhaustion: Thursday 2 PM." Decision: enable conservation mode.</span></p>
        <p><span class="time">3 PM</span> <span class="action">— See a routing decision: "Security review → Gemini. Reason: conservation mode active." Disagree — override with <code>/priority high</code>. System notes the override and routes to Claude.</span></p>
        <p><span class="time">End of day</span> <span class="action">— Open dashboard weekly view. "Harness saved 47,000 Claude tokens this week (12% of capacity). Overhead: 3,200 tokens (0.8%). Net efficiency: 94%." Trust increases.</span></p>
      </div>
    </div>

    <div class="section-label realize">What This Makes You Realize</div>
    <div class="section-content realize">
      <ul>
        <li><strong>The dashboard is a trust mechanism, not an information mechanism.</strong> The real question isn't "what to show" but "what builds trust in automated decisions." Trust grows from: (1) transparency about why decisions were made, (2) ability to override, (3) evidence the system is learning and improving, (4) ROI proof.</li>
        <li><strong>The counterfactual is powerful:</strong> "If you'd used Claude for this task, it would have cost 4,200 tokens. Gemini handled it for $0. Quality difference: negligible." This shows the value of routing in concrete terms.</li>
        <li><strong>"What would make you stop checking?" is the measure of trust.</strong> The dashboard succeeds when the user opens it weekly instead of hourly. Boring is good — it means the system is working.</li>
        <li><strong>Notifications, not dashboards, for urgent events:</strong> "Claude at 90% — switching to conservation mode" as a macOS notification. "Gemini failed mid-task, retrying on Claude" as an inline CLI message. Critical events come to the user; the user goes to the dashboard for curiosity.</li>
        <li><strong>The learning dashboard is unique:</strong> Show what the memory system has captured over time — patterns, routing effectiveness per task type, user override patterns. This is something no existing tool offers. It's a mirror showing the user their own workflow habits.</li>
      </ul>
    </div>

    <div class="section-label threads">Open Threads</div>
    <div class="section-content threads">
      <ul>
        <li>Is a macOS menu bar widget (Swift) worth building vs a Chrome extension or just the web dashboard?</li>
        <li>How do you make the dashboard "boring when things work, interesting when things are unexpected"? Conditional highlighting? Alert-based UI?</li>
        <li>What's the technical stack for the dashboard? Electron vs web? React vs Svelte? SQLite vs something else?</li>
        <li>Should the dashboard show a "learning progress" indicator? ("Memory confidence: 34%. System is still learning your patterns. Improving by ~3% per week.")</li>
      </ul>
    </div>
  </div>

  <!-- AUXILIARY QUESTIONS -->
  <div class="category">
    <div class="category-name">Auxiliary Questions</div>
    <div class="category-desc">Surfaced during research. Not primary focus, but tracked so they don't get lost. May become primary as investigation progresses.</div>
  </div>

  <div class="tool" style="border-color: var(--border); opacity: 0.85;">
    <div class="tool-title">
      <span class="tool-name">AQ-1: Signal Arbitration</span>
      <span class="layer-badge">Layer 2</span>
    </div>
    <p class="tool-desc">When Layer 2's four signals disagree — route says Claude, rate limits say Gemini, memory says "Gemini failed last time," subtask context says "stay on Claude for continuity" — what wins? Priority ordering? Weighted scoring? User override? This IS Layer 2's core problem.</p>
  </div>

  <div class="tool" style="border-color: var(--border); opacity: 0.85;">
    <div class="tool-title">
      <span class="tool-name">AQ-2: Error Recovery & Partial Failure</span>
      <span class="layer-badge">All Layers</span>
    </div>
    <p class="tool-desc">If Gemini fails mid-task: retry same model? Escalate to Claude? Roll back partial state? If 5 decomposed subtasks and subtask 4 fails, what's the contract? Decomposition (IQ-1) makes this more acute — more subtasks = more failure points.</p>
  </div>

  <div class="tool" style="border-color: var(--border); opacity: 0.85;">
    <div class="tool-title">
      <span class="tool-name">AQ-3: Cold Start & Bootstrap</span>
      <span class="layer-badge">Layer 3, 2, 1</span>
    </div>
    <p class="tool-desc">Memory is "Layer 0" but empty on Day 1. System must work immediately. Hardcoded defaults? Starter knowledge? How many task-cycles before memory helps? Cold start is where overhead (IQ-4) is highest and benefit is lowest.</p>
  </div>

  <div class="tool" style="border-color: var(--border); opacity: 0.85;">
    <div class="tool-title">
      <span class="tool-name">AQ-4: Latency Budget</span>
      <span class="layer-badge">All Layers</span>
    </div>
    <p class="tool-desc">Every layer adds latency. Classification (~50-100ms) + memory read (~100-500ms) + decomposition (~1-2s for compound) = 500ms-1.5s before work starts. For "fix this typo," that's noticeable. Per-layer budget? Fast path for trivial tasks? Parallelization?</p>
  </div>

  </div><!-- end tab-investigation -->

</div>

<script>
  document.querySelectorAll('.tab-btn').forEach(btn => {
    btn.addEventListener('click', () => {
      document.querySelectorAll('.tab-btn').forEach(b => b.classList.remove('active'));
      document.querySelectorAll('.tab-panel').forEach(p => p.classList.remove('active'));
      btn.classList.add('active');
      document.getElementById('tab-' + btn.dataset.tab).classList.add('active');
    });
  });
</script>

</body>
</html>
